{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "VARIANT_NUM = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Goal of This File:\n",
    "\n",
    "##### 1. Import Libraries, Helper Functions, and Constants ==> Data Sourcing and Munging ==> Utility Functions ==> Loading the Data ==> Merging all Individual Files\n",
    "\n",
    "##### 2. Imbalanced Data Solution ==> Class Weights ==> Glove + Hyperparameters\n",
    "\n",
    "##### 3. Bidirectional LSTM\n",
    "\n",
    "##### 4. Conclusion ==> Results ==> Tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries, Helper Functions and Load Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas numpy nltk scikit-learn wordcloud\\\n",
    "seaborn gensim tensorflow imblearn xgboost matplotlib unrar pyunpack more-itertools patool transformers > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "unrar is already the newest version (1:5.6.6-2build1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 47 not upgraded.\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "rar is already the newest version (2:5.5.0-1build1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 47 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "VAST = True\n",
    "\n",
    "if VAST:\n",
    "    !sudo apt-get install unrar\n",
    "    !sudo apt-get install rar\n",
    "    \n",
    "GDRIVE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "UTILS_URL = 'https://f005.backblazeb2.com/file/gp-support-files/nlp_ai_utils.py'\n",
    "UPDATING_VALUES_URL = 'https://f005.backblazeb2.com/file/gp-support-files/updating_values.py'\n",
    "ALL_LIBS_URL = 'https://f005.backblazeb2.com/file/gp-support-files/all_libs_dl.py'\n",
    "CHUNKS_URLS_FILE_URL = 'https://f005.backblazeb2.com/file/gp-support-files/chunks_urls.py'\n",
    "\n",
    "UTILS_FILE_NAME = 'nlp_ai_utils'\n",
    "UPDATING_VALUES_FILE_NAME = 'updating_values'\n",
    "ALL_LIBS_FILE_NAME = 'all_libs_dl'\n",
    "CHUNKS_URLS_FILE_NAME = 'chunks_urls'\n",
    "\n",
    "DEP_FILE_EXT = '.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "def get_dependencies(url,file_name,file_extension):\n",
    "    if os.path.exists(file_name + file_extension):\n",
    "        return print(file_name + \" already exists.\")\n",
    "    else:\n",
    "        print(f\"downloading {file_name}...\")\n",
    "        r = requests.get(url)\n",
    "        with open(file_name + file_extension, 'wb') as fd:\n",
    "            for chunk in r.iter_content():\n",
    "                fd.write(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nlp_ai_utils already exists.\n",
      "updating_values already exists.\n",
      "all_libs_dl already exists.\n",
      "chunks_urls already exists.\n"
     ]
    }
   ],
   "source": [
    "get_dependencies(UTILS_URL,UTILS_FILE_NAME,DEP_FILE_EXT)\n",
    "get_dependencies(UPDATING_VALUES_URL,UPDATING_VALUES_FILE_NAME,DEP_FILE_EXT)\n",
    "get_dependencies(ALL_LIBS_URL,ALL_LIBS_FILE_NAME,DEP_FILE_EXT)\n",
    "get_dependencies(CHUNKS_URLS_FILE_URL,CHUNKS_URLS_FILE_NAME,DEP_FILE_EXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "2023-06-02 23:39:02.805019: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-06-02 23:39:02.833123: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from nlp_ai_utils import *\n",
    "from chunks_urls import CHUNKS_URLS\n",
    "from updating_values import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = 'https://f005.backblazeb2.com/file/yelp-review-data/chunk_'\n",
    "end_of_file = '.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_ENABLE_ONEDNN_OPTS = 0\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "LIMIT = 18\n",
    "DATA_URLS = CHUNKS_URLS[:LIMIT]\n",
    "GLOVE_URL = 'https://f005.backblazeb2.com/file/glove-embeddings-dims/glove.6B.100d.txt'\n",
    "READY_DATA_URL = 'https://f005.backblazeb2.com/file/yelp-dataset-ready-for-models/ready_for_models.rar'\n",
    "CLASS_WEIGHTS_URL = 'https://f005.backblazeb2.com/file/yelp-dataset-ready-for-models/class_weights.pickle'\n",
    "UNIQUE_WORDS_URL = 'https://f005.backblazeb2.com/file/yelp-dataset-ready-for-models/unique_words.pickle'\n",
    "\n",
    "if GDRIVE:\n",
    "    PICKLES_DIR = \"pickle_files\"\n",
    "    READY_DATASET_PATH = \"larger_dataset/ready_for_models/\"\n",
    "    LARGER_DATASET_PATH = \"larger_dataset\"\n",
    "    GLOVE_FILES_DIR = \"glove_files\"\n",
    "    SAVED_MODELS_DIR = 'saved_models'\n",
    "else:\n",
    "    PICKLES_DIR = \"../pickle_files\"    \n",
    "    LARGER_DATASET_PATH = \"../larger_dataset\"\n",
    "    READY_DATASET_PATH = \"../larger_dataset/ready_for_models/\"\n",
    "    GLOVE_FILES_DIR = \"../glove_files\"\n",
    "    SAVED_MODELS_DIR = '../saved_models'\n",
    "    \n",
    "PREPROCESSED_CHUNKS_PATH = LARGER_DATASET_PATH + \"/preprocessed_data_chunks/\"\n",
    "BASE_FILE_NAME = \"chunk_\"\n",
    "FILE_FORMAT = \".csv\"\n",
    "DATA_IN_CHUNK = 116505\n",
    "if LIMIT == 60:\n",
    "    ACTUAL_DATA_SHAPE = DATA_IN_CHUNK * LIMIT + 20\n",
    "else:\n",
    "    ACTUAL_DATA_SHAPE = DATA_IN_CHUNK * LIMIT\n",
    "RANDOM_STATE = CONST_RANDOM_STATE\n",
    "DELETE_PICKLES_AFTER_TRAINING = True\n",
    "USE_READY_DATA = False\n",
    "BALANCE_DATA = True\n",
    "USE_MODIFIED_LABELS = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Data Sourcing and Munging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1 Loading The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_1 already exists.\n",
      "chunk_2 already exists.\n",
      "chunk_3 already exists.\n",
      "chunk_4 already exists.\n",
      "chunk_5 already exists.\n",
      "chunk_6 already exists.\n",
      "chunk_7 already exists.\n",
      "chunk_8 already exists.\n",
      "chunk_9 already exists.\n",
      "chunk_10 already exists.\n",
      "chunk_11 already exists.\n",
      "chunk_12 already exists.\n",
      "chunk_13 already exists.\n",
      "chunk_14 already exists.\n",
      "chunk_15 already exists.\n",
      "chunk_16 already exists.\n",
      "chunk_17 already exists.\n",
      "chunk_18 already exists.\n"
     ]
    }
   ],
   "source": [
    "if not USE_READY_DATA:\n",
    "    if not os.path.exists(LARGER_DATASET_PATH):\n",
    "        os.mkdir(LARGER_DATASET_PATH)\n",
    "    if not os.path.exists(PREPROCESSED_CHUNKS_PATH):\n",
    "        os.mkdir(PREPROCESSED_CHUNKS_PATH)\n",
    "    get_chunks(DATA_URLS,LIMIT,1,BASE_FILE_NAME,PREPROCESSED_CHUNKS_PATH,FILE_FORMAT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2 Merging all Individual Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all names of downloaded files\n",
    "if not USE_READY_DATA:\n",
    "    all_file_names = get_all_file_names(BASE_FILE_NAME,LIMIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read all chunks into a list\n",
    "if not USE_READY_DATA:\n",
    "    list_dfs = read_chunks(all_file_names,PREPROCESSED_CHUNKS_PATH,FILE_FORMAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate all chunks into a singular df\n",
    "if not USE_READY_DATA:\n",
    "    df = group_up_chunks(list_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check how much of the data was actually downloaded\n",
    "if not USE_READY_DATA:\n",
    "    percent_loaded = check_no_missing_data(df.shape[0],ACTUAL_DATA_SHAPE)\n",
    "    percent_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USE_READY_DATA:\n",
    "    review_data = df[['text', 'stars']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USE_READY_DATA:\n",
    "    review_data.reset_index(inplace = True)\n",
    "    review_data.drop(['index'],axis = 1,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USE_READY_DATA:\n",
    "    review_data.rename(columns = {'text':'full_review_text','stars':'star_rating'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USE_READY_DATA:\n",
    "    review_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USE_READY_DATA:\n",
    "    review_data.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USE_READY_DATA:\n",
    "    review_data['full_review_text'].replace('', np.nan, inplace=True)\n",
    "    review_data.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imbalanced Data Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if BALANCE_DATA:\n",
    "    label_1 = review_data[review_data['star_rating'] == 1]\n",
    "    label_2 = review_data[review_data['star_rating'] == 2]\n",
    "    label_3 = review_data[review_data['star_rating'] == 3]\n",
    "    label_4 = review_data[review_data['star_rating'] == 4]\n",
    "    label_5 = review_data[review_data['star_rating'] == 5]\n",
    "\n",
    "    minority_class = min([label_1.shape[0],label_2.shape[0],label_3.shape[0],label_4.shape[0],label_5.shape[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(487284, 2)\n",
      "star_rating\n",
      "3.0    162428\n",
      "1.0     81214\n",
      "2.0     81214\n",
      "4.0     81214\n",
      "5.0     81214\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "if BALANCE_DATA:\n",
    "    if USE_MODIFIED_LABELS:\n",
    "        review_data_label_1 = label_1[:int(minority_class / 2)]\n",
    "        review_data_label_2 = label_2[:int(minority_class / 2)]\n",
    "        review_data_label_3 = label_3[:int(minority_class)]\n",
    "        review_data_label_4 = label_4[:int(minority_class / 2)]\n",
    "        review_data_label_5 = label_5[:int(minority_class / 2)]\n",
    "\n",
    "    else:\n",
    "        review_data_label_1 = label_1[:int(minority_class)]\n",
    "        review_data_label_2 = label_2[:int(minority_class)]\n",
    "        review_data_label_3 = label_3[:int(minority_class)]\n",
    "        review_data_label_4 = label_4[:int(minority_class)]\n",
    "        review_data_label_5 = label_5[:int(minority_class)]\n",
    "\n",
    "    review_data = pd.concat([review_data_label_1,review_data_label_2,review_data_label_3,review_data_label_4,review_data_label_5])\n",
    "\n",
    "    print(review_data.shape)\n",
    "\n",
    "    print(review_data['star_rating'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if BALANCE_DATA:\n",
    "    review_data = review_data.sample(frac=1,random_state=RANDOM_STATE)\n",
    "    review_data.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USE_READY_DATA:\n",
    "    X = review_data['full_review_text']\n",
    "    y = review_data['star_rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USE_READY_DATA:\n",
    "    X = pd.Series([str(text) for text in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_MODIFIED_LABELS:\n",
    "    translated_labels = translate_labels(y)\n",
    "    y = pd.Series(encode_sent(translated_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2    162428\n",
      "3    162428\n",
      "1    162428\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(y.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USE_READY_DATA:\n",
    "    y = y - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classes' shapes before and after (if USE_MODIFIED_LABELS = True):\n",
    "\n",
    "BEFORE: 1 ==> Very Bad ==> AFTER: 2 ==> Negative Sentiment\n",
    "\n",
    "BEFORE: 2 ==> Bad ==> AFTER: 2  ==> Negative Sentiment\n",
    "\n",
    "BEFORE: 3 ==> Ok ==> AFTER: 3  ==> Neutral Sentiment\n",
    "\n",
    "BEFORE: 4 ==> Good ==> AFTER: 1  ==> Positive Sentiment\n",
    "\n",
    "BEFORE: 5 ==> Very Good ==> AFTER: 1  ==> Positive Sentiment\n",
    "\n",
    "THEN: 1 is subtracted from each label. So, the Labels go FROM: 2,3,1 TO: 1,2,0\n",
    "\n",
    "#### Classes' shapes before and after (if USE_MODIFIED_LABELS = False):\n",
    "\n",
    "1 is subtracted from each label. So, the Labels go FROM: 1,2,3,4,5 TO: 0,1,2,3,4\n",
    "\n",
    "Why is 1 subracted from each label? To bring the data into the preferred shape of the class weights in both sklearn and keras (starting label is 0, not 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Class Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(PICKLES_DIR):\n",
    "    os.mkdir(PICKLES_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if USE_READY_DATA:\n",
    "    get_chunks([CLASS_WEIGHTS_URL],0,1,'class_weights',PICKLES_DIR + '/','.pickle',False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_READY_DATA:\n",
    "    class_weights = pickle.load(open(PICKLES_DIR + \"/class_weights.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USE_READY_DATA:\n",
    "    if BALANCE_DATA:\n",
    "        class_weights = False\n",
    "    else:\n",
    "        class_weights = compute_class_weight(class_weight = \"balanced\",classes = np.unique(y),y=y)\n",
    "        class_weights = dict(zip(np.unique(y), class_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 GloVe + Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(GLOVE_FILES_DIR):\n",
    "    os.mkdir(GLOVE_FILES_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove.6B.100d already exists.\n"
     ]
    }
   ],
   "source": [
    "if not USE_READY_DATA:\n",
    "    get_chunks([GLOVE_URL],0,1,\"glove.6B.100d\",GLOVE_FILES_DIR + '/','.txt',False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Unique Words...\n"
     ]
    }
   ],
   "source": [
    "if not USE_READY_DATA:\n",
    "    print('Getting Unique Words...')\n",
    "    UNIQUE_WORDS = set(' '.join(X).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_READY_DATA:\n",
    "    get_chunks([UNIQUE_WORDS_URL],0,1,'unique_words',PICKLES_DIR + '/','.pickle',False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_READY_DATA:\n",
    "    UNIQUE_WORDS = pickle.load(open(PICKLES_DIR + \"/unique_words.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(UNIQUE_WORDS)\n",
    "LSTM_UNITS = 256\n",
    "DENSE_UNITS = 1024\n",
    "LABELS_COUNT = len(y.unique())\n",
    "EMBEDDING_DIM = 100\n",
    "MAX_TEXT_LEN = 250\n",
    "CONV_FILTERS = 60\n",
    "CONV_KERNEL_SIZE = 5\n",
    "DROPOUT_VAL = 0.4\n",
    "TRUNC_TYPE = 'post'\n",
    "PADDING_TYPE = 'post'\n",
    "OOV_TOKEN = \"<OOV>\"\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ==> here is a quick explaination of how the dataset will be split using a smaller sample example.\n",
    "##### ==> dataset => 100\n",
    "##### ==> train_set => tr_s (example: 80)\n",
    "##### ==> valid_set => vs (example: 10)\n",
    "##### ==> test_set => te_s (example: 10)\n",
    "##### ==> t = tr_s (80) + vs (10)\n",
    "##### ==> train_set = x[:80]\n",
    "##### ==> valid_set = x[80:t]\n",
    "##### ==> test_set = x[t:] why t? because => vs = ts\n",
    "\n",
    "##### use this guideline if you are confused about how the train-validation-test split was done. Also, this is a future guide for me as well in case I forget."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* train_set_size = 6,990,280 * 0.8 = 5,592,224\n",
    "* valid_set_size = 6,990,280 * 0.1 = 699,028\n",
    "* train_plus_valid = 5,592,224 + 699,028 = 6,291,252\n",
    "\n",
    "==> To Confirm: test_size = 6,990,280 - 6,291,252 = 699,028\n",
    "\n",
    "* train_set = [0:5,592,224]\n",
    "* train_labels = [0:5,592,224]\n",
    "* validation_set = [5,592,224:6,291,252] ==> 699,028\n",
    "* validation_labels = [5,592,224:6,291,252] ==> 699,028\n",
    "* test_set = [6,291,252,6,990,280] ==> 699,028\n",
    "* test_labels = [6,291,252,6,990,280] ==> 699,028"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTUAL_DATA_SHAPE = len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.reset_index(drop=True,inplace=True)\n",
    "y.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not USE_READY_DATA:\n",
    "    TRAIN_PERCENT = 0.8\n",
    "    VALID_TEST_PERCENT = 0.1\n",
    "    TRAIN_SIZE = int(ACTUAL_DATA_SHAPE * TRAIN_PERCENT)\n",
    "    VALID_TEST_SIZE = int(ACTUAL_DATA_SHAPE * VALID_TEST_PERCENT)\n",
    "    TOTAL_TEST_SIZE = TRAIN_SIZE + VALID_TEST_SIZE\n",
    "    train_set = X[:TRAIN_SIZE]\n",
    "    train_labels = y[:TRAIN_SIZE]\n",
    "    validation_set = X[TRAIN_SIZE:TOTAL_TEST_SIZE]\n",
    "    validation_labels = y[TRAIN_SIZE:TOTAL_TEST_SIZE]\n",
    "    test_set = X[TOTAL_TEST_SIZE:]\n",
    "    test_labels = y[TOTAL_TEST_SIZE:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.reset_index(drop=True,inplace=True)\n",
    "train_labels.reset_index(drop=True,inplace=True)\n",
    "validation_set.reset_index(drop=True,inplace=True)\n",
    "validation_labels.reset_index(drop=True,inplace=True)\n",
    "test_set.reset_index(drop=True,inplace=True)\n",
    "test_labels.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(LARGER_DATASET_PATH):\n",
    "    os.mkdir(LARGER_DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(READY_DATASET_PATH):\n",
    "    os.mkdir(READY_DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if USE_READY_DATA:\n",
    "    get_chunks([READY_DATA_URL],0,1,'ready_for_models',READY_DATASET_PATH,'.rar',False)\n",
    "    Archive(os.path.join(READY_DATASET_PATH,\"ready_for_models.rar\")).extractall(READY_DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_READY_DATA:\n",
    "    train_set_padded = pickle.load(open(READY_DATASET_PATH + \"train_set_padded.pickle\", \"rb\"))\n",
    "    train_labels = pickle.load(open(READY_DATASET_PATH + \"train_labels.pickle\", \"rb\"))\n",
    "    valid_set_padded = pickle.load(open(READY_DATASET_PATH + \"valid_set_padded.pickle\", \"rb\"))\n",
    "    validation_labels = pickle.load(open(READY_DATASET_PATH + \"validation_labels.pickle\", \"rb\"))\n",
    "    test_set_padded = pickle.load(open(READY_DATASET_PATH + \"test_set_padded.pickle\", \"rb\"))\n",
    "    test_labels = pickle.load(open(READY_DATASET_PATH + \"test_labels.pickle\", \"rb\"))\n",
    "    EMBEDDING_LAYER = pickle.load(open(READY_DATASET_PATH + \"EMBEDDING_LAYER.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing the Dataset...\n"
     ]
    }
   ],
   "source": [
    "# NOTE: THIS CELL TAKES A WHILE TO RUN.\n",
    "if not os.path.exists(READY_DATASET_PATH + \"train_set_padded.pickle\") and not USE_READY_DATA:\n",
    "    print(\"Tokenizing the Dataset...\")\n",
    "    tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token=OOV_TOKEN)\n",
    "    tokenizer.fit_on_texts(train_set)\n",
    "    words_to_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padding the Training Set.\n",
      "Padding the Validation Set.\n",
      "Padding the Testing Set.\n"
     ]
    }
   ],
   "source": [
    "# NOTE: THIS CELL TAKES A WHILE TO RUN.\n",
    "if not os.path.exists(READY_DATASET_PATH + \"train_set_padded.pickle\") and not USE_READY_DATA:\n",
    "    print(\"Padding the Training Set.\")\n",
    "    train_sequences = tokenizer.texts_to_sequences(train_set)\n",
    "    train_set_padded = pad_sequences(train_sequences, maxlen=MAX_TEXT_LEN, padding=PADDING_TYPE, truncating=TRUNC_TYPE)\n",
    "    \n",
    "if not os.path.exists(READY_DATASET_PATH + \"valid_set_padded.pickle\") and not USE_READY_DATA:\n",
    "    print(\"Padding the Validation Set.\")\n",
    "    valid_sequences = tokenizer.texts_to_sequences(validation_set)\n",
    "    valid_set_padded = pad_sequences(valid_sequences, maxlen=MAX_TEXT_LEN, padding=PADDING_TYPE, truncating=TRUNC_TYPE)\n",
    "    \n",
    "if not os.path.exists(READY_DATASET_PATH + \"test_set_padded.pickle\") and not USE_READY_DATA:\n",
    "    print(\"Padding the Testing Set.\")\n",
    "    test_sequences = tokenizer.texts_to_sequences(test_set)\n",
    "    test_set_padded = pad_sequences(test_sequences, maxlen=MAX_TEXT_LEN, padding=PADDING_TYPE, truncating=TRUNC_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-formatting Train Set Shape.\n",
      "Re-formatting Train Labels Shape.\n",
      "Re-formatting Validation Set Shape.\n",
      "Re-formatting Validation Labels Shape.\n",
      "Re-formatting Testing Set Shape.\n",
      "Re-formatting Testing Labels Shape.\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(READY_DATASET_PATH + \"train_set_padded.pickle\") and not USE_READY_DATA:\n",
    "    print(\"Re-formatting Train Set Shape.\")\n",
    "    train_set_padded = np.array(train_set_padded)\n",
    "    \n",
    "if not os.path.exists(READY_DATASET_PATH + \"train_labels.pickle\") and not USE_READY_DATA:\n",
    "    print(\"Re-formatting Train Labels Shape.\")\n",
    "    train_labels = np.array(train_labels)\n",
    "    \n",
    "if not os.path.exists(READY_DATASET_PATH + \"valid_set_padded.pickle\") and not USE_READY_DATA: \n",
    "    print(\"Re-formatting Validation Set Shape.\")\n",
    "    valid_set_padded = np.array(valid_set_padded)\n",
    "\n",
    "if not os.path.exists(READY_DATASET_PATH + \"validation_labels.pickle\") and not USE_READY_DATA:\n",
    "    print(\"Re-formatting Validation Labels Shape.\")\n",
    "    validation_labels = np.array(validation_labels)\n",
    "\n",
    "if not os.path.exists(READY_DATASET_PATH + \"test_set_padded.pickle\") and not USE_READY_DATA:\n",
    "    print(\"Re-formatting Testing Set Shape.\")\n",
    "    test_set_padded = np.array(test_set_padded)\n",
    "\n",
    "if not os.path.exists(READY_DATASET_PATH + \"test_labels.pickle\") and not USE_READY_DATA:\n",
    "    print(\"Re-formatting Testing Labels Shape.\")\n",
    "    test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USE_READY_DATA:\n",
    "    word_to_vec_map = read_glove_vector(GLOVE_FILES_DIR + '/glove.6B.100d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Embedding From Scratch.\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(READY_DATASET_PATH + \"EMBEDDING_LAYER.pickle\"):\n",
    "    print(\"Generating Embedding From Scratch.\")\n",
    "    vocab_mapping = len(words_to_index)\n",
    "    embed_vector_len = word_to_vec_map['moon'].shape[0]\n",
    "\n",
    "    emb_matrix = np.zeros((vocab_mapping, embed_vector_len))\n",
    "\n",
    "    for word, index in words_to_index.items():\n",
    "        embedding_vector = word_to_vec_map.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            emb_matrix[index-1, :] = embedding_vector\n",
    "\n",
    "    EMBEDDING_LAYER = Embedding(input_dim=vocab_mapping,\\\n",
    "                                output_dim=embed_vector_len, input_length=MAX_TEXT_LEN, weights = [emb_matrix], trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(READY_DATASET_PATH + \"train_set_padded.pickle\") and not USE_READY_DATA:\n",
    "    print(\"Using Pickle File!\")\n",
    "    train_set_padded = pickle.load(open(READY_DATASET_PATH + \"train_set_padded.pickle\", \"rb\"))\n",
    "else:\n",
    "    pickle_out = open(READY_DATASET_PATH + \"train_set_padded.pickle\",'wb')\n",
    "    pickle.dump(train_set_padded,pickle_out)\n",
    "    pickle_out.close()\n",
    "    \n",
    "if os.path.exists(READY_DATASET_PATH + \"train_labels.pickle\") and not USE_READY_DATA:\n",
    "    print(\"Using Pickle File!\")\n",
    "    train_labels = pickle.load(open(READY_DATASET_PATH + \"train_labels.pickle\", \"rb\"))\n",
    "else:\n",
    "    pickle_out = open(READY_DATASET_PATH + \"train_labels.pickle\",'wb')\n",
    "    pickle.dump(train_labels,pickle_out)\n",
    "    pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(READY_DATASET_PATH + \"valid_set_padded.pickle\") and not USE_READY_DATA:\n",
    "    print(\"Using Pickle File!\")\n",
    "    valid_set_padded = pickle.load(open(READY_DATASET_PATH + \"valid_set_padded.pickle\", \"rb\"))\n",
    "else:\n",
    "    pickle_out = open(READY_DATASET_PATH + \"valid_set_padded.pickle\",'wb')\n",
    "    pickle.dump(valid_set_padded,pickle_out)\n",
    "    pickle_out.close()\n",
    "    \n",
    "if os.path.exists(READY_DATASET_PATH + \"validation_labels.pickle\") and not USE_READY_DATA:\n",
    "    print(\"Using Pickle File!\")\n",
    "    validation_labels = pickle.load(open(READY_DATASET_PATH + \"validation_labels.pickle\", \"rb\"))\n",
    "else:\n",
    "    pickle_out = open(READY_DATASET_PATH + \"validation_labels.pickle\",'wb')\n",
    "    pickle.dump(validation_labels,pickle_out)\n",
    "    pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(READY_DATASET_PATH + \"test_set_padded.pickle\") and not USE_READY_DATA:\n",
    "    print(\"Using Pickle File!\")\n",
    "    test_set_padded = pickle.load(open(READY_DATASET_PATH + \"test_set_padded.pickle\", \"rb\"))\n",
    "else:\n",
    "    pickle_out = open(READY_DATASET_PATH + \"test_set_padded.pickle\",'wb')\n",
    "    pickle.dump(test_set_padded,pickle_out)\n",
    "    pickle_out.close()\n",
    "    \n",
    "if os.path.exists(READY_DATASET_PATH + \"test_labels.pickle\") and not USE_READY_DATA:\n",
    "    print(\"Using Pickle File!\")\n",
    "    test_labels = pickle.load(open(READY_DATASET_PATH + \"test_labels.pickle\", \"rb\"))\n",
    "else:\n",
    "    pickle_out = open(READY_DATASET_PATH + \"test_labels.pickle\",'wb')\n",
    "    pickle.dump(test_labels,pickle_out)\n",
    "    pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling Embedding Layer!\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(READY_DATASET_PATH + \"EMBEDDING_LAYER.pickle\") and not USE_READY_DATA:\n",
    "    print(\"Using Pickle File!\")\n",
    "    EMBEDDING_LAYER = pickle.load(open(READY_DATASET_PATH + \"EMBEDDING_LAYER.pickle\", \"rb\"))\n",
    "else:\n",
    "    print(\"Pickling Embedding Layer!\")\n",
    "    pickle_out = open(READY_DATASET_PATH + \"EMBEDDING_LAYER.pickle\",'wb')\n",
    "    pickle.dump(EMBEDDING_LAYER,pickle_out)\n",
    "    pickle_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-02 23:39:39.866211: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-02 23:39:39.874887: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-02 23:39:39.875006: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-02 23:39:40.232614: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-02 23:39:40.232784: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-02 23:39:40.232887: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-02 23:39:40.232979: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22077 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_9059/775785777.py:4: The name tf.keras.backend.set_session is deprecated. Please use tf.compat.v1.keras.backend.set_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_9059/775785777.py:15: The name tf.keras.layers.CuDNNLSTM is deprecated. Please use tf.compat.v1.keras.layers.CuDNNLSTM instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-02 23:39:40.441526: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-02 23:39:40.441673: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-02 23:39:40.441749: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-02 23:39:40.441941: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-02 23:39:40.442022: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-02 23:39:40.442093: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-02 23:39:40.442194: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-02 23:39:40.442268: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-02 23:39:40.442320: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22077 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "configproto = tf.compat.v1.ConfigProto() \n",
    "configproto.gpu_options.allow_growth = True\n",
    "sess = tf.compat.v1.Session(config=configproto) \n",
    "tf.compat.v1.keras.backend.set_session(sess)\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(RANDOM_STATE)\n",
    "np.random.seed(RANDOM_STATE)\n",
    "inputs = tf.keras.Input(shape=(MAX_TEXT_LEN,))\n",
    "x = EMBEDDING_LAYER(inputs)\n",
    "x = tf.keras.layers.Normalization()(x)\n",
    "x = tf.keras.layers.Conv1D(filters=CONV_FILTERS, kernel_size=CONV_KERNEL_SIZE, padding='same', activation='relu')(x)\n",
    "x = tf.keras.layers.GlobalMaxPooling1D(keepdims=True)(x)\n",
    "x = tf.keras.layers.BatchNormalization(synchronized=True)(x)\n",
    "x = tf.keras.layers.Bidirectional(tf.compat.v1.keras.layers.CuDNNLSTM(int(LSTM_UNITS), return_sequences=True))(x)\n",
    "x = tf.keras.layers.Dropout(DROPOUT_VAL)(x)\n",
    "x = tf.keras.layers.Bidirectional(tf.compat.v1.keras.layers.CuDNNLSTM(int(LSTM_UNITS/2)))(x)\n",
    "x = tf.keras.layers.Dropout(DROPOUT_VAL)(x)\n",
    "x = tf.keras.layers.Dense(DENSE_UNITS, activation=\"relu\")(x)\n",
    "outputs = tf.keras.layers.Dense(LABELS_COUNT, activation=\"softmax\")(x)\n",
    "\n",
    "lstm_model = tf.keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 250)]             0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 250, 100)          11364400  \n",
      "                                                                 \n",
      " normalization (Normalizatio  (None, 250, 100)         201       \n",
      " n)                                                              \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 250, 60)           30060     \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 1, 60)            0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 1, 60)            240       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 1, 512)           651264    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1, 512)            0         \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 256)              657408    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1024)              263168    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 3075      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12,969,816\n",
      "Trainable params: 12,969,495\n",
      "Non-trainable params: 321\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"./logs\",histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_callback = MetricsCallback(test_data = valid_set_padded, y_true = validation_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-02 23:39:43.151085: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8600\n",
      "2023-06-02 23:39:43.731774: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:637] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2023-06-02 23:39:43.732599: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x7f97d4824750 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-06-02 23:39:43.732617: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA GeForce RTX 4090, Compute Capability 8.9\n",
      "2023-06-02 23:39:43.735511: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-06-02 23:39:43.822721: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1523/1523 [==============================] - 2s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.54      0.51     16156\n",
      "           1       0.49      0.57      0.53     16401\n",
      "           2       0.44      0.33      0.38     16171\n",
      "\n",
      "    accuracy                           0.48     48728\n",
      "   macro avg       0.48      0.48      0.47     48728\n",
      "weighted avg       0.48      0.48      0.47     48728\n",
      "\n",
      "Macro Weighted F1-Score: 0.4745112593384241\n",
      "3046/3046 [==============================] - 84s 26ms/step - loss: 1.0710 - accuracy: 0.4174 - val_loss: 1.0233 - val_accuracy: 0.4804 - lr: 1.0000e-05\n",
      "Epoch 2/60\n",
      "1523/1523 [==============================] - 2s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.62      0.58     16156\n",
      "           1       0.57      0.60      0.58     16401\n",
      "           2       0.47      0.38      0.42     16171\n",
      "\n",
      "    accuracy                           0.53     48728\n",
      "   macro avg       0.53      0.53      0.53     48728\n",
      "weighted avg       0.53      0.53      0.53     48728\n",
      "\n",
      "Macro Weighted F1-Score: 0.5281545749983894\n",
      "3046/3046 [==============================] - 27s 9ms/step - loss: 0.9846 - accuracy: 0.5101 - val_loss: 0.9523 - val_accuracy: 0.5332 - lr: 1.0000e-05\n",
      "Epoch 3/60\n",
      "1523/1523 [==============================] - 2s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.65      0.62     16156\n",
      "           1       0.60      0.63      0.62     16401\n",
      "           2       0.49      0.41      0.44     16171\n",
      "\n",
      "    accuracy                           0.56     48728\n",
      "   macro avg       0.56      0.56      0.56     48728\n",
      "weighted avg       0.56      0.56      0.56     48728\n",
      "\n",
      "Macro Weighted F1-Score: 0.5597760705208203\n",
      "3046/3046 [==============================] - 22s 7ms/step - loss: 0.9299 - accuracy: 0.5496 - val_loss: 0.9079 - val_accuracy: 0.5647 - lr: 1.0000e-05\n",
      "Epoch 4/60\n",
      "1523/1523 [==============================] - 2s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.67      0.64     16156\n",
      "           1       0.63      0.65      0.64     16401\n",
      "           2       0.50      0.44      0.47     16171\n",
      "\n",
      "    accuracy                           0.59     48728\n",
      "   macro avg       0.58      0.59      0.58     48728\n",
      "weighted avg       0.58      0.59      0.58     48728\n",
      "\n",
      "Macro Weighted F1-Score: 0.5834095794112525\n",
      "3046/3046 [==============================] - 21s 7ms/step - loss: 0.8881 - accuracy: 0.5763 - val_loss: 0.8722 - val_accuracy: 0.5869 - lr: 1.0000e-05\n",
      "Epoch 5/60\n",
      "1523/1523 [==============================] - 2s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.68      0.66     16156\n",
      "           1       0.65      0.66      0.66     16401\n",
      "           2       0.51      0.47      0.49     16171\n",
      "\n",
      "    accuracy                           0.60     48728\n",
      "   macro avg       0.60      0.60      0.60     48728\n",
      "weighted avg       0.60      0.60      0.60     48728\n",
      "\n",
      "Macro Weighted F1-Score: 0.6019592554524908\n",
      "3046/3046 [==============================] - 21s 7ms/step - loss: 0.8546 - accuracy: 0.5971 - val_loss: 0.8431 - val_accuracy: 0.6044 - lr: 1.0000e-05\n",
      "Epoch 6/60\n",
      "1523/1523 [==============================] - 2s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.70      0.68     16156\n",
      "           1       0.66      0.68      0.67     16401\n",
      "           2       0.52      0.48      0.50     16171\n",
      "\n",
      "    accuracy                           0.62     48728\n",
      "   macro avg       0.61      0.62      0.62     48728\n",
      "weighted avg       0.61      0.62      0.62     48728\n",
      "\n",
      "Macro Weighted F1-Score: 0.6160269207210645\n",
      "3046/3046 [==============================] - 21s 7ms/step - loss: 0.8265 - accuracy: 0.6138 - val_loss: 0.8192 - val_accuracy: 0.6181 - lr: 1.0000e-05\n",
      "Epoch 7/60\n",
      "1523/1523 [==============================] - 2s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.71      0.69     16156\n",
      "           1       0.68      0.69      0.68     16401\n",
      "           2       0.53      0.49      0.51     16171\n",
      "\n",
      "    accuracy                           0.63     48728\n",
      "   macro avg       0.63      0.63      0.63     48728\n",
      "weighted avg       0.63      0.63      0.63     48728\n",
      "\n",
      "Macro Weighted F1-Score: 0.6273262418155796\n",
      "3046/3046 [==============================] - 20s 6ms/step - loss: 0.8035 - accuracy: 0.6262 - val_loss: 0.7997 - val_accuracy: 0.6294 - lr: 1.0000e-05\n",
      "Epoch 8/60\n",
      "1523/1523 [==============================] - 2s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.71      0.70     16156\n",
      "           1       0.68      0.69      0.69     16401\n",
      "           2       0.54      0.51      0.52     16171\n",
      "\n",
      "    accuracy                           0.64     48728\n",
      "   macro avg       0.64      0.64      0.64     48728\n",
      "weighted avg       0.64      0.64      0.64     48728\n",
      "\n",
      "Macro Weighted F1-Score: 0.6363617946961205\n",
      "3046/3046 [==============================] - 19s 6ms/step - loss: 0.7835 - accuracy: 0.6380 - val_loss: 0.7835 - val_accuracy: 0.6379 - lr: 1.0000e-05\n",
      "Epoch 9/60\n",
      "1523/1523 [==============================] - 2s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.71      0.71     16156\n",
      "           1       0.69      0.70      0.70     16401\n",
      "           2       0.54      0.53      0.53     16171\n",
      "\n",
      "    accuracy                           0.65     48728\n",
      "   macro avg       0.64      0.65      0.64     48728\n",
      "weighted avg       0.64      0.65      0.65     48728\n",
      "\n",
      "Macro Weighted F1-Score: 0.6451376075370456\n",
      "3046/3046 [==============================] - 19s 6ms/step - loss: 0.7673 - accuracy: 0.6478 - val_loss: 0.7696 - val_accuracy: 0.6458 - lr: 1.0000e-05\n",
      "Epoch 10/60\n",
      "1523/1523 [==============================] - 2s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.73      0.71     16156\n",
      "           1       0.70      0.70      0.70     16401\n",
      "           2       0.55      0.53      0.54     16171\n",
      "\n",
      "    accuracy                           0.65     48728\n",
      "   macro avg       0.65      0.65      0.65     48728\n",
      "weighted avg       0.65      0.65      0.65     48728\n",
      "\n",
      "Macro Weighted F1-Score: 0.6522989356341605\n",
      "3046/3046 [==============================] - 20s 6ms/step - loss: 0.7529 - accuracy: 0.6553 - val_loss: 0.7577 - val_accuracy: 0.6532 - lr: 1.0000e-05\n",
      "Epoch 11/60\n",
      "1523/1523 [==============================] - 2s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.73      0.72     16156\n",
      "           1       0.71      0.70      0.71     16401\n",
      "           2       0.55      0.54      0.55     16171\n",
      "\n",
      "    accuracy                           0.66     48728\n",
      "   macro avg       0.66      0.66      0.66     48728\n",
      "weighted avg       0.66      0.66      0.66     48728\n",
      "\n",
      "Macro Weighted F1-Score: 0.6582415638882415\n",
      "3046/3046 [==============================] - 19s 6ms/step - loss: 0.7403 - accuracy: 0.6629 - val_loss: 0.7474 - val_accuracy: 0.6587 - lr: 1.0000e-05\n",
      "Epoch 12/60\n",
      "1523/1523 [==============================] - 2s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.73      0.73     16156\n",
      "           1       0.71      0.71      0.71     16401\n",
      "           2       0.56      0.54      0.55     16171\n",
      "\n",
      "    accuracy                           0.66     48728\n",
      "   macro avg       0.66      0.66      0.66     48728\n",
      "weighted avg       0.66      0.66      0.66     48728\n",
      "\n",
      "Macro Weighted F1-Score: 0.6632795228147895\n",
      "3046/3046 [==============================] - 20s 6ms/step - loss: 0.7294 - accuracy: 0.6689 - val_loss: 0.7380 - val_accuracy: 0.6640 - lr: 1.0000e-05\n",
      "Epoch 13/60\n",
      "1523/1523 [==============================] - 2s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.73      0.73     16156\n",
      "           1       0.72      0.71      0.71     16401\n",
      "           2       0.56      0.56      0.56     16171\n",
      "\n",
      "    accuracy                           0.67     48728\n",
      "   macro avg       0.67      0.67      0.67     48728\n",
      "weighted avg       0.67      0.67      0.67     48728\n",
      "\n",
      "Macro Weighted F1-Score: 0.6675655823461618\n",
      "3046/3046 [==============================] - 19s 6ms/step - loss: 0.7191 - accuracy: 0.6740 - val_loss: 0.7300 - val_accuracy: 0.6675 - lr: 1.0000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/60\n",
      "1523/1523 [==============================] - 2s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.74      0.73     16156\n",
      "           1       0.72      0.72      0.72     16401\n",
      "           2       0.57      0.55      0.56     16171\n",
      "\n",
      "    accuracy                           0.67     48728\n",
      "   macro avg       0.67      0.67      0.67     48728\n",
      "weighted avg       0.67      0.67      0.67     48728\n",
      "\n",
      "Macro Weighted F1-Score: 0.6716549464276709\n",
      "3046/3046 [==============================] - 19s 6ms/step - loss: 0.7099 - accuracy: 0.6794 - val_loss: 0.7224 - val_accuracy: 0.6722 - lr: 1.0000e-05\n",
      "Epoch 15/60\n",
      "1523/1523 [==============================] - 2s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.74      0.74     16156\n",
      "           1       0.72      0.72      0.72     16401\n",
      "           2       0.57      0.56      0.57     16171\n",
      "\n",
      "    accuracy                           0.68     48728\n",
      "   macro avg       0.68      0.68      0.68     48728\n",
      "weighted avg       0.68      0.68      0.68     48728\n",
      "\n",
      "Macro Weighted F1-Score: 0.676236408029418\n",
      "3046/3046 [==============================] - 19s 6ms/step - loss: 0.7020 - accuracy: 0.6831 - val_loss: 0.7158 - val_accuracy: 0.6765 - lr: 1.0000e-05\n",
      "Epoch 16/60\n",
      "1523/1523 [==============================] - 2s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.75      0.74     16156\n",
      "           1       0.73      0.72      0.73     16401\n",
      "           2       0.57      0.56      0.57     16171\n",
      "\n",
      "    accuracy                           0.68     48728\n",
      "   macro avg       0.68      0.68      0.68     48728\n",
      "weighted avg       0.68      0.68      0.68     48728\n",
      "\n",
      "Macro Weighted F1-Score: 0.678578230470436\n",
      "3046/3046 [==============================] - 19s 6ms/step - loss: 0.6940 - accuracy: 0.6873 - val_loss: 0.7101 - val_accuracy: 0.6794 - lr: 1.0000e-05\n",
      "Epoch 17/60\n",
      "1523/1523 [==============================] - 2s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.74      0.74     16156\n",
      "           1       0.73      0.74      0.73     16401\n",
      "           2       0.57      0.57      0.57     16171\n",
      "\n",
      "    accuracy                           0.68     48728\n",
      "   macro avg       0.68      0.68      0.68     48728\n",
      "weighted avg       0.68      0.68      0.68     48728\n",
      "\n",
      "Macro Weighted F1-Score: 0.6820234074970776\n",
      "3046/3046 [==============================] - 19s 6ms/step - loss: 0.6872 - accuracy: 0.6909 - val_loss: 0.7043 - val_accuracy: 0.6823 - lr: 1.0000e-05\n",
      "Epoch 18/60\n",
      "1523/1523 [==============================] - 2s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.75      0.75     16156\n",
      "           1       0.73      0.73      0.73     16401\n",
      "           2       0.58      0.57      0.57     16171\n",
      "\n",
      "    accuracy                           0.68     48728\n",
      "   macro avg       0.68      0.68      0.68     48728\n",
      "weighted avg       0.68      0.68      0.68     48728\n",
      "\n",
      "Macro Weighted F1-Score: 0.6845064147547331\n",
      "3046/3046 [==============================] - 19s 6ms/step - loss: 0.6802 - accuracy: 0.6948 - val_loss: 0.6990 - val_accuracy: 0.6849 - lr: 1.0000e-05\n",
      "Epoch 19/60\n",
      "1523/1523 [==============================] - 2s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.75      0.75     16156\n",
      "           1       0.73      0.74      0.74     16401\n",
      "           2       0.58      0.57      0.58     16171\n",
      "\n",
      "    accuracy                           0.69     48728\n",
      "   macro avg       0.69      0.69      0.69     48728\n",
      "weighted avg       0.69      0.69      0.69     48728\n",
      "\n",
      "Macro Weighted F1-Score: 0.6873377062383726\n",
      "3046/3046 [==============================] - 19s 6ms/step - loss: 0.6745 - accuracy: 0.6981 - val_loss: 0.6944 - val_accuracy: 0.6877 - lr: 1.0000e-05\n",
      "Epoch 20/60\n",
      "1523/1523 [==============================] - 2s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.75      0.75     16156\n",
      "           1       0.74      0.73      0.74     16401\n",
      "           2       0.58      0.58      0.58     16171\n",
      "\n",
      "    accuracy                           0.69     48728\n",
      "   macro avg       0.69      0.69      0.69     48728\n",
      "weighted avg       0.69      0.69      0.69     48728\n",
      "\n",
      "Macro Weighted F1-Score: 0.6899655105085646\n",
      "3046/3046 [==============================] - 19s 6ms/step - loss: 0.6682 - accuracy: 0.7012 - val_loss: 0.6902 - val_accuracy: 0.6898 - lr: 1.0000e-05\n",
      "Epoch 21/60\n",
      "1523/1523 [==============================] - 2s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.76      0.75     16156\n",
      "           1       0.74      0.73      0.74     16401\n",
      "           2       0.58      0.58      0.58     16171\n",
      "\n",
      "    accuracy                           0.69     48728\n",
      "   macro avg       0.69      0.69      0.69     48728\n",
      "weighted avg       0.69      0.69      0.69     48728\n",
      "\n",
      "Macro Weighted F1-Score: 0.6915299087040944\n",
      "3046/3046 [==============================] - 19s 6ms/step - loss: 0.6626 - accuracy: 0.7042 - val_loss: 0.6862 - val_accuracy: 0.6915 - lr: 1.0000e-05\n",
      "Epoch 22/60\n",
      "1523/1523 [==============================] - 2s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.76      0.76     16156\n",
      "           1       0.74      0.74      0.74     16401\n",
      "           2       0.59      0.58      0.58     16171\n",
      "\n",
      "    accuracy                           0.69     48728\n",
      "   macro avg       0.69      0.69      0.69     48728\n",
      "weighted avg       0.69      0.69      0.69     48728\n",
      "\n",
      "Macro Weighted F1-Score: 0.6931406465803697\n",
      "3046/3046 [==============================] - 18s 6ms/step - loss: 0.6577 - accuracy: 0.7062 - val_loss: 0.6826 - val_accuracy: 0.6933 - lr: 1.0000e-05\n",
      "Epoch 23/60\n",
      "1523/1523 [==============================] - 2s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.76      0.76     16156\n",
      "           1       0.74      0.74      0.74     16401\n",
      "           2       0.59      0.59      0.59     16171\n",
      "\n",
      "    accuracy                           0.70     48728\n",
      "   macro avg       0.70      0.70      0.70     48728\n",
      "weighted avg       0.70      0.70      0.70     48728\n",
      "\n",
      "Macro Weighted F1-Score: 0.6954674906957008\n",
      "3046/3046 [==============================] - 19s 6ms/step - loss: 0.6527 - accuracy: 0.7090 - val_loss: 0.6790 - val_accuracy: 0.6954 - lr: 1.0000e-05\n",
      "Epoch 24/60\n",
      "1523/1523 [==============================] - 2s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.76      0.76     16156\n",
      "           1       0.74      0.75      0.74     16401\n",
      "           2       0.59      0.59      0.59     16171\n",
      "\n",
      "    accuracy                           0.70     48728\n",
      "   macro avg       0.70      0.70      0.70     48728\n",
      "weighted avg       0.70      0.70      0.70     48728\n",
      "\n",
      "Macro Weighted F1-Score: 0.6969109661296645\n",
      "3046/3046 [==============================] - 19s 6ms/step - loss: 0.6476 - accuracy: 0.7118 - val_loss: 0.6763 - val_accuracy: 0.6971 - lr: 1.0000e-05\n",
      "Epoch 25/60\n",
      "1523/1523 [==============================] - 2s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.76      0.76     16156\n",
      "           1       0.74      0.74      0.74     16401\n",
      "           2       0.59      0.59      0.59     16171\n",
      "\n",
      "    accuracy                           0.70     48728\n",
      "   macro avg       0.70      0.70      0.70     48728\n",
      "weighted avg       0.70      0.70      0.70     48728\n",
      "\n",
      "Macro Weighted F1-Score: 0.698904203573897\n",
      "3046/3046 [==============================] - 19s 6ms/step - loss: 0.6437 - accuracy: 0.7148 - val_loss: 0.6734 - val_accuracy: 0.6990 - lr: 1.0000e-05\n",
      "Epoch 26/60\n",
      "1523/1523 [==============================] - 2s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.76      0.76     16156\n",
      "           1       0.75      0.74      0.74     16401\n",
      "           2       0.59      0.59      0.59     16171\n",
      "\n",
      "    accuracy                           0.70     48728\n",
      "   macro avg       0.70      0.70      0.70     48728\n",
      "weighted avg       0.70      0.70      0.70     48728\n",
      "\n",
      "Macro Weighted F1-Score: 0.700506729652006\n",
      "3046/3046 [==============================] - 19s 6ms/step - loss: 0.6390 - accuracy: 0.7167 - val_loss: 0.6706 - val_accuracy: 0.7004 - lr: 1.0000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/60\n",
      "1523/1523 [==============================] - 2s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.77      0.77     16156\n",
      "           1       0.75      0.74      0.75     16401\n",
      "           2       0.59      0.59      0.59     16171\n",
      "\n",
      "    accuracy                           0.70     48728\n",
      "   macro avg       0.70      0.70      0.70     48728\n",
      "weighted avg       0.70      0.70      0.70     48728\n",
      "\n",
      "Macro Weighted F1-Score: 0.7018196891862206\n",
      "3046/3046 [==============================] - 19s 6ms/step - loss: 0.6349 - accuracy: 0.7186 - val_loss: 0.6680 - val_accuracy: 0.7019 - lr: 1.0000e-05\n",
      "Epoch 28/60\n",
      "1523/1523 [==============================] - 2s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.77      0.77     16156\n",
      "           1       0.75      0.75      0.75     16401\n",
      "           2       0.59      0.60      0.60     16171\n",
      "\n",
      "    accuracy                           0.70     48728\n",
      "   macro avg       0.70      0.70      0.70     48728\n",
      "weighted avg       0.70      0.70      0.70     48728\n",
      "\n",
      "Macro Weighted F1-Score: 0.7034865576446584\n",
      "3046/3046 [==============================] - 19s 6ms/step - loss: 0.6312 - accuracy: 0.7200 - val_loss: 0.6656 - val_accuracy: 0.7034 - lr: 1.0000e-05\n",
      "Epoch 29/60\n",
      "1523/1523 [==============================] - 2s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.77      0.77     16156\n",
      "           1       0.75      0.74      0.75     16401\n",
      "           2       0.59      0.60      0.60     16171\n",
      "\n",
      "    accuracy                           0.70     48728\n",
      "   macro avg       0.70      0.70      0.70     48728\n",
      "weighted avg       0.70      0.70      0.70     48728\n",
      "\n",
      "Macro Weighted F1-Score: 0.7042764352617006\n",
      "3046/3046 [==============================] - 18s 6ms/step - loss: 0.6277 - accuracy: 0.7226 - val_loss: 0.6634 - val_accuracy: 0.7042 - lr: 1.0000e-05\n",
      "Epoch 30/60\n",
      "1523/1523 [==============================] - 2s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.77      0.77     16156\n",
      "           1       0.75      0.75      0.75     16401\n",
      "           2       0.60      0.60      0.60     16171\n",
      "\n",
      "    accuracy                           0.71     48728\n",
      "   macro avg       0.71      0.71      0.71     48728\n",
      "weighted avg       0.71      0.71      0.71     48728\n",
      "\n",
      "Macro Weighted F1-Score: 0.7053703900255622\n",
      "3046/3046 [==============================] - 19s 6ms/step - loss: 0.6238 - accuracy: 0.7239 - val_loss: 0.6615 - val_accuracy: 0.7054 - lr: 1.0000e-05\n",
      "Epoch 31/60\n",
      "1523/1523 [==============================] - 2s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.77      0.77     16156\n",
      "           1       0.75      0.76      0.75     16401\n",
      "           2       0.60      0.59      0.59     16171\n",
      "\n",
      "    accuracy                           0.71     48728\n",
      "   macro avg       0.71      0.71      0.71     48728\n",
      "weighted avg       0.71      0.71      0.71     48728\n",
      "\n",
      "Macro Weighted F1-Score: 0.7056688766385159\n",
      "3046/3046 [==============================] - 19s 6ms/step - loss: 0.6209 - accuracy: 0.7260 - val_loss: 0.6599 - val_accuracy: 0.7060 - lr: 1.0000e-05\n",
      "Epoch 32/60\n",
      "1523/1523 [==============================] - 2s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.77      0.77     16156\n",
      "           1       0.75      0.75      0.75     16401\n",
      "           2       0.60      0.60      0.60     16171\n",
      "\n",
      "    accuracy                           0.71     48728\n",
      "   macro avg       0.71      0.71      0.71     48728\n",
      "weighted avg       0.71      0.71      0.71     48728\n",
      "\n",
      "Macro Weighted F1-Score: 0.7071931895771655\n",
      "3046/3046 [==============================] - 19s 6ms/step - loss: 0.6171 - accuracy: 0.7278 - val_loss: 0.6576 - val_accuracy: 0.7070 - lr: 1.0000e-05\n",
      "Epoch 33/60\n",
      "1523/1523 [==============================] - 2s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.77      0.77     16156\n",
      "           1       0.75      0.75      0.75     16401\n",
      "           2       0.60      0.60      0.60     16171\n",
      "\n",
      "    accuracy                           0.71     48728\n",
      "   macro avg       0.71      0.71      0.71     48728\n",
      "weighted avg       0.71      0.71      0.71     48728\n",
      "\n",
      "Macro Weighted F1-Score: 0.7078163425628539\n",
      "3046/3046 [==============================] - 19s 6ms/step - loss: 0.6140 - accuracy: 0.7298 - val_loss: 0.6560 - val_accuracy: 0.7078 - lr: 1.0000e-05\n",
      "Epoch 34/60\n",
      "1523/1523 [==============================] - 2s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.77      0.77     16156\n",
      "           1       0.75      0.75      0.75     16401\n",
      "           2       0.60      0.60      0.60     16171\n",
      "\n",
      "    accuracy                           0.71     48728\n",
      "   macro avg       0.71      0.71      0.71     48728\n",
      "weighted avg       0.71      0.71      0.71     48728\n",
      "\n",
      "Macro Weighted F1-Score: 0.7085000087075808\n",
      "3046/3046 [==============================] - 19s 6ms/step - loss: 0.6109 - accuracy: 0.7312 - val_loss: 0.6553 - val_accuracy: 0.7083 - lr: 1.0000e-05\n",
      "Epoch 35/60\n",
      "1523/1523 [==============================] - 2s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.78      0.77     16156\n",
      "           1       0.75      0.75      0.75     16401\n",
      "           2       0.60      0.60      0.60     16171\n",
      "\n",
      "    accuracy                           0.71     48728\n",
      "   macro avg       0.71      0.71      0.71     48728\n",
      "weighted avg       0.71      0.71      0.71     48728\n",
      "\n",
      "Macro Weighted F1-Score: 0.7085851133661727\n",
      "3046/3046 [==============================] - 19s 6ms/step - loss: 0.6080 - accuracy: 0.7321 - val_loss: 0.6536 - val_accuracy: 0.7089 - lr: 1.0000e-05\n",
      "Epoch 36/60\n",
      "1523/1523 [==============================] - 2s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.77      0.77     16156\n",
      "           1       0.76      0.75      0.75     16401\n",
      "           2       0.60      0.61      0.60     16171\n",
      "\n",
      "    accuracy                           0.71     48728\n",
      "   macro avg       0.71      0.71      0.71     48728\n",
      "weighted avg       0.71      0.71      0.71     48728\n",
      "\n",
      "Macro Weighted F1-Score: 0.7095693186636028\n",
      "3046/3046 [==============================] - 19s 6ms/step - loss: 0.6052 - accuracy: 0.7341 - val_loss: 0.6520 - val_accuracy: 0.7090 - lr: 1.0000e-05\n",
      "Epoch 37/60\n",
      "1523/1523 [==============================] - 2s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.78      0.77     16156\n",
      "           1       0.76      0.75      0.75     16401\n",
      "           2       0.60      0.60      0.60     16171\n",
      "\n",
      "    accuracy                           0.71     48728\n",
      "   macro avg       0.71      0.71      0.71     48728\n",
      "weighted avg       0.71      0.71      0.71     48728\n",
      "\n",
      "Macro Weighted F1-Score: 0.7105516657470219\n",
      "3046/3046 [==============================] - 18s 6ms/step - loss: 0.6026 - accuracy: 0.7354 - val_loss: 0.6506 - val_accuracy: 0.7106 - lr: 1.0000e-05\n",
      "Epoch 38/60\n",
      "1523/1523 [==============================] - 2s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.77      0.77     16156\n",
      "           1       0.75      0.76      0.76     16401\n",
      "           2       0.60      0.60      0.60     16171\n",
      "\n",
      "    accuracy                           0.71     48728\n",
      "   macro avg       0.71      0.71      0.71     48728\n",
      "weighted avg       0.71      0.71      0.71     48728\n",
      "\n",
      "Macro Weighted F1-Score: 0.7113112093589355\n",
      "3046/3046 [==============================] - 19s 6ms/step - loss: 0.5998 - accuracy: 0.7367 - val_loss: 0.6497 - val_accuracy: 0.7115 - lr: 1.0000e-05\n",
      "Epoch 39/60\n",
      "1523/1523 [==============================] - 2s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.77      0.78     16156\n",
      "           1       0.76      0.75      0.76     16401\n",
      "           2       0.60      0.61      0.60     16171\n",
      "\n",
      "    accuracy                           0.71     48728\n",
      "   macro avg       0.71      0.71      0.71     48728\n",
      "weighted avg       0.71      0.71      0.71     48728\n",
      "\n",
      "Macro Weighted F1-Score: 0.7123341844632741\n",
      "3046/3046 [==============================] - 19s 6ms/step - loss: 0.5976 - accuracy: 0.7380 - val_loss: 0.6484 - val_accuracy: 0.7122 - lr: 1.0000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/60\n",
      "1523/1523 [==============================] - 2s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.78      0.78     16156\n",
      "           1       0.76      0.76      0.76     16401\n",
      "           2       0.60      0.60      0.60     16171\n",
      "\n",
      "    accuracy                           0.71     48728\n",
      "   macro avg       0.71      0.71      0.71     48728\n",
      "weighted avg       0.71      0.71      0.71     48728\n",
      "\n",
      "Macro Weighted F1-Score: 0.7126953049805684\n",
      "3046/3046 [==============================] - 18s 6ms/step - loss: 0.5948 - accuracy: 0.7393 - val_loss: 0.6478 - val_accuracy: 0.7127 - lr: 1.0000e-05\n",
      "Epoch 41/60\n",
      "1523/1523 [==============================] - 2s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.78      0.78     16156\n",
      "           1       0.76      0.76      0.76     16401\n",
      "           2       0.61      0.60      0.60     16171\n",
      "\n",
      "    accuracy                           0.71     48728\n",
      "   macro avg       0.71      0.71      0.71     48728\n",
      "weighted avg       0.71      0.71      0.71     48728\n",
      "\n",
      "Macro Weighted F1-Score: 0.7137226766694452\n",
      "3046/3046 [==============================] - 19s 6ms/step - loss: 0.5918 - accuracy: 0.7412 - val_loss: 0.6467 - val_accuracy: 0.7140 - lr: 1.0000e-05\n",
      "Epoch 42/60\n",
      "1523/1523 [==============================] - 2s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.78      0.78     16156\n",
      "           1       0.76      0.76      0.76     16401\n",
      "           2       0.61      0.60      0.61     16171\n",
      "\n",
      "    accuracy                           0.71     48728\n",
      "   macro avg       0.71      0.71      0.71     48728\n",
      "weighted avg       0.71      0.71      0.71     48728\n",
      "\n",
      "Macro Weighted F1-Score: 0.714263374399634\n",
      "3046/3046 [==============================] - 19s 6ms/step - loss: 0.5899 - accuracy: 0.7415 - val_loss: 0.6463 - val_accuracy: 0.7144 - lr: 1.0000e-05\n",
      "Epoch 43/60\n",
      "1523/1523 [==============================] - 2s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.78      0.78     16156\n",
      "           1       0.76      0.76      0.76     16401\n",
      "           2       0.61      0.61      0.61     16171\n",
      "\n",
      "    accuracy                           0.72     48728\n",
      "   macro avg       0.72      0.72      0.72     48728\n",
      "weighted avg       0.72      0.72      0.72     48728\n",
      "\n",
      "Macro Weighted F1-Score: 0.715398497689484\n",
      "3046/3046 [==============================] - 19s 6ms/step - loss: 0.5874 - accuracy: 0.7433 - val_loss: 0.6452 - val_accuracy: 0.7154 - lr: 1.0000e-05\n",
      "Epoch 44/60\n",
      "1523/1523 [==============================] - 2s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.78      0.78     16156\n",
      "           1       0.76      0.76      0.76     16401\n",
      "           2       0.61      0.61      0.61     16171\n",
      "\n",
      "    accuracy                           0.72     48728\n",
      "   macro avg       0.72      0.72      0.72     48728\n",
      "weighted avg       0.72      0.72      0.72     48728\n",
      "\n",
      "Macro Weighted F1-Score: 0.7165285951704332\n",
      "3046/3046 [==============================] - 19s 6ms/step - loss: 0.5850 - accuracy: 0.7440 - val_loss: 0.6439 - val_accuracy: 0.7163 - lr: 1.0000e-05\n",
      "Epoch 45/60\n",
      "1523/1523 [==============================] - 2s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.78      0.78     16156\n",
      "           1       0.76      0.76      0.76     16401\n",
      "           2       0.61      0.60      0.61     16171\n",
      "\n",
      "    accuracy                           0.72     48728\n",
      "   macro avg       0.72      0.72      0.72     48728\n",
      "weighted avg       0.72      0.72      0.72     48728\n",
      "\n",
      "Macro Weighted F1-Score: 0.7164051669899032\n",
      "3046/3046 [==============================] - 18s 6ms/step - loss: 0.5825 - accuracy: 0.7456 - val_loss: 0.6438 - val_accuracy: 0.7168 - lr: 1.0000e-05\n",
      "Epoch 46/60\n",
      "1523/1523 [==============================] - 2s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.78      0.78     16156\n",
      "           1       0.76      0.76      0.76     16401\n",
      "           2       0.61      0.61      0.61     16171\n",
      "\n",
      "    accuracy                           0.72     48728\n",
      "   macro avg       0.72      0.72      0.72     48728\n",
      "weighted avg       0.72      0.72      0.72     48728\n",
      "\n",
      "Macro Weighted F1-Score: 0.7174396323260798\n",
      "3046/3046 [==============================] - 18s 6ms/step - loss: 0.5802 - accuracy: 0.7466 - val_loss: 0.6434 - val_accuracy: 0.7175 - lr: 1.0000e-05\n",
      "Epoch 47/60\n",
      "1523/1523 [==============================] - 2s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.78      0.78     16156\n",
      "           1       0.76      0.76      0.76     16401\n",
      "           2       0.61      0.61      0.61     16171\n",
      "\n",
      "    accuracy                           0.72     48728\n",
      "   macro avg       0.72      0.72      0.72     48728\n",
      "weighted avg       0.72      0.72      0.72     48728\n",
      "\n",
      "Macro Weighted F1-Score: 0.718279735393537\n",
      "3046/3046 [==============================] - 18s 6ms/step - loss: 0.5780 - accuracy: 0.7479 - val_loss: 0.6426 - val_accuracy: 0.7183 - lr: 1.0000e-05\n",
      "Epoch 48/60\n",
      "1523/1523 [==============================] - 2s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.79      0.78     16156\n",
      "           1       0.76      0.76      0.76     16401\n",
      "           2       0.61      0.60      0.61     16171\n",
      "\n",
      "    accuracy                           0.72     48728\n",
      "   macro avg       0.72      0.72      0.72     48728\n",
      "weighted avg       0.72      0.72      0.72     48728\n",
      "\n",
      "Macro Weighted F1-Score: 0.7175964543022749\n",
      "3046/3046 [==============================] - 18s 6ms/step - loss: 0.5755 - accuracy: 0.7487 - val_loss: 0.6424 - val_accuracy: 0.7182 - lr: 1.0000e-05\n",
      "Epoch 49/60\n",
      "1523/1523 [==============================] - 2s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.78      0.78     16156\n",
      "           1       0.76      0.76      0.76     16401\n",
      "           2       0.61      0.61      0.61     16171\n",
      "\n",
      "    accuracy                           0.72     48728\n",
      "   macro avg       0.72      0.72      0.72     48728\n",
      "weighted avg       0.72      0.72      0.72     48728\n",
      "\n",
      "Macro Weighted F1-Score: 0.7183232968304242\n",
      "3046/3046 [==============================] - 18s 6ms/step - loss: 0.5743 - accuracy: 0.7498 - val_loss: 0.6413 - val_accuracy: 0.7182 - lr: 1.0000e-05\n",
      "Epoch 50/60\n",
      "1523/1523 [==============================] - 2s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.78      0.78     16156\n",
      "           1       0.76      0.77      0.76     16401\n",
      "           2       0.61      0.60      0.61     16171\n",
      "\n",
      "    accuracy                           0.72     48728\n",
      "   macro avg       0.72      0.72      0.72     48728\n",
      "weighted avg       0.72      0.72      0.72     48728\n",
      "\n",
      "Macro Weighted F1-Score: 0.7180595216146569\n",
      "3046/3046 [==============================] - 18s 6ms/step - loss: 0.5720 - accuracy: 0.7505 - val_loss: 0.6410 - val_accuracy: 0.7184 - lr: 1.0000e-05\n",
      "Epoch 51/60\n",
      "1523/1523 [==============================] - 2s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.78      0.78     16156\n",
      "           1       0.76      0.77      0.76     16401\n",
      "           2       0.61      0.61      0.61     16171\n",
      "\n",
      "    accuracy                           0.72     48728\n",
      "   macro avg       0.72      0.72      0.72     48728\n",
      "weighted avg       0.72      0.72      0.72     48728\n",
      "\n",
      "Macro Weighted F1-Score: 0.7185885722176174\n",
      "3046/3046 [==============================] - 19s 6ms/step - loss: 0.5696 - accuracy: 0.7516 - val_loss: 0.6403 - val_accuracy: 0.7189 - lr: 1.0000e-05\n",
      "Epoch 52/60\n",
      "1523/1523 [==============================] - 2s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.79      0.78     16156\n",
      "           1       0.76      0.76      0.76     16401\n",
      "           2       0.61      0.60      0.61     16171\n",
      "\n",
      "    accuracy                           0.72     48728\n",
      "   macro avg       0.72      0.72      0.72     48728\n",
      "weighted avg       0.72      0.72      0.72     48728\n",
      "\n",
      "Macro Weighted F1-Score: 0.7182298312391918\n",
      "3046/3046 [==============================] - 18s 6ms/step - loss: 0.5682 - accuracy: 0.7524 - val_loss: 0.6413 - val_accuracy: 0.7187 - lr: 1.0000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/60\n",
      "1523/1523 [==============================] - 2s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.78      0.78     16156\n",
      "           1       0.76      0.77      0.76     16401\n",
      "           2       0.61      0.61      0.61     16171\n",
      "\n",
      "    accuracy                           0.72     48728\n",
      "   macro avg       0.72      0.72      0.72     48728\n",
      "weighted avg       0.72      0.72      0.72     48728\n",
      "\n",
      "Macro Weighted F1-Score: 0.7184454910456515\n",
      "3046/3046 [==============================] - 19s 6ms/step - loss: 0.5660 - accuracy: 0.7542 - val_loss: 0.6405 - val_accuracy: 0.7187 - lr: 1.0000e-05\n",
      "Epoch 54/60\n",
      "1523/1523 [==============================] - 2s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.78      0.78     16156\n",
      "           1       0.76      0.76      0.76     16401\n",
      "           2       0.61      0.61      0.61     16171\n",
      "\n",
      "    accuracy                           0.72     48728\n",
      "   macro avg       0.72      0.72      0.72     48728\n",
      "weighted avg       0.72      0.72      0.72     48728\n",
      "\n",
      "Macro Weighted F1-Score: 0.7189905996735834\n",
      "3046/3046 [==============================] - 19s 6ms/step - loss: 0.5638 - accuracy: 0.7550 - val_loss: 0.6404 - val_accuracy: 0.7192 - lr: 1.0000e-05\n",
      "Epoch 55/60\n",
      "1523/1523 [==============================] - 2s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.78      0.78     16156\n",
      "           1       0.76      0.76      0.76     16401\n",
      "           2       0.61      0.61      0.61     16171\n",
      "\n",
      "    accuracy                           0.72     48728\n",
      "   macro avg       0.72      0.72      0.72     48728\n",
      "weighted avg       0.72      0.72      0.72     48728\n",
      "\n",
      "Macro Weighted F1-Score: 0.719172106517588\n",
      "3046/3046 [==============================] - 19s 6ms/step - loss: 0.5623 - accuracy: 0.7559 - val_loss: 0.6401 - val_accuracy: 0.7193 - lr: 1.0000e-05\n",
      "Epoch 56/60\n",
      "1523/1523 [==============================] - 2s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.78      0.78     16156\n",
      "           1       0.76      0.77      0.76     16401\n",
      "           2       0.61      0.60      0.61     16171\n",
      "\n",
      "    accuracy                           0.72     48728\n",
      "   macro avg       0.72      0.72      0.72     48728\n",
      "weighted avg       0.72      0.72      0.72     48728\n",
      "\n",
      "Macro Weighted F1-Score: 0.7190161631547811\n",
      "3046/3046 [==============================] - 19s 6ms/step - loss: 0.5598 - accuracy: 0.7570 - val_loss: 0.6398 - val_accuracy: 0.7195 - lr: 1.0000e-05\n",
      "Epoch 57/60\n",
      "1523/1523 [==============================] - 2s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.78      0.78     16156\n",
      "           1       0.76      0.77      0.76     16401\n",
      "           2       0.61      0.61      0.61     16171\n",
      "\n",
      "    accuracy                           0.72     48728\n",
      "   macro avg       0.72      0.72      0.72     48728\n",
      "weighted avg       0.72      0.72      0.72     48728\n",
      "\n",
      "Macro Weighted F1-Score: 0.7192300454339432\n",
      "3046/3046 [==============================] - 19s 6ms/step - loss: 0.5582 - accuracy: 0.7576 - val_loss: 0.6400 - val_accuracy: 0.7193 - lr: 1.0000e-05\n",
      "Epoch 58/60\n",
      "1523/1523 [==============================] - 2s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.78      0.78     16156\n",
      "           1       0.76      0.77      0.76     16401\n",
      "           2       0.61      0.61      0.61     16171\n",
      "\n",
      "    accuracy                           0.72     48728\n",
      "   macro avg       0.72      0.72      0.72     48728\n",
      "weighted avg       0.72      0.72      0.72     48728\n",
      "\n",
      "Macro Weighted F1-Score: 0.7190961744896448\n",
      "3046/3046 [==============================] - 18s 6ms/step - loss: 0.5557 - accuracy: 0.7589 - val_loss: 0.6399 - val_accuracy: 0.7193 - lr: 1.0000e-05\n",
      "Epoch 59/60\n",
      "1523/1523 [==============================] - 2s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.79      0.78     16156\n",
      "           1       0.76      0.76      0.76     16401\n",
      "           2       0.61      0.61      0.61     16171\n",
      "\n",
      "    accuracy                           0.72     48728\n",
      "   macro avg       0.72      0.72      0.72     48728\n",
      "weighted avg       0.72      0.72      0.72     48728\n",
      "\n",
      "Macro Weighted F1-Score: 0.7189689981230784\n",
      "3046/3046 [==============================] - 18s 6ms/step - loss: 0.5550 - accuracy: 0.7600 - val_loss: 0.6394 - val_accuracy: 0.7191 - lr: 1.0000e-05\n",
      "Epoch 60/60\n",
      "1523/1523 [==============================] - 2s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.78      0.78     16156\n",
      "           1       0.76      0.77      0.76     16401\n",
      "           2       0.61      0.60      0.61     16171\n",
      "\n",
      "    accuracy                           0.72     48728\n",
      "   macro avg       0.72      0.72      0.72     48728\n",
      "weighted avg       0.72      0.72      0.72     48728\n",
      "\n",
      "Macro Weighted F1-Score: 0.7195599737013045\n",
      "3046/3046 [==============================] - 19s 6ms/step - loss: 0.5526 - accuracy: 0.7603 - val_loss: 0.6415 - val_accuracy: 0.7199 - lr: 1.0000e-05\n"
     ]
    }
   ],
   "source": [
    "if not BALANCE_DATA:\n",
    "    lstm_model.compile(optimizer=Adam(learning_rate = 1e-5,amsgrad=True), loss = SparseCategoricalCrossentropy(), metrics=['accuracy'])\n",
    "    history = lstm_model.fit(train_set_padded, train_labels,validation_data = (valid_set_padded,validation_labels),\\\n",
    "                    batch_size=BATCH_SIZE, epochs=EPOCHS,class_weight=class_weights,\\\n",
    "                        callbacks=[tensorboard_callback,metrics_callback,EarlyStopping(patience=8,min_delta=0.0005,mode='max',verbose = 1,restore_best_weights=True,monitor='val_loss'),ReduceLROnPlateau(factor=0.1, patience=4)])                  \n",
    "else:\n",
    "    lstm_model.compile(optimizer=Adam(learning_rate = 1e-5,amsgrad=True), loss = SparseCategoricalCrossentropy(), metrics=['accuracy'])\n",
    "    history = lstm_model.fit(train_set_padded, train_labels,validation_data = (valid_set_padded,validation_labels),\\\n",
    "                    batch_size=BATCH_SIZE, epochs=EPOCHS,\\\n",
    "                             callbacks=[tensorboard_callback,metrics_callback,ReduceLROnPlateau(monitor=\"val_loss\",factor=0.1,verbose = 1,min_delta=0.001,patience=4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(SAVED_MODELS_DIR):\n",
    "    os.mkdir(SAVED_MODELS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model.save_weights(SAVED_MODELS_DIR + \"/lstm_model_variant_\" + str(VARIANT_NUM) + \".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_out = open(SAVED_MODELS_DIR + \"/lstm_model_params_\" + str(VARIANT_NUM) + \".pickle\",'wb')\n",
    "pickle.dump({'EMBEDDING_DIM':EMBEDDING_DIM,'MAX_TEXT_LEN':MAX_TEXT_LEN,'BATCH_SIZE':BATCH_SIZE,'EPOCHS':EPOCHS,\\\n",
    "             'train_set_size':len(train_set),'optimizer':str(lstm_model.optimizer),\\\n",
    "             'learning_rate':str(lstm_model.optimizer.learning_rate),\\\n",
    "             'conv_filters':CONV_FILTERS,'conv_kernel':CONV_KERNEL_SIZE,\\\n",
    "             'chunks_used':(len(train_set) + len(validation_set) + len(test_set)) // DATA_IN_CHUNK,\\\n",
    "            'LSTM_UNITS':LSTM_UNITS,'DENSE_UNITS':DENSE_UNITS,'LABELS_COUNT':LABELS_COUNT},pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "if VAST:\n",
    "    !tar -czf lstm_variant_15_logs.tar.gz logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DELETE_PICKLES_AFTER_TRAINING:\n",
    "    if GDRIVE:\n",
    "        !rm -r larger_dataset/ready_for_models\n",
    "    else:\n",
    "        !rm -r ../larger_dataset/ready_for_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(SAVED_MODELS_DIR + \"/\" + 'lstm_model_' + str(VARIANT_NUM) + '.npy',history.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "381/381 [==============================] - 1s 2ms/step - loss: 0.6506 - accuracy: 0.7147\n",
      "test loss, test acc: [0.6505745053291321, 0.7147489190101624]\n"
     ]
    }
   ],
   "source": [
    "results = lstm_model.evaluate(test_set_padded, test_labels, batch_size=BATCH_SIZE)\n",
    "print(\"test loss, test acc:\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir logs/fit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
