{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Goal of This File:\n",
    "\n",
    "##### 1. Import Libraries, Helper Functions, and Constants ==> Data Sourcing and Munging ==> Utility Functions ==> Loading the Data ==> Merging all Individual Files\n",
    "\n",
    "##### 2. Imbalanced Data Solution ==> Hyperparameters\n",
    "\n",
    "##### 3. BERT\n",
    "\n",
    "##### 4. Conclusion ==> Results ==> Tensorboard"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries, Helper Functions and Load Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "GDRIVE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.4.1)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.23.5)\n",
      "Requirement already satisfied: nltk in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.1.1)\n",
      "Requirement already satisfied: wordcloud in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.8.2.2)\n",
      "Requirement already satisfied: seaborn in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.12.0)\n",
      "Requirement already satisfied: gensim in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.3.1)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.12.0)\n",
      "Requirement already satisfied: imblearn in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: xgboost in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.7.5)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.6.0)\n",
      "Requirement already satisfied: unrar in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.4)\n",
      "Requirement already satisfied: pyunpack in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.3)\n",
      "Requirement already satisfied: more-itertools in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (9.1.0)\n",
      "Requirement already satisfied: patool in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.12)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
      "     ---------------------------------------- 7.1/7.1 MB 2.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (2022.9.13)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: click in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.8.1)\n",
      "Requirement already satisfied: pillow in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wordcloud) (9.2.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gensim) (6.3.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.12.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow) (2.12.0)\n",
      "Requirement already satisfied: jax>=0.3.15 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (14.0.6)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: tensorboard<2.13,>=2.12 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (2.12.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (0.27.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (65.5.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: keras<2.13,>=2.12.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (2.12.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (4.22.1)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (22.10.26)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (4.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (21.3)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (3.7.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (2.12.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.50.0)\n",
      "Requirement already satisfied: imbalanced-learn in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from imblearn) (0.9.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (1.0.5)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (4.37.4)\n",
      "Requirement already satisfied: entrypoint2 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyunpack) (1.1)\n",
      "Requirement already satisfied: easyprocess in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyunpack) (1.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (3.8.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (6.0)\n",
      "Collecting huggingface-hub<1.0,>=0.14.1\n",
      "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
      "     -------------------------------------- 236.8/236.8 kB 3.7 MB/s eta 0:00:00\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.3-cp310-cp310-win_amd64.whl (3.5 MB)\n",
      "     ---------------------------------------- 3.5/3.5 MB 3.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2.28.1)\n",
      "Collecting fsspec\n",
      "  Downloading fsspec-2023.5.0-py3-none-any.whl (160 kB)\n",
      "     -------------------------------------- 160.1/160.1 kB 3.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from tqdm->nltk) (0.4.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (1.26.12)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.12.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: ml-dtypes>=0.0.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow) (0.0.4)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.17.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (0.7.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (5.2.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.2.2)\n",
      "Installing collected packages: tokenizers, fsspec, huggingface-hub, transformers\n",
      "Successfully installed fsspec-2023.5.0 huggingface-hub-0.15.1 tokenizers-0.13.3 transformers-4.29.2\n"
     ]
    }
   ],
   "source": [
    "if GDRIVE:\n",
    "    !pip install pandas numpy nltk scikit-learn wordcloud\\\n",
    "    seaborn gensim tensorflow imblearn xgboost matplotlib unrar pyunpack more-itertools patool transformers > /dev/null\n",
    "\n",
    "else:\n",
    "    !pip install pandas numpy nltk scikit-learn wordcloud\\\n",
    "    seaborn gensim tensorflow imblearn xgboost matplotlib unrar pyunpack more-itertools patool transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The system cannot find the path specified.\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas numpy nltk scikit-learn wordcloud\\\n",
    "seaborn gensim tensorflow imblearn xgboost matplotlib unrar pyunpack more-itertools patool transformers > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sudo' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'sudo' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "VAST = True\n",
    "\n",
    "if VAST:\n",
    "    !sudo apt-get install unrar\n",
    "    !sudo apt-get install rar\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "UTILS_URL = 'https://f005.backblazeb2.com/file/gp-support-files/nlp_ai_utils.py'\n",
    "UPDATING_VALUES_URL = 'https://f005.backblazeb2.com/file/gp-support-files/updating_values.py'\n",
    "ALL_LIBS_URL = 'https://f005.backblazeb2.com/file/gp-support-files/all_libs_dl.py'\n",
    "CHUNKS_URLS_FILE_URL = 'https://f005.backblazeb2.com/file/gp-support-files/chunks_urls.py'\n",
    "\n",
    "UTILS_FILE_NAME = 'nlp_ai_utils'\n",
    "UPDATING_VALUES_FILE_NAME = 'updating_values'\n",
    "ALL_LIBS_FILE_NAME = 'all_libs_dl'\n",
    "CHUNKS_URLS_FILE_NAME = 'chunks_urls'\n",
    "\n",
    "DEP_FILE_EXT = '.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "def get_dependencies(url,file_name,file_extension):\n",
    "    if os.path.exists(file_name + file_extension):\n",
    "        return print(file_name + \" already exists.\")\n",
    "    else:\n",
    "        print(f\"downloading {file_name}...\")\n",
    "        r = requests.get(url)\n",
    "        with open(file_name + file_extension, 'wb') as fd:\n",
    "            for chunk in r.iter_content():\n",
    "                fd.write(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nlp_ai_utils already exists.\n",
      "updating_values already exists.\n",
      "all_libs_dl already exists.\n",
      "chunks_urls already exists.\n"
     ]
    }
   ],
   "source": [
    "get_dependencies(UTILS_URL,UTILS_FILE_NAME,DEP_FILE_EXT)\n",
    "get_dependencies(UPDATING_VALUES_URL,UPDATING_VALUES_FILE_NAME,DEP_FILE_EXT)\n",
    "get_dependencies(ALL_LIBS_URL,ALL_LIBS_FILE_NAME,DEP_FILE_EXT)\n",
    "get_dependencies(CHUNKS_URLS_FILE_URL,CHUNKS_URLS_FILE_NAME,DEP_FILE_EXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from nlp_ai_utils import *\n",
    "from chunks_urls import CHUNKS_URLS\n",
    "from updating_values import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_ENABLE_ONEDNN_OPTS = 0\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "USE_READY_DATA = False\n",
    "LIMIT = 6\n",
    "DATA_URLS = CHUNKS_URLS[:LIMIT]\n",
    "# GLOVE_CHOSEN_DIM = 100\n",
    "\n",
    "# GLOVE_URL = 'https://f005.backblazeb2.com/file/glove-embeddings-dims/glove.6B.' + str(GLOVE_CHOSEN_DIM) + 'd' + '.txt'\n",
    "\n",
    "if USE_READY_DATA:\n",
    "    READY_DATA_URL = 'https://f005.backblazeb2.com/file/yelp-dataset-ready-for-models/ready_for_models.rar'\n",
    "    CLASS_WEIGHTS_URL = 'https://f005.backblazeb2.com/file/yelp-dataset-ready-for-models/class_weights.pickle'\n",
    "    UNIQUE_WORDS_URL = 'https://f005.backblazeb2.com/file/yelp-dataset-ready-for-models/unique_words.pickle'\n",
    "else:\n",
    "    READY_DATA_URL = False\n",
    "    CLASS_WEIGHTS_URL = False\n",
    "    UNIQUE_WORDS_URL = False\n",
    "\n",
    "if GDRIVE:\n",
    "    PICKLES_DIR = \"pickle_files\"\n",
    "    READY_DATASET_PATH = \"larger_dataset/ready_for_models/\"\n",
    "    LARGER_DATASET_PATH = \"larger_dataset\"\n",
    "    #GLOVE_FILES_DIR = \"glove_files\"\n",
    "    SAVED_MODELS_DIR = 'saved_models'\n",
    "else:\n",
    "    PICKLES_DIR = \"../pickle_files\"    \n",
    "    LARGER_DATASET_PATH = \"../larger_dataset\"\n",
    "    READY_DATASET_PATH = \"../larger_dataset/ready_for_models/\"\n",
    "    #GLOVE_FILES_DIR = \"../glove_files\"\n",
    "    SAVED_MODELS_DIR = '../saved_models'\n",
    "    \n",
    "PREPROCESSED_CHUNKS_PATH = LARGER_DATASET_PATH + \"/preprocessed_data_chunks/\"\n",
    "BASE_FILE_NAME = \"chunk_\"\n",
    "FILE_FORMAT = \".csv\"\n",
    "DATA_IN_CHUNK = 116505\n",
    "if LIMIT == len(CHUNKS_URLS):\n",
    "    ACTUAL_DATA_SHAPE = DATA_IN_CHUNK * LIMIT + 20\n",
    "else:\n",
    "    ACTUAL_DATA_SHAPE = DATA_IN_CHUNK * LIMIT\n",
    "    \n",
    "RANDOM_STATE = CONST_RANDOM_STATE\n",
    "DELETE_PICKLES_AFTER_TRAINING = False\n",
    "BALANCE_DATA = True\n",
    "USE_MODIFIED_LABELS = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Data Sourcing and Munging"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1 Loading The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_1 already exists.\n",
      "chunk_2 already exists.\n",
      "chunk_3 already exists.\n",
      "chunk_4 already exists.\n",
      "chunk_5 already exists.\n",
      "chunk_6 already exists.\n"
     ]
    }
   ],
   "source": [
    "if not USE_READY_DATA:\n",
    "    if not os.path.exists(LARGER_DATASET_PATH):\n",
    "        os.mkdir(LARGER_DATASET_PATH)\n",
    "    if not os.path.exists(PREPROCESSED_CHUNKS_PATH):\n",
    "        os.mkdir(PREPROCESSED_CHUNKS_PATH)\n",
    "    get_chunks(DATA_URLS,LIMIT,1,BASE_FILE_NAME,PREPROCESSED_CHUNKS_PATH,FILE_FORMAT)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2 Merging all Individual Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all names of downloaded files\n",
    "if not USE_READY_DATA:\n",
    "    all_file_names = get_all_file_names(BASE_FILE_NAME,LIMIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read all chunks into a list\n",
    "if not USE_READY_DATA:\n",
    "    list_dfs = read_chunks(all_file_names,PREPROCESSED_CHUNKS_PATH,FILE_FORMAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate all chunks into a singular df\n",
    "if not USE_READY_DATA:\n",
    "    df = group_up_chunks(list_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check how much of the data was actually downloaded\n",
    "if not USE_READY_DATA:\n",
    "    percent_loaded = check_no_missing_data(df.shape[0],ACTUAL_DATA_SHAPE)\n",
    "    percent_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USE_READY_DATA:\n",
    "    review_data = df[['text', 'stars']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USE_READY_DATA:\n",
    "    review_data.reset_index(inplace = True)\n",
    "    review_data.drop(['index'],axis = 1,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USE_READY_DATA:\n",
    "    review_data.rename(columns = {'text':'full_review_text','stars':'star_rating'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USE_READY_DATA:\n",
    "    review_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USE_READY_DATA:\n",
    "    review_data.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USE_READY_DATA:\n",
    "    review_data['full_review_text'].replace('', np.nan, inplace=True)\n",
    "    review_data.dropna(inplace = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imbalanced Data Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if BALANCE_DATA:\n",
    "    label_1 = review_data[review_data['star_rating'] == 1]\n",
    "    label_2 = review_data[review_data['star_rating'] == 2]\n",
    "    label_3 = review_data[review_data['star_rating'] == 3]\n",
    "    label_4 = review_data[review_data['star_rating'] == 4]\n",
    "    label_5 = review_data[review_data['star_rating'] == 5]\n",
    "\n",
    "    minority_class = min([label_1.shape[0],label_2.shape[0],label_3.shape[0],label_4.shape[0],label_5.shape[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(160320, 2)\n",
      "3.0    53440\n",
      "1.0    26720\n",
      "2.0    26720\n",
      "4.0    26720\n",
      "5.0    26720\n",
      "Name: star_rating, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "if BALANCE_DATA:\n",
    "    if USE_MODIFIED_LABELS:\n",
    "        review_data_label_1 = label_1[:int(minority_class / 2)]\n",
    "        review_data_label_2 = label_2[:int(minority_class / 2)]\n",
    "        review_data_label_3 = label_3[:int(minority_class)]\n",
    "        review_data_label_4 = label_4[:int(minority_class / 2)]\n",
    "        review_data_label_5 = label_5[:int(minority_class / 2)]\n",
    "\n",
    "    else:\n",
    "        review_data_label_1 = label_1[:int(minority_class)]\n",
    "        review_data_label_2 = label_2[:int(minority_class)]\n",
    "        review_data_label_3 = label_3[:int(minority_class)]\n",
    "        review_data_label_4 = label_4[:int(minority_class)]\n",
    "        review_data_label_5 = label_5[:int(minority_class)]\n",
    "\n",
    "    review_data = pd.concat([review_data_label_1,review_data_label_2,review_data_label_3,review_data_label_4,review_data_label_5])\n",
    "\n",
    "    print(review_data.shape)\n",
    "\n",
    "    print(review_data['star_rating'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USE_READY_DATA:\n",
    "    X = review_data['full_review_text']\n",
    "    y = review_data['star_rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USE_READY_DATA:\n",
    "    X = pd.Series([str(text) for text in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_MODIFIED_LABELS:\n",
    "    translated_labels = translate_labels(y)\n",
    "    y = pd.Series(encode_sent(translated_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2    53440\n",
      "3    53440\n",
      "1    53440\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(y.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USE_READY_DATA:\n",
    "    y = y - 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classes' shapes before and after (if USE_MODIFIED_LABELS = True):\n",
    "\n",
    "BEFORE: 1 ==> Very Bad ==> AFTER: 2 ==> Negative Sentiment\n",
    "\n",
    "BEFORE: 2 ==> Bad ==> AFTER: 2  ==> Negative Sentiment\n",
    "\n",
    "BEFORE: 3 ==> Ok ==> AFTER: 3  ==> Neutral Sentiment\n",
    "\n",
    "BEFORE: 4 ==> Good ==> AFTER: 1  ==> Positive Sentiment\n",
    "\n",
    "BEFORE: 5 ==> Very Good ==> AFTER: 1  ==> Positive Sentiment\n",
    "\n",
    "THEN: 1 is subtracted from each label. So, the Labels go FROM: 2,3,1 TO: 1,2,0\n",
    "\n",
    "#### Classes' shapes before and after (if USE_MODIFIED_LABELS = False):\n",
    "\n",
    "1 is subtracted from each label. So, the Labels go FROM: 1,2,3,4,5 TO: 0,1,2,3,4\n",
    "\n",
    "Why is 1 subracted from each label? To bring the data into the preferred shape of the class weights in both sklearn and keras (starting label is 0, not 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(PICKLES_DIR):\n",
    "    os.mkdir(PICKLES_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USE_READY_DATA:\n",
    "    if BALANCE_DATA:\n",
    "        class_weights = False\n",
    "    else:\n",
    "        class_weights = compute_class_weight(class_weight = \"balanced\",classes = np.unique(y),y=y)\n",
    "        class_weights = dict(zip(np.unique(y), class_weights))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_READY_DATA:\n",
    "    get_chunks([UNIQUE_WORDS_URL],0,1,'unique_words',PICKLES_DIR + '/','.pickle',False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_READY_DATA:\n",
    "    UNIQUE_WORDS = pickle.load(open(PICKLES_DIR + \"/unique_words.pickle\", \"rb\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PERCENT = 0.8\n",
    "ATTENTION_UNITS = 64\n",
    "DENSE_UNITS = 1024\n",
    "LABELS_COUNT = len(y.unique())\n",
    "EMBEDDING_DIM = 100\n",
    "MAX_TEXT_LEN = 250\n",
    "CONV_FILTERS = 60\n",
    "CONV_KERNEL_SIZE = 3\n",
    "DROPOUT_VAL = 0.2\n",
    "TRUNC_TYPE = 'post'\n",
    "PADDING_TYPE = 'post'\n",
    "OOV_TOKEN = \"<OOV>\"\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTUAL_DATA_SHAPE = len(X)\n",
    "\n",
    "if BALANCE_DATA:\n",
    "    temp = pd.DataFrame({'text':X,'labels':y})\n",
    "    shuffled = temp.sample(frac=1,random_state=RANDOM_STATE)\n",
    "    X = shuffled['text']\n",
    "    y = shuffled['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.reset_index(drop=True,inplace=True)\n",
    "y.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(READY_DATASET_PATH):\n",
    "    os.mkdir(READY_DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if USE_READY_DATA:\n",
    "    get_chunks([READY_DATA_URL],0,1,'ready_for_models',READY_DATASET_PATH,'.rar',False)\n",
    "    Archive(os.path.join(READY_DATASET_PATH,\"ready_for_models.rar\")).extractall(READY_DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_READY_DATA:\n",
    "    train_set_padded = pickle.load(open(READY_DATASET_PATH + \"train_set_padded.pickle\", \"rb\"))\n",
    "    train_labels = pickle.load(open(READY_DATASET_PATH + \"train_labels.pickle\", \"rb\"))\n",
    "    valid_set_padded = pickle.load(open(READY_DATASET_PATH + \"valid_set_padded.pickle\", \"rb\"))\n",
    "    validation_labels = pickle.load(open(READY_DATASET_PATH + \"validation_labels.pickle\", \"rb\"))\n",
    "    test_set_padded = pickle.load(open(READY_DATASET_PATH + \"test_set_padded.pickle\", \"rb\"))\n",
    "    test_labels = pickle.load(open(READY_DATASET_PATH + \"test_labels.pickle\", \"rb\"))\n",
    "    EMBEDDING_LAYER = pickle.load(open(READY_DATASET_PATH + \"EMBEDDING_LAYER.pickle\", \"rb\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow_hub in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.13.0)\n",
      "Requirement already satisfied: numpy>=1.12.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow_hub) (1.23.5)\n",
      "Requirement already satisfied: protobuf>=3.19.6 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow_hub) (3.19.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-text\n",
      "  Using cached tensorflow_text-2.10.0-cp310-cp310-win_amd64.whl (5.0 MB)\n",
      "Collecting tensorflow<2.11,>=2.10.0\n",
      "  Using cached tensorflow-2.10.1-cp310-cp310-win_amd64.whl (455.9 MB)\n",
      "Requirement already satisfied: tensorflow-hub>=0.8.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-text) (0.13.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text) (4.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text) (1.6.3)\n",
      "Requirement already satisfied: keras<2.11,>=2.10.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text) (2.10.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text) (1.3.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text) (65.5.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text) (22.10.26)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text) (14.0.6)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text) (1.50.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text) (1.14.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text) (21.3)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text) (1.23.5)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text) (3.7.0)\n",
      "Collecting tensorboard<2.11,>=2.10\n",
      "  Using cached tensorboard-2.10.1-py3-none-any.whl (5.9 MB)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text) (2.1.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text) (1.1.2)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text) (0.27.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text) (0.2.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text) (0.4.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text) (3.3.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text) (3.19.6)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text) (1.16.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.11,>=2.10.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text) (2.10.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from astunparse>=1.6.0->tensorflow<2.11,>=2.10.0->tensorflow-text) (0.38.4)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text) (1.8.1)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text) (0.6.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text) (2.28.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text) (3.4.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text) (2.2.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text) (2.17.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from packaging->tensorflow<2.11,>=2.10.0->tensorflow-text) (3.0.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text) (5.2.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text) (1.3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text) (1.26.12)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text) (3.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text) (3.2.2)\n",
      "Installing collected packages: google-auth-oauthlib, tensorboard, tensorflow, tensorflow-text\n",
      "  Attempting uninstall: google-auth-oauthlib\n",
      "    Found existing installation: google-auth-oauthlib 1.0.0\n",
      "    Uninstalling google-auth-oauthlib-1.0.0:\n",
      "      Successfully uninstalled google-auth-oauthlib-1.0.0\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.12.1\n",
      "    Uninstalling tensorboard-2.12.1:\n",
      "      Successfully uninstalled tensorboard-2.12.1\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.12.0\n",
      "    Uninstalling tensorflow-2.12.0:\n",
      "      Successfully uninstalled tensorflow-2.12.0\n",
      "  Rolling back uninstall of tensorflow\n",
      "  Moving to c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages\\tensorflow-2.12.0.dist-info\\\n",
      "   from C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\Lib\\site-packages\\~ensorflow-2.12.0.dist-info\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\user\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\Lib\\\\site-packages\\\\tensorflow\\\\compiler\\\\tf2tensorrt\\\\_pywrap_py_utils.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-estimator==2.3.0\n",
      "  Downloading tensorflow_estimator-2.3.0-py2.py3-none-any.whl (459 kB)\n",
      "     -------------------------------------- 459.0/459.0 kB 1.9 MB/s eta 0:00:00\n",
      "Installing collected packages: tensorflow-estimator\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.10.0\n",
      "    Uninstalling tensorflow-estimator-2.10.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.10.0\n",
      "Successfully installed tensorflow-estimator-2.3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-intel 2.12.0 requires keras<2.13,>=2.12.0, but you have keras 2.10.0 which is incompatible.\n",
      "tensorflow-intel 2.12.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.19.6 which is incompatible.\n",
      "tensorflow-intel 2.12.0 requires tensorboard<2.13,>=2.12, but you have tensorboard 2.10.1 which is incompatible.\n",
      "tensorflow-intel 2.12.0 requires tensorflow-estimator<2.13,>=2.12.0, but you have tensorflow-estimator 2.3.0 which is incompatible.\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'normalization' from 'tensorflow.python.keras.layers' (c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32md:\\new pc stuff\\E-learning\\Year 3\\Graduation Project\\sentiment-analysis-yelp\\models_advanced_variants\\bert_3labels_base_acc_00_chunks_6.ipynb Cell 50\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/new%20pc%20stuff/E-learning/Year%203/Graduation%20Project/sentiment-analysis-yelp/models_advanced_variants/bert_3labels_base_acc_00_chunks_6.ipynb#X65sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow_hub\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mhub\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/new%20pc%20stuff/E-learning/Year%203/Graduation%20Project/sentiment-analysis-yelp/models_advanced_variants/bert_3labels_base_acc_00_chunks_6.ipynb#X65sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow_text\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtext\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow_hub\\__init__.py:90\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     85\u001b[0m _ensure_tf_install()\n\u001b[0;32m     88\u001b[0m \u001b[39m# pylint: disable=g-import-not-at-top\u001b[39;00m\n\u001b[0;32m     89\u001b[0m \u001b[39m# pylint: disable=g-bad-import-order\u001b[39;00m\n\u001b[1;32m---> 90\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_hub\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mestimator\u001b[39;00m \u001b[39mimport\u001b[39;00m LatestModuleExporter\n\u001b[0;32m     91\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_hub\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mestimator\u001b[39;00m \u001b[39mimport\u001b[39;00m register_module_for_export\n\u001b[0;32m     92\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_hub\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfeature_column\u001b[39;00m \u001b[39mimport\u001b[39;00m image_embedding_column\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow_hub\\estimator.py:62\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     55\u001b[0m       \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m     56\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mThere is already a module registered to be exported as \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m     57\u001b[0m           export_name)\n\u001b[0;32m     58\u001b[0m   tf\u001b[39m.\u001b[39mcompat\u001b[39m.\u001b[39mv1\u001b[39m.\u001b[39madd_to_collection(_EXPORT_MODULES_COLLECTION,\n\u001b[0;32m     59\u001b[0m                                  (export_name, module))\n\u001b[1;32m---> 62\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mLatestModuleExporter\u001b[39;00m(tf_estimator\u001b[39m.\u001b[39;49mExporter):\n\u001b[0;32m     63\u001b[0m   \u001b[39m\"\"\"Regularly exports registered modules into timestamped directories.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[39m  Warning: Deprecated. This belongs to the hub.Module API and TF1 Hub format.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[39m  THIS FUNCTION IS DEPRECATED.\u001b[39;00m\n\u001b[0;32m     89\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m     91\u001b[0m   \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, serving_input_fn, exports_to_keep\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\lazy_loader.py:58\u001b[0m, in \u001b[0;36mLazyLoader.__getattr__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getattr__\u001b[39m(\u001b[39mself\u001b[39m, item):\n\u001b[1;32m---> 58\u001b[0m   module \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_load()\n\u001b[0;32m     59\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(module, item)\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\lazy_loader.py:41\u001b[0m, in \u001b[0;36mLazyLoader._load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[39m\"\"\"Load the module and insert it into the parent's globals.\"\"\"\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[39m# Import the target module and insert it into the parent's namespace\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m module \u001b[39m=\u001b[39m importlib\u001b[39m.\u001b[39;49mimport_module(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__name__\u001b[39;49m)\n\u001b[0;32m     42\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_parent_module_globals[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_local_name] \u001b[39m=\u001b[39m module\n\u001b[0;32m     44\u001b[0m \u001b[39m# Emit a warning if one was specified\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    124\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    125\u001b[0m         level \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[39mreturn\u001b[39;00m _bootstrap\u001b[39m.\u001b[39;49m_gcd_import(name[level:], package, level)\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\api\\_v1\\estimator\\__init__.py:10\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_estimator\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mestimator\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapi\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_v1\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mestimator\u001b[39;00m \u001b[39mimport\u001b[39;00m experimental\n\u001b[0;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_estimator\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mestimator\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapi\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_v1\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mestimator\u001b[39;00m \u001b[39mimport\u001b[39;00m export\n\u001b[1;32m---> 10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_estimator\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mestimator\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapi\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_v1\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mestimator\u001b[39;00m \u001b[39mimport\u001b[39;00m inputs\n\u001b[0;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_estimator\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mestimator\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapi\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_v1\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mestimator\u001b[39;00m \u001b[39mimport\u001b[39;00m tpu\n\u001b[0;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_estimator\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mestimator\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcanned\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbaseline\u001b[39;00m \u001b[39mimport\u001b[39;00m BaselineClassifier\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\api\\_v1\\estimator\\experimental\\__init__.py:10\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_estimator\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mestimator\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcanned\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdnn\u001b[39;00m \u001b[39mimport\u001b[39;00m dnn_logit_fn_builder\n\u001b[0;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_estimator\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mestimator\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcanned\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkmeans\u001b[39;00m \u001b[39mimport\u001b[39;00m KMeansClustering \u001b[39mas\u001b[39;00m KMeans\n\u001b[1;32m---> 10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_estimator\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mestimator\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcanned\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlinear\u001b[39;00m \u001b[39mimport\u001b[39;00m LinearSDCA\n\u001b[0;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_estimator\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mestimator\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcanned\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlinear\u001b[39;00m \u001b[39mimport\u001b[39;00m linear_logit_fn_builder\n\u001b[0;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_estimator\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mestimator\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mearly_stopping\u001b[39;00m \u001b[39mimport\u001b[39;00m make_early_stopping_hook\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\canned\\dnn.py:28\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtf_export\u001b[39;00m \u001b[39mimport\u001b[39;00m estimator_export\n\u001b[0;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_estimator\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mestimator\u001b[39;00m \u001b[39mimport\u001b[39;00m estimator\n\u001b[1;32m---> 28\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_estimator\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mestimator\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcanned\u001b[39;00m \u001b[39mimport\u001b[39;00m head \u001b[39mas\u001b[39;00m head_lib\n\u001b[0;32m     29\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_estimator\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mestimator\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcanned\u001b[39;00m \u001b[39mimport\u001b[39;00m optimizers\n\u001b[0;32m     30\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_estimator\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mestimator\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mhead\u001b[39;00m \u001b[39mimport\u001b[39;00m head_utils\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'normalization' from 'tensorflow.python.keras.layers' (c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = \"https://tfhub.dev/google/nnlm-en-dim50/2\"\n",
    "hub_layer = hub.KerasLayer(embedding, input_shape=[], dtype=tf.string, trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-02 13:12:00.381933: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-02 13:12:00.385842: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-02 13:12:00.386166: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-02 13:12:00.388075: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-02 13:12:00.388377: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-02 13:12:00.388663: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-02 13:12:00.862742: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-02 13:12:00.863153: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-02 13:12:00.863440: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-02 13:12:00.863706: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22078 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:2b:00.0, compute capability: 8.9\n",
      "2023-06-02 13:12:01.955946: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:637] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(hub_layer)\n",
    "model.add(tf.keras.layers.Dense(DENSE_UNITS, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(LABELS_COUNT, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ==> here is a quick explaination of how the dataset will be split using a smaller sample example.\n",
    "##### ==> dataset => 100\n",
    "##### ==> train_set => tr_s (example: 80)\n",
    "##### ==> valid_set => vs (example: 10)\n",
    "##### ==> test_set => te_s (example: 10)\n",
    "##### ==> t = tr_s (80) + vs (10)\n",
    "##### ==> train_set = x[:80]\n",
    "##### ==> valid_set = x[80:t]\n",
    "##### ==> test_set = x[t:] why t? because => vs = ts\n",
    "\n",
    "##### use this guideline if you are confused about how the train-validation-test split was done. Also, this is a future guide for me as well in case I forget."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* train_set_size = 6,990,280 * 0.8 = 5,592,224\n",
    "* valid_set_size = 6,990,280 * 0.1 = 699,028\n",
    "* train_plus_valid = 5,592,224 + 699,028 = 6,291,252\n",
    "\n",
    "==> To Confirm: test_size = 6,990,280 - 6,291,252 = 699,028\n",
    "\n",
    "* train_set = [0:5,592,224]\n",
    "* train_labels = [0:5,592,224]\n",
    "* validation_set = [5,592,224:6,291,252] ==> 699,028\n",
    "* validation_labels = [5,592,224:6,291,252] ==> 699,028\n",
    "* test_set = [6,291,252,6,990,280] ==> 699,028\n",
    "* test_labels = [6,291,252,6,990,280] ==> 699,028"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALID_TEST_PERCENT = round(1 - TRAIN_PERCENT,1) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE = int(ACTUAL_DATA_SHAPE * TRAIN_PERCENT)\n",
    "VALID_TEST_SIZE = int(ACTUAL_DATA_SHAPE * VALID_TEST_PERCENT)\n",
    "TOTAL_TEST_SIZE = TRAIN_SIZE + VALID_TEST_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = X[:TRAIN_SIZE]\n",
    "train_labels = y[:TRAIN_SIZE]\n",
    "\n",
    "valid_set= X[TRAIN_SIZE:TOTAL_TEST_SIZE]\n",
    "valid_labels = y[TRAIN_SIZE:TOTAL_TEST_SIZE]\n",
    "\n",
    "test_set = X[TOTAL_TEST_SIZE:]\n",
    "test_labels = y[TOTAL_TEST_SIZE:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.reset_index(drop=True,inplace=True)\n",
    "train_labels.reset_index(drop=True,inplace=True)\n",
    "\n",
    "valid_set.reset_index(drop=True,inplace=True)\n",
    "valid_labels.reset_index(drop=True,inplace=True)\n",
    "\n",
    "test_set.reset_index(drop=True,inplace=True)\n",
    "test_labels.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = np.array(train_set)\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "valid_set = np.array(valid_set)\n",
    "valid_labels = np.array(valid_labels)\n",
    "\n",
    "test_set = np.array(test_set)\n",
    "test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type list).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-5\u001b[39m),loss\u001b[38;5;241m=\u001b[39mSparseCategoricalCrossentropy(),metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m----> 3\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_input_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_attention_masks\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m          \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvalid_input_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_attention_masks\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/constant_op.py:103\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    101\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[1;32m    102\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type list)."
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model.compile(optimizer=Adam(learning_rate=1e-5),loss=SparseCategoricalCrossentropy(),metrics=['accuracy'])\n",
    "model.fit(train_set,train_labels,epochs=EPOCHS,batch_size=BATCH_SIZE,\\\n",
    "          validation_data=(valid_set,valid_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(SAVED_MODELS_DIR):\n",
    "    os.mkdir(SAVED_MODELS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(SAVED_MODELS_DIR + \"/bert_model\" + \".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_out = open(SAVED_MODELS_DIR + \"/bert_model_params\" + \".pickle\",'wb')\n",
    "pickle.dump({'MAX_TEXT_LEN':MAX_TEXT_LEN,'BATCH_SIZE':BATCH_SIZE,'EPOCHS':EPOCHS,\\\n",
    "             'train_set_size':len(train_input_ids),'optimizer':str(bert_model.optimizer),\\\n",
    "             'learning_rate':str(bert_model.optimizer.learning_rate),\\\n",
    "             'chunks_used':(len(train_input_ids) + len(valid_input_ids) + len(test_input_ids)) // DATA_IN_CHUNK,\\\n",
    "             'LABELS_COUNT':LABELS_COUNT},pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if VAST:\n",
    "    !tar -czf bert_model_logs.tar.gz logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DELETE_PICKLES_AFTER_TRAINING:\n",
    "    if GDRIVE:\n",
    "        !rm -r larger_dataset/ready_for_models\n",
    "    else:\n",
    "        !rm -r ../larger_dataset/ready_for_models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate([test_input_ids, test_attention_masks], test_labels, batch_size=BATCH_SIZE)\n",
    "print(\"test loss, test acc:\", results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir logs/fit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
