{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HqDDQbkSSN0C"
   },
   "source": [
    "# Word2Vec vs GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bAtjHaXjSN0E"
   },
   "source": [
    "# * Goal of This File:\n",
    "\n",
    "##### 1. Import Libraries, Helper Functions, and Constants ==> Data Sourcing and Munging ==> Utility Functions ==> Loading the Data ==> Merging all Individual Files\n",
    "\n",
    "##### 2. Comparison ==> Class Weights ==> Word2Vec + GloVe\n",
    "\n",
    "##### 3. RNN with Word2Vec \n",
    "\n",
    "##### 4. RNN with GloVe \n",
    "\n",
    "##### 5. Conclusion ==> Results ==> Tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cQArejLuSN0E"
   },
   "source": [
    "## (1) Import Libraries, Helper Functions and Load Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "3qXcept3SN0F",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas numpy nltk scikit-learn wordcloud\\\n",
    "seaborn gensim tensorflow imblearn xgboost matplotlib unrar pyunpack more-itertools patool keras-tqdm > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z3BYgukWSN0G",
    "outputId": "54766177-8890-4b5e-fc81-c36bf76e8aef",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "unrar is already the newest version (1:5.6.6-2build1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 34 not upgraded.\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "rar is already the newest version (2:5.5.0-1build1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 34 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "VAST = True\n",
    "\n",
    "if VAST:\n",
    "    !sudo apt-get install unrar\n",
    "    !sudo apt-get install rar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oY1UOJcjSN0H",
    "outputId": "723f2223-23e3-4305-c43b-d6aa7900dcf4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 12:10:21.891416: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nlp_ai_utils import *\n",
    "from chunks_urls import CHUNKS_URLS\n",
    "from updating_values import *\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "jPwfZb2FSN0H"
   },
   "outputs": [],
   "source": [
    "TF_ENABLE_ONEDNN_OPTS = 0\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "DATA_URLS = CHUNKS_URLS\n",
    "GLOVE_URL = 'https://f005.backblazeb2.com/file/glove-embeddings-dims/glove.6B.100d.txt'\n",
    "READY_DATA_URL = 'https://f005.backblazeb2.com/file/yelp-dataset-ready-for-models/ready_for_models.rar'\n",
    "CLASS_WEIGHTS_URL = 'https://f005.backblazeb2.com/file/yelp-dataset-ready-for-models/class_weights.pickle'\n",
    "UNIQUE_WORDS_URL = 'https://f005.backblazeb2.com/file/yelp-dataset-ready-for-models/unique_words.pickle'\n",
    "LIMIT = DATA_LIMIT\n",
    "TRAINED_MODELS_COUNT = TRAINED_MODELS\n",
    "LARGER_DATASET_PATH = \"../larger_dataset\"\n",
    "PREPROCESSED_CHUNKS_PATH = LARGER_DATASET_PATH + \"/preprocessed_data_chunks/\"\n",
    "BASE_FILE_NAME = \"chunk_\"\n",
    "FILE_FORMAT = \".csv\"\n",
    "DATA_IN_CHUNK = 116505\n",
    "if LIMIT == 60:\n",
    "    ACTUAL_DATA_SHAPE = DATA_IN_CHUNK * LIMIT + 20\n",
    "else:\n",
    "    ACTUAL_DATA_SHAPE = DATA_IN_CHUNK * LIMIT\n",
    "RANDOM_STATE = CONST_RANDOM_STATE\n",
    "READY_DATASET_PATH = \"../larger_dataset/ready_for_models/\"\n",
    "USE_READY_DATA = False\n",
    "USE_MODIFIED_LABELS = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_5pINYx5SN0H"
   },
   "source": [
    "### 1.1 Data Sourcing and Munging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DpjFMyE5SN0I"
   },
   "source": [
    "#### 1.1.1 Loading The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XmnbrbMgSN0I",
    "outputId": "ec66ec5d-415b-4354-946b-1a11c0644ccd",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_1 already exists.\n",
      "chunk_2 already exists.\n",
      "chunk_3 already exists.\n",
      "chunk_4 already exists.\n",
      "chunk_5 already exists.\n",
      "chunk_6 already exists.\n"
     ]
    }
   ],
   "source": [
    "if not USE_READY_DATA:\n",
    "    if not os.path.exists(LARGER_DATASET_PATH):\n",
    "        os.mkdir(LARGER_DATASET_PATH)\n",
    "    if not os.path.exists(PREPROCESSED_CHUNKS_PATH):\n",
    "        os.mkdir(PREPROCESSED_CHUNKS_PATH)\n",
    "    get_chunks(DATA_URLS,LIMIT,1,BASE_FILE_NAME,PREPROCESSED_CHUNKS_PATH,FILE_FORMAT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l-qySRdJSN0I"
   },
   "source": [
    "#### 1.1.2 Merging all Individual Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "D7inaNpWSN0J"
   },
   "outputs": [],
   "source": [
    "#get all names of downloaded files\n",
    "if not USE_READY_DATA:\n",
    "    all_file_names = get_all_file_names(BASE_FILE_NAME,LIMIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "tHPuDFKNSN0J"
   },
   "outputs": [],
   "source": [
    "#read all chunks into a list\n",
    "if not USE_READY_DATA:\n",
    "    list_dfs = read_chunks(all_file_names,PREPROCESSED_CHUNKS_PATH,FILE_FORMAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "9Qon9-WeSN0K"
   },
   "outputs": [],
   "source": [
    "#concatenate all chunks into a singular df\n",
    "if not USE_READY_DATA:\n",
    "    df = group_up_chunks(list_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ncXq_dEgSN0K"
   },
   "outputs": [],
   "source": [
    "#check how much of the data was actually downloaded\n",
    "if not USE_READY_DATA:\n",
    "    percent_loaded = check_no_missing_data(df.shape[0],ACTUAL_DATA_SHAPE)\n",
    "    percent_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "QotW0tPASN0K"
   },
   "outputs": [],
   "source": [
    "if not USE_READY_DATA:\n",
    "    review_data = df[['text', 'stars']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "9FNvqtvbSN0K"
   },
   "outputs": [],
   "source": [
    "if not USE_READY_DATA:\n",
    "    review_data.reset_index(inplace = True)\n",
    "    review_data.drop(['index'],axis = 1,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "N1i2zjVeSN0K"
   },
   "outputs": [],
   "source": [
    "if not USE_READY_DATA:\n",
    "    review_data.rename(columns = {'text':'full_review_text','stars':'star_rating'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ovSSxU4mSN0K"
   },
   "outputs": [],
   "source": [
    "if not USE_READY_DATA:\n",
    "    review_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "-mv5SWEKSN0L"
   },
   "outputs": [],
   "source": [
    "if not USE_READY_DATA:\n",
    "    review_data.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "qLAZOAj5SN0L"
   },
   "outputs": [],
   "source": [
    "if not USE_READY_DATA:\n",
    "    review_data['full_review_text'].replace('', np.nan, inplace=True)\n",
    "    review_data.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U3G6cAmfSN0L"
   },
   "source": [
    "## (2) Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "KznC9UBvSN0L"
   },
   "outputs": [],
   "source": [
    "if not USE_READY_DATA:\n",
    "    X = review_data['full_review_text']\n",
    "    y = review_data['star_rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "duSH-zs_SN0L"
   },
   "outputs": [],
   "source": [
    "if not USE_READY_DATA:\n",
    "    X = pd.Series([str(text) for text in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "LHUAg3OcSN0L"
   },
   "outputs": [],
   "source": [
    "if USE_MODIFIED_LABELS:\n",
    "    translated_labels = translate_labels(y)\n",
    "    y = pd.Series(encode_sent(translated_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "vg3vGHW8SN0L"
   },
   "outputs": [],
   "source": [
    "if not USE_READY_DATA:\n",
    "    y = [label - 1 for label in y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VIpf4Q45SN0M"
   },
   "source": [
    "### 2.0 Classes' Shapes Explanation\n",
    "\n",
    "#### Classes' shapes before and after (if USE_MODIFIED_LABELS = True):\n",
    "\n",
    "BEFORE: 1 ==> Very Bad ==> AFTER: 2 ==> Negative Sentiment\n",
    "\n",
    "BEFORE: 2 ==> Bad ==> AFTER: 2  ==> Negative Sentiment\n",
    "\n",
    "BEFORE: 3 ==> Ok ==> AFTER: 3  ==> Neutral Sentiment\n",
    "\n",
    "BEFORE: 4 ==> Good ==> AFTER: 1  ==> Positive Sentiment\n",
    "\n",
    "BEFORE: 5 ==> Very Good ==> AFTER: 1  ==> Positive Sentiment\n",
    "\n",
    "THEN: 1 is subtracted from each label. So, the Labels go FROM: 2,3,1 TO: 1,2,0\n",
    "\n",
    "#### Classes' shapes before and after (if USE_MODIFIED_LABELS = False):\n",
    "\n",
    "1 is subtracted from each label. So, the Labels go FROM: 1,2,3,4,5 TO: 0,1,2,3,4\n",
    "\n",
    "Why is 1 subracted from each label? To bring the data into the preferred shape of the class weights in both sklearn and keras (starting label is 0, not 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p7yxMjR6SN0M"
   },
   "source": [
    "### 2.1 Class Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "XVH9sMXgSN0M"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(\"../pickle_files\"):\n",
    "    os.mkdir(\"../pickle_files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "WhWgooyISN0M",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if USE_READY_DATA:\n",
    "    get_chunks([CLASS_WEIGHTS_URL],0,1,'class_weights','../pickle_files/','.pickle',False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "2G2j4grpSN0M"
   },
   "outputs": [],
   "source": [
    "if USE_READY_DATA:\n",
    "    class_weights = pickle.load(open(\"../pickle_files/class_weights.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "DoyKJaKxSN0M"
   },
   "outputs": [],
   "source": [
    "if not USE_READY_DATA:\n",
    "    y = pd.Series(y)\n",
    "    class_weights = compute_class_weight(class_weight = \"balanced\",classes = np.unique(y),y=y)\n",
    "    class_weights = dict(zip(np.unique(y), class_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zxF4eD_aSN0N"
   },
   "source": [
    "### 2.2 Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 397
    },
    "id": "3KoTC0c9SN0N",
    "outputId": "e900795b-5d73-4f2d-a269-57d4b40711f7",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding found.\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(\"../pickle_files/w2v_model.model\"):\n",
    "    print(\"Creating Embedding from scratch.\")\n",
    "    w2v_model = Word2Vec(sentences=[nltk.word_tokenize(text) for text in X], vector_size=100, window=5, min_count=1, workers=4)\n",
    "    w2v_model.save(\"../pickle_files/w2v_model.model\")\n",
    "else:\n",
    "    print(\"Embedding found.\")\n",
    "    w2v_model = Word2Vec.load(\"../pickle_files/w2v_model.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "g459zXaaSN0N"
   },
   "outputs": [],
   "source": [
    "if os.path.exists(\"../pickle_files/w2v_model.model\"):\n",
    "    w2v_model = Word2Vec.load(\"../pickle_files/w2v_model.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "j5k5jmriSN0N"
   },
   "outputs": [],
   "source": [
    "word_index_gensim = {}\n",
    "\n",
    "for i, word in enumerate(w2v_model.wv.key_to_index):\n",
    "    word_index_gensim[word] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "z28jvDbMSN0N"
   },
   "outputs": [],
   "source": [
    "vocab_size = len(word_index_gensim)\n",
    "embedding_dim_gensim = w2v_model.vector_size\n",
    "\n",
    "embedding_matrix_gensim = np.zeros((vocab_size, embedding_dim_gensim))\n",
    "\n",
    "for word, i in word_index_gensim.items():\n",
    "    if word in w2v_model.wv.key_to_index:\n",
    "        embedding_matrix_gensim[i] = w2v_model.wv.get_vector(word)\n",
    "\n",
    "EMBEDDING_LAYER_WORD2VEC = Embedding(vocab_size,\n",
    "                            embedding_dim_gensim,\n",
    "                            weights=[embedding_matrix_gensim],\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I-d6NdblSN0N"
   },
   "source": [
    "### 2.3 GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "YhEwuumlSN0O"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('../glove_files'):\n",
    "    os.mkdir('../glove_files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "L7JVLt_tSN0R",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove.6B.100d already exists.\n"
     ]
    }
   ],
   "source": [
    "if not USE_READY_DATA:\n",
    "    get_chunks([GLOVE_URL],0,1,\"glove.6B.100d\",'../glove_files/','.txt',False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "itztmYAmSN0R"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Unique Words...\n"
     ]
    }
   ],
   "source": [
    "if not USE_READY_DATA:\n",
    "    print('Getting Unique Words...')\n",
    "    UNIQUE_WORDS = set(' '.join(X).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "moiTV3fBSN0R"
   },
   "outputs": [],
   "source": [
    "if USE_READY_DATA:\n",
    "    get_chunks([UNIQUE_WORDS_URL],0,1,'unique_words',\"../pickle_files/\",'.pickle',False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "jk6btFMmSN0S"
   },
   "outputs": [],
   "source": [
    "if USE_READY_DATA:\n",
    "    UNIQUE_WORDS = pickle.load(open(\"../pickle_files/unique_words.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Trn0nRBSN0S"
   },
   "source": [
    "### 2.4 Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "YAfomsqcSN0S"
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(UNIQUE_WORDS)\n",
    "RNN_UNITS = 64\n",
    "CONV_FILTERS = 32\n",
    "CONV_KERNEL_SIZE = 3\n",
    "DROPOUT_PERCENT = 0.2\n",
    "DENSE_UNITS = 512\n",
    "LABELS_COUNT = len(y.unique())\n",
    "EMBEDDING_DIM = 100\n",
    "MAX_TEXT_LEN = 200\n",
    "TRUNC_TYPE = 'post'\n",
    "PADDING_TYPE = 'post'\n",
    "OOV_TOKEN = \"<OOV>\"\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bz4V60a_SN0S"
   },
   "source": [
    "### 2.5 Data Split\n",
    "\n",
    "##### ==> here is a quick explaination of how the dataset will be split using a smaller sample example.\n",
    "##### ==> dataset => 100\n",
    "##### ==> train_set => tr_s (example: 80)\n",
    "##### ==> valid_set => vs (example: 10)\n",
    "##### ==> test_set => te_s (example: 10)\n",
    "##### ==> t = tr_s (80) + vs (10)\n",
    "##### ==> train_set = x[:80]\n",
    "##### ==> valid_set = x[80:t]\n",
    "##### ==> test_set = x[t:] why t? because => vs = ts\n",
    "\n",
    "##### use this guideline if you are confused about how the train-validation-test split was done. Also, this is a future guide for me as well in case I forget."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1c-pMDAjSN0S"
   },
   "source": [
    "* train_set_size = 6,990,280 * 0.8 = 5,592,224\n",
    "* valid_set_size = 6,990,280 * 0.1 = 699,028\n",
    "* train_plus_valid = 5,592,224 + 699,028 = 6,291,252\n",
    "\n",
    "==> To Confirm: test_size = 6,990,280 - 6,291,252 = 699,028\n",
    "\n",
    "* train_set = [0:5,592,224]\n",
    "* train_labels = [0:5,592,224]\n",
    "* validation_set = [5,592,224:6,291,252] ==> 699,028\n",
    "* validation_labels = [5,592,224:6,291,252] ==> 699,028\n",
    "* test_set = [6,291,252,6,990,280] ==> 699,028\n",
    "* test_labels = [6,291,252,6,990,280] ==> 699,028"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "sIPyA_MzSN0S",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not USE_READY_DATA:\n",
    "    TRAIN_PERCENT = 0.8\n",
    "    VALID_TEST_PERCENT = 0.1\n",
    "    TRAIN_SIZE = int(ACTUAL_DATA_SHAPE * TRAIN_PERCENT)\n",
    "    VALID_TEST_SIZE = int(ACTUAL_DATA_SHAPE * VALID_TEST_PERCENT)\n",
    "    TOTAL_TEST_SIZE = TRAIN_SIZE + VALID_TEST_SIZE\n",
    "    train_set = X[:TRAIN_SIZE]\n",
    "    train_labels = y[:TRAIN_SIZE]\n",
    "    validation_set = X[TRAIN_SIZE:TOTAL_TEST_SIZE]\n",
    "    validation_labels = y[TRAIN_SIZE:TOTAL_TEST_SIZE]\n",
    "    test_set = X[TOTAL_TEST_SIZE:]\n",
    "    test_labels = y[TOTAL_TEST_SIZE:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "1QZTLqYuSN0T"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(\"../larger_dataset\"):\n",
    "    os.mkdir(\"../larger_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "YAbYtVwCSN0T"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(READY_DATASET_PATH):\n",
    "    os.mkdir(READY_DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "2MbKPixFSN0T",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if USE_READY_DATA:\n",
    "    get_chunks([READY_DATA_URL],0,1,'ready_for_models',READY_DATASET_PATH,'.rar',False)\n",
    "    Archive(os.path.join(READY_DATASET_PATH,\"ready_for_models.rar\")).extractall(READY_DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "q-faLUcSSN0T"
   },
   "outputs": [],
   "source": [
    "if USE_READY_DATA:\n",
    "    train_set_padded = pickle.load(open(READY_DATASET_PATH + \"train_set_padded.pickle\", \"rb\"))\n",
    "    train_labels = pickle.load(open(READY_DATASET_PATH + \"train_labels.pickle\", \"rb\"))\n",
    "    valid_set_padded = pickle.load(open(READY_DATASET_PATH + \"valid_set_padded.pickle\", \"rb\"))\n",
    "    validation_labels = pickle.load(open(READY_DATASET_PATH + \"validation_labels.pickle\", \"rb\"))\n",
    "    test_set_padded = pickle.load(open(READY_DATASET_PATH + \"test_set_padded.pickle\", \"rb\"))\n",
    "    test_labels = pickle.load(open(READY_DATASET_PATH + \"test_labels.pickle\", \"rb\"))\n",
    "    EMBEDDING_LAYER = pickle.load(open(READY_DATASET_PATH + \"EMBEDDING_LAYER.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "wKhnRQ8KSN0T"
   },
   "outputs": [],
   "source": [
    "# NOTE: THIS CELL TAKES A WHILE TO RUN.\n",
    "if not os.path.exists(READY_DATASET_PATH + \"train_set_padded.pickle\"):\n",
    "    print(\"Tokenizing the Dataset...\")\n",
    "    tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token=OOV_TOKEN)\n",
    "    tokenizer.fit_on_texts(train_set)\n",
    "    words_to_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "QbYJI9XcSN0U"
   },
   "outputs": [],
   "source": [
    "# NOTE: THIS CELL TAKES A WHILE TO RUN.\n",
    "if not os.path.exists(READY_DATASET_PATH + \"train_set_padded.pickle\"):\n",
    "    print(\"Padding the Training Set.\")\n",
    "    train_sequences = tokenizer.texts_to_sequences(train_set)\n",
    "    train_set_padded = pad_sequences(train_sequences, maxlen=MAX_TEXT_LEN, padding=PADDING_TYPE, truncating=TRUNC_TYPE)\n",
    "    \n",
    "if not os.path.exists(READY_DATASET_PATH + \"valid_set_padded.pickle\"):\n",
    "    print(\"Padding the Validation Set.\")\n",
    "    valid_sequences = tokenizer.texts_to_sequences(validation_set)\n",
    "    valid_set_padded = pad_sequences(valid_sequences, maxlen=MAX_TEXT_LEN, padding=PADDING_TYPE, truncating=TRUNC_TYPE)\n",
    "    \n",
    "if not os.path.exists(READY_DATASET_PATH + \"test_set_padded.pickle\"):\n",
    "    print(\"Padding the Testing Set.\")\n",
    "    test_sequences = tokenizer.texts_to_sequences(test_set)\n",
    "    test_set_padded = pad_sequences(test_sequences, maxlen=MAX_TEXT_LEN, padding=PADDING_TYPE, truncating=TRUNC_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "3fS_vJJTSN0U"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(READY_DATASET_PATH + \"train_set_padded.pickle\"):\n",
    "    print(\"Re-formatting Train Set Shape.\")\n",
    "    train_set_padded = np.array(train_set_padded)\n",
    "    \n",
    "if not os.path.exists(READY_DATASET_PATH + \"train_labels.pickle\"):\n",
    "    print(\"Re-formatting Train Labels Shape.\")\n",
    "    train_labels = np.array(train_labels)\n",
    "    \n",
    "if not os.path.exists(READY_DATASET_PATH + \"valid_set_padded.pickle\"): \n",
    "    print(\"Re-formatting Validation Set Shape.\")\n",
    "    valid_set_padded = np.array(valid_set_padded)\n",
    "\n",
    "if not os.path.exists(READY_DATASET_PATH + \"validation_labels.pickle\"):\n",
    "    print(\"Re-formatting Validation Labels Shape.\")\n",
    "    validation_labels = np.array(validation_labels)\n",
    "\n",
    "if not os.path.exists(READY_DATASET_PATH + \"test_set_padded.pickle\"):\n",
    "    print(\"Re-formatting Testing Set Shape.\")\n",
    "    test_set_padded = np.array(test_set_padded)\n",
    "\n",
    "if not os.path.exists(READY_DATASET_PATH + \"test_labels.pickle\"):\n",
    "    print(\"Re-formatting Testing Labels Shape.\")\n",
    "    test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "fm78zL3YSN0U"
   },
   "outputs": [],
   "source": [
    "if not USE_READY_DATA:\n",
    "    word_to_vec_map = read_glove_vector('../glove_files/glove.6B.100d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "68gkRbd6SN0V"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(READY_DATASET_PATH + \"EMBEDDING_LAYER_GLOVE.pickle\"):\n",
    "    print(\"Generating Embedding From Scratch.\")\n",
    "    vocab_mapping = len(words_to_index)\n",
    "    embed_vector_len = word_to_vec_map['moon'].shape[0]\n",
    "\n",
    "    emb_matrix = np.zeros((vocab_mapping, embed_vector_len))\n",
    "\n",
    "    for word, index in words_to_index.items():\n",
    "        embedding_vector = word_to_vec_map.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            emb_matrix[index, :] = embedding_vector\n",
    "\n",
    "    EMBEDDING_LAYER_GLOVE = Embedding(input_dim=vocab_mapping,\\\n",
    "                                output_dim=embed_vector_len, input_length=MAX_TEXT_LEN, weights = [emb_matrix], trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "LtIUeDjYSN0V"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Pickle File!\n",
      "Using Pickle File!\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(READY_DATASET_PATH + \"train_set_padded.pickle\"):\n",
    "    print(\"Using Pickle File!\")\n",
    "    train_set_padded = pickle.load(open(READY_DATASET_PATH + \"train_set_padded.pickle\", \"rb\"))\n",
    "else:\n",
    "    pickle_out = open(READY_DATASET_PATH + \"train_set_padded.pickle\",'wb')\n",
    "    pickle.dump(train_set_padded,pickle_out)\n",
    "    pickle_out.close()\n",
    "    \n",
    "if os.path.exists(READY_DATASET_PATH + \"train_labels.pickle\"):\n",
    "    print(\"Using Pickle File!\")\n",
    "    train_labels = pickle.load(open(READY_DATASET_PATH + \"train_labels.pickle\", \"rb\"))\n",
    "else:\n",
    "    pickle_out = open(READY_DATASET_PATH + \"train_labels.pickle\",'wb')\n",
    "    pickle.dump(train_labels,pickle_out)\n",
    "    pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "yYRv2FSwSN0V"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Pickle File!\n",
      "Using Pickle File!\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(READY_DATASET_PATH + \"valid_set_padded.pickle\"):\n",
    "    print(\"Using Pickle File!\")\n",
    "    valid_set_padded = pickle.load(open(READY_DATASET_PATH + \"valid_set_padded.pickle\", \"rb\"))\n",
    "else:\n",
    "    pickle_out = open(READY_DATASET_PATH + \"valid_set_padded.pickle\",'wb')\n",
    "    pickle.dump(valid_set_padded,pickle_out)\n",
    "    pickle_out.close()\n",
    "    \n",
    "if os.path.exists(READY_DATASET_PATH + \"validation_labels.pickle\"):\n",
    "    print(\"Using Pickle File!\")\n",
    "    validation_labels = pickle.load(open(READY_DATASET_PATH + \"validation_labels.pickle\", \"rb\"))\n",
    "else:\n",
    "    pickle_out = open(READY_DATASET_PATH + \"validation_labels.pickle\",'wb')\n",
    "    pickle.dump(validation_labels,pickle_out)\n",
    "    pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "ZF0gK8SkSN0V"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Pickle File!\n",
      "Using Pickle File!\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(READY_DATASET_PATH + \"test_set_padded.pickle\"):\n",
    "    print(\"Using Pickle File!\")\n",
    "    test_set_padded = pickle.load(open(READY_DATASET_PATH + \"test_set_padded.pickle\", \"rb\"))\n",
    "else:\n",
    "    pickle_out = open(READY_DATASET_PATH + \"test_set_padded.pickle\",'wb')\n",
    "    pickle.dump(test_set_padded,pickle_out)\n",
    "    pickle_out.close()\n",
    "    \n",
    "if os.path.exists(READY_DATASET_PATH + \"test_labels.pickle\"):\n",
    "    print(\"Using Pickle File!\")\n",
    "    test_labels = pickle.load(open(READY_DATASET_PATH + \"test_labels.pickle\", \"rb\"))\n",
    "else:\n",
    "    pickle_out = open(READY_DATASET_PATH + \"test_labels.pickle\",'wb')\n",
    "    pickle.dump(test_labels,pickle_out)\n",
    "    pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "gG2lVc88SN0V"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Pickle File!\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(READY_DATASET_PATH + \"EMBEDDING_LAYER_GLOVE.pickle\"):\n",
    "    print(\"Using Pickle File!\")\n",
    "    EMBEDDING_LAYER_GLOVE = pickle.load(open(READY_DATASET_PATH + \"EMBEDDING_LAYER_GLOVE.pickle\", \"rb\"))\n",
    "else:\n",
    "    print(\"Pickling Embedding Layer!\")\n",
    "    pickle_out = open(READY_DATASET_PATH + \"EMBEDDING_LAYER_GLOVE.pickle\",'wb')\n",
    "    pickle.dump(EMBEDDING_LAYER_GLOVE,pickle_out)\n",
    "    pickle_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bG3d6YPQSN0W"
   },
   "source": [
    "## (3) RNN with Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "BN09_mPxSN0W"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 12:10:40.990714: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-10 12:10:40.997967: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-10 12:10:40.998541: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    }
   ],
   "source": [
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "for device in gpu_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "bfQIqn3LSN0W",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 12:10:41.030704: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-10 12:10:41.031374: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-10 12:10:41.031727: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-10 12:10:41.484248: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-10 12:10:41.484657: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-10 12:10:41.485022: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-10 12:10:41.485353: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22250 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:61:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(RANDOM_STATE)\n",
    "np.random.seed(RANDOM_STATE)\n",
    "rnn_model_w2v = tf.keras.models.Sequential([\n",
    "    EMBEDDING_LAYER_WORD2VEC,\n",
    "    tf.keras.layers.Conv1D(filters=CONV_FILTERS, kernel_size=CONV_KERNEL_SIZE, padding='same', activation='relu'),\n",
    "    tf.keras.layers.GlobalMaxPooling1D(keepdims = True),\n",
    "    tf.keras.layers.SimpleRNN(RNN_UNITS),\n",
    "    tf.keras.layers.Dense(DENSE_UNITS, activation = \"relu\"),\n",
    "    tf.keras.layers.Dense(LABELS_COUNT, activation = \"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "S4GHgZMKSN0W",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 100)         25334700  \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, None, 32)          9632      \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 1, 32)            0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " simple_rnn (SimpleRNN)      (None, 64)                6208      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               33280     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 1539      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 25,385,359\n",
      "Trainable params: 50,659\n",
      "Non-trainable params: 25,334,700\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "rnn_model_w2v.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "bccmrhBSSN0W"
   },
   "outputs": [],
   "source": [
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"./logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "NRCdddyvSN0W"
   },
   "outputs": [],
   "source": [
    "metrics_callback = MetricsCallback(test_data = valid_set_padded, y_true = validation_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "AKIUmJxqSN0W",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 12:10:43.478914: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8600\n",
      "2023-05-10 12:10:44.033625: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:637] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2023-05-10 12:10:44.035853: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x46ef83d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-05-10 12:10:44.035883: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA GeForce RTX 3090, Compute Capability 8.6\n",
      "2023-05-10 12:10:44.039363: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-05-10 12:10:44.140898: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    1/17476 [..............................] - ETA: 15:35:18 - loss: 1.1886 - accuracy: 0.6562WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0033s vs `on_train_batch_end` time: 0.0042s). Check your callbacks.\n",
      "2185/2185 [==============================] - 2s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.73      0.83     45732\n",
      "           1       0.80      0.78      0.79     18313\n",
      "           2       0.22      0.66      0.33      5858\n",
      "\n",
      "    accuracy                           0.74     69903\n",
      "   macro avg       0.66      0.72      0.65     69903\n",
      "weighted avg       0.86      0.74      0.78     69903\n",
      "\n",
      "Macro Weighted F1-Score: 0.7759539852696815\n",
      "17476/17476 [==============================] - 82s 4ms/step - loss: 0.7279 - accuracy: 0.7090 - val_loss: 0.6173 - val_accuracy: 0.7352 - lr: 0.0010\n",
      "Epoch 2/15\n",
      "2185/2185 [==============================] - 2s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.74      0.84     45732\n",
      "           1       0.83      0.76      0.79     18313\n",
      "           2       0.23      0.70      0.34      5858\n",
      "\n",
      "    accuracy                           0.74     69903\n",
      "   macro avg       0.67      0.73      0.66     69903\n",
      "weighted avg       0.87      0.74      0.78     69903\n",
      "\n",
      "Macro Weighted F1-Score: 0.7836387111509789\n",
      "17476/17476 [==============================] - 79s 5ms/step - loss: 0.6625 - accuracy: 0.7441 - val_loss: 0.6162 - val_accuracy: 0.7418 - lr: 0.0010\n",
      "Epoch 3/15\n",
      "2185/2185 [==============================] - 2s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.87      0.89     45732\n",
      "           1       0.85      0.76      0.80     18313\n",
      "           2       0.30      0.55      0.39      5858\n",
      "\n",
      "    accuracy                           0.81     69903\n",
      "   macro avg       0.69      0.73      0.70     69903\n",
      "weighted avg       0.85      0.81      0.83     69903\n",
      "\n",
      "Macro Weighted F1-Score: 0.8290888824609759\n",
      "17476/17476 [==============================] - 78s 4ms/step - loss: 0.6446 - accuracy: 0.7534 - val_loss: 0.4873 - val_accuracy: 0.8130 - lr: 0.0010\n",
      "Epoch 4/15\n",
      "2185/2185 [==============================] - 2s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.79      0.87     45732\n",
      "           1       0.86      0.71      0.78     18313\n",
      "           2       0.25      0.71      0.37      5858\n",
      "\n",
      "    accuracy                           0.76     69903\n",
      "   macro avg       0.69      0.74      0.67     69903\n",
      "weighted avg       0.87      0.76      0.80     69903\n",
      "\n",
      "Macro Weighted F1-Score: 0.8017721986715695\n",
      "17476/17476 [==============================] - 79s 5ms/step - loss: 0.6340 - accuracy: 0.7558 - val_loss: 0.5810 - val_accuracy: 0.7649 - lr: 0.0010\n",
      "Epoch 5/15\n",
      "2185/2185 [==============================] - 2s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.80      0.87     45732\n",
      "           1       0.84      0.76      0.80     18313\n",
      "           2       0.26      0.67      0.38      5858\n",
      "\n",
      "    accuracy                           0.78     69903\n",
      "   macro avg       0.69      0.74      0.68     69903\n",
      "weighted avg       0.87      0.78      0.81     69903\n",
      "\n",
      "Macro Weighted F1-Score: 0.8122339414365608\n",
      "17476/17476 [==============================] - 78s 4ms/step - loss: 0.6272 - accuracy: 0.7585 - val_loss: 0.5623 - val_accuracy: 0.7815 - lr: 0.0010\n",
      "Epoch 6/15\n",
      "2185/2185 [==============================] - 2s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.79      0.87     45732\n",
      "           1       0.82      0.81      0.82     18313\n",
      "           2       0.27      0.64      0.38      5858\n",
      "\n",
      "    accuracy                           0.79     69903\n",
      "   macro avg       0.68      0.75      0.69     69903\n",
      "weighted avg       0.87      0.79      0.81     69903\n",
      "\n",
      "Macro Weighted F1-Score: 0.8148835420616013\n",
      "17476/17476 [==============================] - 79s 5ms/step - loss: 0.5967 - accuracy: 0.7717 - val_loss: 0.5182 - val_accuracy: 0.7864 - lr: 1.0000e-04\n",
      "Epoch 7/15\n",
      "2185/2185 [==============================] - 2s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.80      0.87     45732\n",
      "           1       0.84      0.79      0.81     18313\n",
      "           2       0.26      0.66      0.38      5858\n",
      "\n",
      "    accuracy                           0.78     69903\n",
      "   macro avg       0.69      0.75      0.69     69903\n",
      "weighted avg       0.87      0.78      0.81     69903\n",
      "\n",
      "Macro Weighted F1-Score: 0.8147660097923776\n",
      "17476/17476 [==============================] - 80s 5ms/step - loss: 0.5868 - accuracy: 0.7742 - val_loss: 0.5057 - val_accuracy: 0.7844 - lr: 1.0000e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7efcaeeffd60>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_model_w2v.compile(optimizer=Adam(), loss = SparseCategoricalCrossentropy(), metrics=['accuracy'])\n",
    "rnn_model_w2v.fit(train_set_padded, train_labels,validation_data = (valid_set_padded,validation_labels),\\\n",
    "              batch_size=BATCH_SIZE, epochs=EPOCHS,class_weight=class_weights,\\\n",
    "              callbacks=[tensorboard_callback,metrics_callback,EarlyStopping(patience=4),ReduceLROnPlateau(factor=0.1, patience=2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "daGiQKYlSN0X"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(\"../saved_models\"):\n",
    "    os.mkdir(\"../saved_models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "SVbWTV2NSN0X"
   },
   "outputs": [],
   "source": [
    "TRAINED_MODELS_COUNT = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "rnn_model_w2v.save_weights(\"../saved_models/rnn_model_w2v_\" + str(TRAINED_MODELS_COUNT) + \".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "LMEjzvtUSN0X"
   },
   "outputs": [],
   "source": [
    "pickle_out = open(\"../saved_models/rnn_model_w2v_params_\" + str(TRAINED_MODELS_COUNT) + \".pickle\",'wb')\n",
    "pickle.dump({'EMBEDDING_DIM':EMBEDDING_DIM,'MAX_TEXT_LEN':MAX_TEXT_LEN,'BATCH_SIZE':BATCH_SIZE,'EPOCHS':EPOCHS,\\\n",
    "             'train_set_size':len(train_set),'RNN_UNITS':RNN_UNITS,\\\n",
    "             'CONV_FILTERS':CONV_FILTERS,'CONV_KERNEL_SIZE':CONV_KERNEL_SIZE,\n",
    "             'chunks_used':(len(train_set) + len(validation_set) + len(test_set)) // DATA_IN_CHUNK,\\\n",
    "                'DENSE_UNITS':DENSE_UNITS,'LABELS_COUNT':LABELS_COUNT},pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aFksBET6SN0X"
   },
   "source": [
    "## (4) RNN with GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "ORKjgV_vSN0X"
   },
   "outputs": [],
   "source": [
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "for device in gpu_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "v41jWXLgSN0X"
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(RANDOM_STATE)\n",
    "np.random.seed(RANDOM_STATE)\n",
    "rnn_model_glove = tf.keras.models.Sequential([\n",
    "    EMBEDDING_LAYER_GLOVE,\n",
    "    tf.keras.layers.Conv1D(filters=CONV_FILTERS, kernel_size=CONV_KERNEL_SIZE, padding='same', activation='relu'),\n",
    "    tf.keras.layers.GlobalMaxPooling1D(keepdims = True),\n",
    "    tf.keras.layers.SimpleRNN(RNN_UNITS),\n",
    "    tf.keras.layers.Dense(DENSE_UNITS, activation = \"relu\"),\n",
    "    tf.keras.layers.Dense(LABELS_COUNT, activation = \"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "ysqlAseVSN0X"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 200, 100)          21927500  \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 200, 32)           9632      \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 1, 32)            0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " simple_rnn (SimpleRNN)      (None, 64)                6208      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               33280     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 1539      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21,978,159\n",
      "Trainable params: 21,978,159\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "rnn_model_glove.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "XefltqCpSN0Y"
   },
   "outputs": [],
   "source": [
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"./logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "Ufsmbsd7SN0Y"
   },
   "outputs": [],
   "source": [
    "metrics_callback = MetricsCallback(test_data = valid_set_padded, y_true = validation_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "4cBIXhUsSN0Y",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "2185/2185 [==============================] - 3s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.77      0.87     45732\n",
      "           1       0.88      0.85      0.86     18313\n",
      "           2       0.26      0.72      0.38      5858\n",
      "\n",
      "    accuracy                           0.79     69903\n",
      "   macro avg       0.71      0.78      0.70     69903\n",
      "weighted avg       0.89      0.79      0.82     69903\n",
      "\n",
      "Macro Weighted F1-Score: 0.8245087694979837\n",
      "17476/17476 [==============================] - 194s 11ms/step - loss: 0.5765 - accuracy: 0.7812 - val_loss: 0.5029 - val_accuracy: 0.7890 - lr: 0.0010\n",
      "Epoch 2/15\n",
      "2185/2185 [==============================] - 2s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.83      0.90     45732\n",
      "           1       0.90      0.82      0.86     18313\n",
      "           2       0.30      0.73      0.43      5858\n",
      "\n",
      "    accuracy                           0.82     69903\n",
      "   macro avg       0.73      0.79      0.73     69903\n",
      "weighted avg       0.90      0.82      0.85     69903\n",
      "\n",
      "Macro Weighted F1-Score: 0.8473602093991996\n",
      "17476/17476 [==============================] - 112s 6ms/step - loss: 0.4850 - accuracy: 0.8210 - val_loss: 0.4137 - val_accuracy: 0.8193 - lr: 0.0010\n",
      "Epoch 3/15\n",
      "2185/2185 [==============================] - 2s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.86      0.91     45732\n",
      "           1       0.89      0.82      0.86     18313\n",
      "           2       0.32      0.68      0.43      5858\n",
      "\n",
      "    accuracy                           0.83     69903\n",
      "   macro avg       0.73      0.79      0.73     69903\n",
      "weighted avg       0.89      0.83      0.86     69903\n",
      "\n",
      "Macro Weighted F1-Score: 0.8557730567989444\n",
      "17476/17476 [==============================] - 107s 6ms/step - loss: 0.4228 - accuracy: 0.8437 - val_loss: 0.3999 - val_accuracy: 0.8334 - lr: 0.0010\n",
      "Epoch 4/15\n",
      "2185/2185 [==============================] - 2s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.86      0.91     45732\n",
      "           1       0.91      0.79      0.84     18313\n",
      "           2       0.30      0.70      0.42      5858\n",
      "\n",
      "    accuracy                           0.83     69903\n",
      "   macro avg       0.73      0.78      0.73     69903\n",
      "weighted avg       0.90      0.83      0.85     69903\n",
      "\n",
      "Macro Weighted F1-Score: 0.8512791478271882\n",
      "17476/17476 [==============================] - 106s 6ms/step - loss: 0.3654 - accuracy: 0.8661 - val_loss: 0.4183 - val_accuracy: 0.8263 - lr: 0.0010\n",
      "Epoch 5/15\n",
      "2185/2185 [==============================] - 3s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.85      0.90     45732\n",
      "           1       0.91      0.78      0.84     18313\n",
      "           2       0.30      0.70      0.42      5858\n",
      "\n",
      "    accuracy                           0.82     69903\n",
      "   macro avg       0.72      0.78      0.72     69903\n",
      "weighted avg       0.89      0.82      0.85     69903\n",
      "\n",
      "Macro Weighted F1-Score: 0.8464425635794802\n",
      "17476/17476 [==============================] - 106s 6ms/step - loss: 0.3146 - accuracy: 0.8848 - val_loss: 0.4396 - val_accuracy: 0.8199 - lr: 0.0010\n",
      "Epoch 6/15\n",
      "2185/2185 [==============================] - 3s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.87      0.91     45732\n",
      "           1       0.88      0.83      0.85     18313\n",
      "           2       0.32      0.61      0.42      5858\n",
      "\n",
      "    accuracy                           0.84     69903\n",
      "   macro avg       0.72      0.77      0.73     69903\n",
      "weighted avg       0.89      0.84      0.86     69903\n",
      "\n",
      "Macro Weighted F1-Score: 0.8551633324478497\n",
      "17476/17476 [==============================] - 106s 6ms/step - loss: 0.2248 - accuracy: 0.9191 - val_loss: 0.4907 - val_accuracy: 0.8364 - lr: 1.0000e-04\n",
      "Epoch 7/15\n",
      "2185/2185 [==============================] - 2s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.88      0.92     45732\n",
      "           1       0.88      0.83      0.85     18313\n",
      "           2       0.33      0.58      0.42      5858\n",
      "\n",
      "    accuracy                           0.84     69903\n",
      "   macro avg       0.72      0.76      0.73     69903\n",
      "weighted avg       0.88      0.84      0.86     69903\n",
      "\n",
      "Macro Weighted F1-Score: 0.8579561352985471\n",
      "17476/17476 [==============================] - 105s 6ms/step - loss: 0.1979 - accuracy: 0.9285 - val_loss: 0.5029 - val_accuracy: 0.8418 - lr: 1.0000e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7efcaeeff1c0>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_model_glove.compile(optimizer=Adam(), loss = SparseCategoricalCrossentropy(), metrics=['accuracy'])\n",
    "rnn_model_glove.fit(train_set_padded, train_labels,validation_data = (valid_set_padded,validation_labels),\\\n",
    "              batch_size=BATCH_SIZE, epochs=EPOCHS,class_weight=class_weights,\\\n",
    "              callbacks=[tensorboard_callback,metrics_callback,EarlyStopping(patience=4),ReduceLROnPlateau(factor=0.1, patience=2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "AF19BDgdSN0Y"
   },
   "outputs": [],
   "source": [
    "TRAINED_MODELS_COUNT = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "rnn_model_glove.save_weights(\"../saved_models/rnn_model_glove_\" + str(TRAINED_MODELS_COUNT) + \".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "LLRLJdQJSN0Y"
   },
   "outputs": [],
   "source": [
    "pickle_out = open(\"../saved_models/rnn_model_glove_params_\" + str(TRAINED_MODELS_COUNT) + \".pickle\",'wb')\n",
    "pickle.dump({'EMBEDDING_DIM':EMBEDDING_DIM,'MAX_TEXT_LEN':MAX_TEXT_LEN,'BATCH_SIZE':BATCH_SIZE,'EPOCHS':EPOCHS,\\\n",
    "             'train_set_size':len(train_set),'RNN_UNITS':RNN_UNITS,\\\n",
    "             'CONV_FILTERS':CONV_FILTERS,'CONV_KERNEL_SIZE':CONV_KERNEL_SIZE,\n",
    "             'chunks_used':(len(train_set) + len(validation_set) + len(test_set)) // DATA_IN_CHUNK,\\\n",
    "                'DENSE_UNITS':DENSE_UNITS,'LABELS_COUNT':LABELS_COUNT},pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vn_uBxcjSN0Y"
   },
   "source": [
    "## (5) Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "WEueWs3oSN0Z"
   },
   "outputs": [],
   "source": [
    "if VAST:\n",
    "    !tar -czf rnn_logs.tar.gz logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rKJjTbLWSN0Z"
   },
   "source": [
    "### 5.1 Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dDtH1K0WSN0Z"
   },
   "source": [
    "### 5.2 Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lv0LcxbTSN0Z"
   },
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir logs/fit"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
