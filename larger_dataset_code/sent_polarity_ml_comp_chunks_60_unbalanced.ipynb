{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rifd8reqW3F6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The system cannot find the path specified.\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas numpy nltk scikit-learn wordcloud seaborn gensim tensorflow imblearn xgboost matplotlib unrar pyunpack more-itertools patool > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K95MLp74Whnn",
        "outputId": "26b3c0a8-7b9c-47c7-d2db-edebf00644fc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
            "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\.libs\\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll\n",
            "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
            "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package vader_lexicon to\n",
            "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nlp_ai_utils import *\n",
        "from chunks_urls import CHUNKS_URLS\n",
        "from updating_values import DATA_LIMIT\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('vader_lexicon')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "HKLXkAIk4xWR"
      },
      "outputs": [],
      "source": [
        "#Set this to false if you are running this on your local machine, and true if you are running this on google colab\n",
        "GDRIVE = False\n",
        "\n",
        "LOCAL_DIR = '../pickle_files/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "p5TthWK2Whno"
      },
      "outputs": [],
      "source": [
        "#TF_ENABLE_ONEDNN_OPTS = 0\n",
        "URLS = CHUNKS_URLS\n",
        "LIMIT = 60\n",
        "LARGER_DATASET_PATH = \"../larger_dataset\"\n",
        "PREPROCESSED_CHUNKS_PATH = LARGER_DATASET_PATH + \"/preprocessed_data_chunks/\"\n",
        "BASE_FILE_NAME = \"chunk_\"\n",
        "FILE_FORMAT = \".csv\"\n",
        "ACTUAL_DATA_SHAPE = 6990280\n",
        "RANDOM_STATE = 42\n",
        "USE_MODIFIED_LABELS = True\n",
        "BALANCE_DATA = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_zFMpwCWhno",
        "outputId": "b1f3b37d-804d-4c1d-dd58-7812f9e15977"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "chunk_1 already exists.\n",
            "chunk_2 already exists.\n",
            "chunk_3 already exists.\n",
            "chunk_4 already exists.\n",
            "chunk_5 already exists.\n",
            "chunk_6 already exists.\n",
            "chunk_7 already exists.\n",
            "chunk_8 already exists.\n",
            "chunk_9 already exists.\n",
            "chunk_10 already exists.\n",
            "chunk_11 already exists.\n",
            "chunk_12 already exists.\n",
            "chunk_13 already exists.\n",
            "chunk_14 already exists.\n",
            "chunk_15 already exists.\n",
            "chunk_16 already exists.\n",
            "chunk_17 already exists.\n",
            "chunk_18 already exists.\n",
            "chunk_19 already exists.\n",
            "chunk_20 already exists.\n",
            "chunk_21 already exists.\n",
            "chunk_22 already exists.\n",
            "chunk_23 already exists.\n",
            "chunk_24 already exists.\n",
            "chunk_25 already exists.\n",
            "chunk_26 already exists.\n",
            "chunk_27 already exists.\n",
            "chunk_28 already exists.\n",
            "chunk_29 already exists.\n",
            "chunk_30 already exists.\n",
            "chunk_31 already exists.\n",
            "chunk_32 already exists.\n",
            "chunk_33 already exists.\n",
            "chunk_34 already exists.\n",
            "chunk_35 already exists.\n",
            "chunk_36 already exists.\n",
            "chunk_37 already exists.\n",
            "chunk_38 already exists.\n",
            "chunk_39 already exists.\n",
            "chunk_40 already exists.\n",
            "chunk_41 already exists.\n",
            "chunk_42 already exists.\n",
            "chunk_43 already exists.\n",
            "chunk_44 already exists.\n",
            "chunk_45 already exists.\n",
            "chunk_46 already exists.\n",
            "chunk_47 already exists.\n",
            "chunk_48 already exists.\n",
            "chunk_49 already exists.\n",
            "chunk_50 already exists.\n",
            "chunk_51 already exists.\n",
            "chunk_52 already exists.\n",
            "chunk_53 already exists.\n",
            "chunk_54 already exists.\n",
            "chunk_55 already exists.\n",
            "chunk_56 already exists.\n",
            "chunk_57 already exists.\n",
            "chunk_58 already exists.\n",
            "chunk_59 already exists.\n",
            "chunk_60 already exists.\n"
          ]
        }
      ],
      "source": [
        "if not os.path.exists(LARGER_DATASET_PATH):\n",
        "    os.mkdir(LARGER_DATASET_PATH)\n",
        "if not os.path.exists(PREPROCESSED_CHUNKS_PATH):\n",
        "    os.mkdir(PREPROCESSED_CHUNKS_PATH)\n",
        "get_chunks(URLS,LIMIT,1,BASE_FILE_NAME,PREPROCESSED_CHUNKS_PATH,FILE_FORMAT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "wmWRJ1SFWhno"
      },
      "outputs": [],
      "source": [
        "#get all names of downloaded files\n",
        "all_file_names = get_all_file_names(BASE_FILE_NAME,LIMIT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "DAZRHnjmWhno"
      },
      "outputs": [],
      "source": [
        "#read all chunks into a list\n",
        "list_dfs = read_chunks(all_file_names,PREPROCESSED_CHUNKS_PATH,FILE_FORMAT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "YI2hHGAUWhno"
      },
      "outputs": [],
      "source": [
        "#concatenate all chunks into a singular df\n",
        "df = group_up_chunks(list_dfs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "fEjoDPCmWhno"
      },
      "outputs": [],
      "source": [
        "review_data = df[['text', 'stars']].copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "KMInJzTLWhnp"
      },
      "outputs": [],
      "source": [
        "review_data.reset_index(inplace = True)\n",
        "review_data.drop(['index'],axis = 1,inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "4kuvY3cfWhnp"
      },
      "outputs": [],
      "source": [
        "review_data.rename(columns = {'text':'full_review_text','stars':'star_rating'}, inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "VDNLY2NaWhnp"
      },
      "outputs": [],
      "source": [
        "review_data['full_review_text'] = pd.Series([str(text) for text in review_data['full_review_text']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "v6NDgOeZWhnp"
      },
      "outputs": [],
      "source": [
        "review_data.isnull().sum()\n",
        "review_data.dropna(inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Xm0rucFqWhnp"
      },
      "outputs": [],
      "source": [
        "review_data['full_review_text'].replace('', np.nan, inplace=True)\n",
        "review_data.dropna(inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2ZLPQ5EWhnp",
        "outputId": "439a322a-7e9a-4b21-e457-0f36fb8237cf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "6990280"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(review_data['full_review_text'])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Balancing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "if BALANCE_DATA:\n",
        "    if not USE_MODIFIED_LABELS:\n",
        "        label_1 = review_data[review_data['star_rating'] == 1]\n",
        "        label_2 = review_data[review_data['star_rating'] == 2]\n",
        "        label_3 = review_data[review_data['star_rating'] == 3]\n",
        "        label_4 = review_data[review_data['star_rating'] == 4]\n",
        "        label_5 = review_data[review_data['star_rating'] == 5]\n",
        "\n",
        "        minority_class = min([label_1.shape[0],label_2.shape[0],label_3.shape[0],label_4.shape[0],label_5.shape[0]])\n",
        "    else:\n",
        "        label_1 = review_data[review_data['star_rating'] == 1]\n",
        "        label_2 = review_data[review_data['star_rating'] == 2]\n",
        "        label_3 = review_data[review_data['star_rating'] == 3]\n",
        "        label_4 = review_data[review_data['star_rating'] == 4]\n",
        "        label_5 = review_data[review_data['star_rating'] == 5]\n",
        "\n",
        "        minority_class = label_3.shape[0] / 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "if BALANCE_DATA:\n",
        "    if USE_MODIFIED_LABELS:\n",
        "        review_data_label_1 = label_1[:int(minority_class)]\n",
        "        review_data_label_2 = label_2[:int(minority_class)]\n",
        "        review_data_label_3 = label_3\n",
        "        review_data_label_4 = label_4[:int(minority_class)]\n",
        "        review_data_label_5 = label_5[:int(minority_class)]\n",
        "\n",
        "    else:\n",
        "        review_data_label_1 = label_1[:int(minority_class)]\n",
        "        review_data_label_2 = label_2[:int(minority_class)]\n",
        "        review_data_label_3 = label_3[:int(minority_class)]\n",
        "        review_data_label_4 = label_4[:int(minority_class)]\n",
        "        review_data_label_5 = label_5[:int(minority_class)]\n",
        "\n",
        "    review_data = pd.concat([review_data_label_1,review_data_label_2,review_data_label_3,review_data_label_4,review_data_label_5])\n",
        "\n",
        "    print(review_data.shape)\n",
        "\n",
        "    print(review_data['star_rating'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "if BALANCE_DATA:\n",
        "    review_data = review_data.sample(frac=1,random_state=RANDOM_STATE)\n",
        "    review_data.reset_index(drop=True,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = review_data['full_review_text']\n",
        "y = review_data['star_rating']"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8naulMOaWhnp"
      },
      "source": [
        "## Comparisons"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4Cy1d2F7Whnq"
      },
      "source": [
        "### Sentiment Polarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "293RkaYJWhnq"
      },
      "outputs": [],
      "source": [
        "if GDRIVE:\n",
        "  if os.path.exists(\"sent_data.pickle\"):\n",
        "      sent_data = pickle.load(open(\"sent_data.pickle\",'rb'))\n",
        "  else:\n",
        "      sid = SentimentIntensityAnalyzer()\n",
        "      sent_polarity_info = [sid.polarity_scores(review) for review in review_data['full_review_text']]\n",
        "else:\n",
        "  if os.path.exists(LOCAL_DIR + \"sent_data.pickle\"):\n",
        "      sent_data = pickle.load(open(LOCAL_DIR + \"sent_data.pickle\",'rb'))\n",
        "  else:\n",
        "      sid = SentimentIntensityAnalyzer()\n",
        "      sent_polarity_info = [sid.polarity_scores(review) for review in review_data['full_review_text']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "S_DoH38qWhnq"
      },
      "outputs": [],
      "source": [
        "if GDRIVE:\n",
        "  if not os.path.exists(\"sent_data.pickle\"):\n",
        "      review_sentiment = [classify_sentiment(scores) for scores in sent_polarity_info]\n",
        "      sent_polarity = [extract_sent_polarity(scores) for scores in sent_polarity_info]\n",
        "      review_data['str_sent'] = review_sentiment\n",
        "      review_data['sent_polarity'] = sent_polarity\n",
        "      sentiment_labels = translate_labels(y)\n",
        "      y_true_sent = encode_sent(sentiment_labels)\n",
        "      y_pred_sent = encode_sent(review_data['str_sent'])\n",
        "else:\n",
        "  if not os.path.exists(\"../pickle_files/sent_data.pickle\"):\n",
        "    review_sentiment = [classify_sentiment(scores) for scores in sent_polarity_info]\n",
        "    sent_polarity = [extract_sent_polarity(scores) for scores in sent_polarity_info]\n",
        "    review_data['str_sent'] = review_sentiment\n",
        "    review_data['sent_polarity'] = sent_polarity\n",
        "    sentiment_labels = translate_labels(y)\n",
        "    y_true_sent = encode_sent(sentiment_labels)\n",
        "    y_pred_sent = encode_sent(review_data['str_sent'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "SBidEoAIWhnr"
      },
      "outputs": [],
      "source": [
        "if GDRIVE:\n",
        "  if not os.path.exists(\"sent_data.pickle\"):\n",
        "      sent_data = review_data[['str_sent', 'sent_polarity']].copy()\n",
        "      pickle_out = open(\"sent_data.pickle\",'wb')\n",
        "      pickle.dump(sent_data,pickle_out)\n",
        "      pickle_out.close()\n",
        "else:\n",
        "  if not os.path.exists(\"../pickle_files/sent_data.pickle\"):\n",
        "    sent_data = review_data[['str_sent', 'sent_polarity']].copy()\n",
        "    pickle_out = open(\"../pickle_files/sent_data.pickle\",'wb')\n",
        "    pickle.dump(sent_data,pickle_out)\n",
        "    pickle_out.close() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "VXIDNPcgWhnr"
      },
      "outputs": [],
      "source": [
        "if GDRIVE:\n",
        "  if not os.path.exists(\"sent_data.pickle\"):\n",
        "      print(metrics.accuracy_score(y_true_sent,y_pred_sent))\n",
        "else:\n",
        "  if not os.path.exists(\"../pickle_files/sent_data.pickle\"):\n",
        "    print(metrics.accuracy_score(y_true_sent,y_pred_sent))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ayl0pBrwAlI9",
        "outputId": "555ae394-1704-49c0-ef17-9b3d15105a60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.74973305790326\n"
          ]
        }
      ],
      "source": [
        "sentiment_labels = translate_labels(y)\n",
        "y_true_sent = encode_sent(sentiment_labels)\n",
        "y_pred_sent = encode_sent(sent_data['str_sent'])\n",
        "print(metrics.accuracy_score(y_true_sent,y_pred_sent))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vG84ADPtWhnr"
      },
      "source": [
        "### Machine Learning Models"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "sZ0MJhumWhnr"
      },
      "source": [
        "#### Word Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "wX79PWYqWhnr"
      },
      "outputs": [],
      "source": [
        "if GDRIVE:\n",
        "  if not os.path.exists('pickle_files'):\n",
        "      os.mkdir('pickle_files')\n",
        "else:\n",
        "  if not os.path.exists('../pickle_files'):\n",
        "    os.mkdir('../pickle_files')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "qa2i6gGCWhnr"
      },
      "outputs": [],
      "source": [
        "count_model = CountVectorizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6DgGc5AWhnr",
        "outputId": "c9c42c04-cb56-4431-d567-eb0787917786"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found Pickle File.\n"
          ]
        }
      ],
      "source": [
        "if GDRIVE:\n",
        "  if not os.path.exists(\"pickle_files/word2vec_model_sklearn.pickle\"):\n",
        "      print(\"Creating Embedding From Scratch.\")\n",
        "      word2vec_model_sklearn = count_model.fit_transform(X.apply(lambda x: np.str_(x)))\n",
        "      pickle_out = open(\"pickle_files/word2vec_model_sklearn.pickle\",'wb')\n",
        "      pickle.dump(word2vec_model_sklearn,pickle_out)\n",
        "      pickle_out.close()\n",
        "  else:\n",
        "      print(\"Found Pickle File.\")\n",
        "      word2vec_model_sklearn = pickle.load(open(\"pickle_files/word2vec_model_sklearn.pickle\",'rb'))\n",
        "else:\n",
        "  if not os.path.exists(\"../pickle_files/word2vec_model_sklearn.pickle\"):\n",
        "      print(\"Creating Embedding From Scratch.\")\n",
        "      word2vec_model_sklearn = count_model.fit_transform(X.apply(lambda x: np.str_(x)))\n",
        "      pickle_out = open(\"../pickle_files/word2vec_model_sklearn.pickle\",'wb')\n",
        "      pickle.dump(word2vec_model_sklearn,pickle_out)\n",
        "      pickle_out.close()\n",
        "  else:\n",
        "      print(\"Found Pickle File.\")\n",
        "      word2vec_model_sklearn = pickle.load(open(\"../pickle_files/word2vec_model_sklearn.pickle\",'rb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "TyImbVlpWhns"
      },
      "outputs": [],
      "source": [
        "x_train,x_test,y_train,y_test = create_train_test_split(word2vec_model_sklearn,y_true_sent,test_size=0.2)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "aHn-92ciWhns"
      },
      "source": [
        "#### Data Resampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "YziFpxmDWhns"
      },
      "outputs": [],
      "source": [
        "if GDRIVE:\n",
        "  if not os.path.exists(\"pickle_files/smote_x_y_train.pickle\"):\n",
        "      random_undersampler = RandomUnderSampler(random_state=RANDOM_STATE)\n",
        "      x_train,y_train = random_undersampler.fit_resample(x_train,y_train)\n",
        "else:\n",
        "  if not os.path.exists(\"../pickle_files/smote_x_y_train.pickle\"):\n",
        "      random_undersampler = RandomUnderSampler(random_state=RANDOM_STATE)\n",
        "      x_train,y_train = random_undersampler.fit_resample(x_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "Wte7WKONWhns"
      },
      "outputs": [],
      "source": [
        "if GDRIVE:\n",
        "  if not os.path.exists(\"pickle_files/smote_x_y_train.pickle\"):\n",
        "      pickle_out = open(\"pickle_files/smote_x_y_train.pickle\",'wb')\n",
        "      pickle.dump({'x_train':x_train,'y_train':y_train},pickle_out)\n",
        "      pickle_out.close()\n",
        "else:\n",
        "  if not os.path.exists(\"../pickle_files/smote_x_y_train.pickle\"):\n",
        "      pickle_out = open(\"../pickle_files/smote_x_y_train.pickle\",'wb')\n",
        "      pickle.dump({'x_train':x_train,'y_train':y_train},pickle_out)\n",
        "      pickle_out.close()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "s8Zf5uQ1Whns"
      },
      "source": [
        "#### Training and Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "WuEq1AZiWhns"
      },
      "outputs": [],
      "source": [
        "# rf_clf = RandomForestClassifier(n_estimators=25,random_state=RANDOM_STATE)\n",
        "lr_clf = LogisticRegression(random_state=RANDOM_STATE)\n",
        "bag_mnnb_clf = BaggingClassifier(base_estimator = MultinomialNB(),n_estimators=25,random_state = RANDOM_STATE)\n",
        "xgb_clf = XGBClassifier(random_state = RANDOM_STATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "1aOWB9rYWhns"
      },
      "outputs": [],
      "source": [
        "y_train_xgb,y_test_xgb = adjust_xgb_labels(y_train,y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "z0QRCmejWhnt"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ],
      "source": [
        "if GDRIVE:\n",
        "  # if not os.path.exists('pickle_files/rf_semibalanced.pickle'):\n",
        "  #   rf_clf.fit(x_train,y_train)\n",
        "  #   y_pred_rf = rf_clf.predict(x_test)\n",
        "  #   pickle_out = open(\"pickle_files/rf_semibalanced.pickle\",'wb')\n",
        "  #   pickle.dump({'rf_clf':rf_clf,'rf_pred':y_pred_rf},pickle_out)\n",
        "  #   pickle_out.close()\n",
        "\n",
        "  if not os.path.exists('pickle_files/lr_semibalanced.pickle'):\n",
        "    lr_clf.fit(x_train,y_train)\n",
        "    y_pred_lr = lr_clf.predict(x_test)\n",
        "    pickle_out = open(\"pickle_files/lr_semibalanced.pickle\",'wb')\n",
        "    pickle.dump({'lr_clf':lr_clf,'lr_pred':y_pred_lr},pickle_out)\n",
        "    pickle_out.close()\n",
        "\n",
        "  if not os.path.exists('pickle_files/bag_mnnb_semibalanced.pickle'):\n",
        "    bag_mnnb_clf.fit(x_train,y_train)\n",
        "    y_pred_mnnb = bag_mnnb_clf.predict(x_test)\n",
        "    pickle_out = open(\"pickle_files/bag_mnnb_semibalanced.pickle\",'wb')\n",
        "    pickle.dump({'bag_mnnb_clf':bag_mnnb_clf,'y_pred_mnnb':y_pred_mnnb},pickle_out)\n",
        "    pickle_out.close()\n",
        "\n",
        "  if not os.path.exists('pickle_files/xgb_semibalanced.pickle'):\n",
        "    xgb_clf.fit(x_train,y_train_xgb)\n",
        "    y_pred_xgb = xgb_clf.predict(x_test)\n",
        "    pickle_out = open(\"pickle_files/xgb_semibalanced.pickle\",'wb')\n",
        "    pickle.dump({'xgb_clf':xgb_clf,'xgb_pred':y_pred_xgb},pickle_out)\n",
        "    pickle_out.close()\n",
        "else:\n",
        "  # if not os.path.exists('../pickle_files/rf_semibalanced.pickle'):\n",
        "  #   rf_clf.fit(x_train,y_train)\n",
        "  #   y_pred_rf = rf_clf.predict(x_test)\n",
        "  #   pickle_out = open(\"../pickle_files/rf_semibalanced.pickle\",'wb')\n",
        "  #   pickle.dump({'rf_clf':rf_clf,'rf_pred':y_pred_rf},pickle_out)\n",
        "  #   pickle_out.close()\n",
        "\n",
        "  if not os.path.exists('../pickle_files/lr_semibalanced.pickle'):\n",
        "    lr_clf.fit(x_train,y_train)\n",
        "    y_pred_lr = lr_clf.predict(x_test)\n",
        "    pickle_out = open(\"../pickle_files/lr_semibalanced.pickle\",'wb')\n",
        "    pickle.dump({'lr_clf':lr_clf,'lr_pred':y_pred_lr},pickle_out)\n",
        "    pickle_out.close()\n",
        "\n",
        "  if not os.path.exists('../pickle_files/bag_mnnb_semibalanced.pickle'):\n",
        "    bag_mnnb_clf.fit(x_train,y_train)\n",
        "    y_pred_mnnb = bag_mnnb_clf.predict(x_test)\n",
        "    pickle_out = open(\"../pickle_files/bag_mnnb_semibalanced.pickle\",'wb')\n",
        "    pickle.dump({'bag_mnnb_clf':bag_mnnb_clf,'y_pred_mnnb':y_pred_mnnb},pickle_out)\n",
        "    pickle_out.close()\n",
        "\n",
        "  if not os.path.exists('../pickle_files/xgb_semibalanced.pickle'):\n",
        "    xgb_clf.fit(x_train,y_train_xgb)\n",
        "    y_pred_xgb = xgb_clf.predict(x_test)\n",
        "    pickle_out = open(\"../pickle_files/xgb_semibalanced.pickle\",'wb')\n",
        "    pickle.dump({'xgb_clf':xgb_clf,'xgb_pred':y_pred_xgb},pickle_out)\n",
        "    pickle_out.close() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "# if os.path.exists(\"../pickle_files/rf.pickle\"):\n",
        "#     print(\"Found RF Pickle File!\")\n",
        "#     rf_info = pickle.load(open('../pickle_files/rf.pickle','rb'))\n",
        "#     rf_clf = rf_info['rf_clf']\n",
        "#     y_pred_rf = rf_info['rf_pred']\n",
        "if os.path.exists(\"../pickle_files/lr.pickle\"):\n",
        "    print(\"Found LR Pickle File!\")\n",
        "    lr_info = pickle.load(open('../pickle_files/lr.pickle','rb'))\n",
        "    lr_clf = lr_info['lr_clf']\n",
        "    y_pred_lr = lr_info['lr_pred']\n",
        "if os.path.exists(\"../pickle_files/bag_mnnb.pickle\"):\n",
        "    print(\"Found Bag_mnnb Pickle File!\")\n",
        "    bag_mnnb_info = pickle.load(open('../pickle_files/bag_mnnb.pickle','rb'))\n",
        "    bag_mnnb_clf = bag_mnnb_info['bag_mnnb_clf']\n",
        "    y_pred_mnnb = bag_mnnb_info['y_pred_mnnb']\n",
        "if os.path.exists(\"../pickle_files/xgb.pickle\"):\n",
        "    print(\"Found xgb Pickle File!\")\n",
        "    xgb_info = pickle.load(open('../pickle_files/xgb.pickle','rb'))\n",
        "    xgb_clf = xgb_info['xgb_clf']\n",
        "    y_pred_xgb = xgb_info['xgb_pred'] "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LR: 0.8706210623894894\n",
            "MNNB: 0.798803481405609\n",
            "XGB: 0.8478279840006409\n"
          ]
        }
      ],
      "source": [
        "# print(f\"RF: {metrics.accuracy_score(y_test,y_pred_rf)}\")\n",
        "print(f\"LR: {metrics.accuracy_score(y_test,y_pred_lr)}\")\n",
        "print(f\"MNNB: {metrics.accuracy_score(y_test,y_pred_mnnb)}\")\n",
        "print(f\"XGB: {metrics.accuracy_score(y_test_xgb,y_pred_xgb)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YrqwLXu7Whnt",
        "outputId": "2557418f-bfa1-47c8-ad5c-8ac69d8020be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PRINTING METRIC(S) FOR lr\n",
            "Classification Report = \n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "           1       0.90      0.97      0.93    936909\n",
            "           2       0.84      0.86      0.85    322760\n",
            "           3       0.53      0.26      0.35    138387\n",
            "\n",
            "    accuracy                           0.87   1398056\n",
            "   macro avg       0.76      0.69      0.71   1398056\n",
            "weighted avg       0.85      0.87      0.86   1398056\n",
            "\n",
            "PRINTING METRIC(S) FOR bag_mnnb\n",
            "Classification Report = \n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "           1       0.93      0.87      0.90    936909\n",
            "           2       0.73      0.73      0.73    322760\n",
            "           3       0.35      0.51      0.41    138387\n",
            "\n",
            "    accuracy                           0.80   1398056\n",
            "   macro avg       0.67      0.70      0.68   1398056\n",
            "weighted avg       0.83      0.80      0.81   1398056\n",
            "\n",
            "PRINTING METRIC(S) FOR xgb\n",
            "Classification Report = \n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.97      0.92    936909\n",
            "           1       0.83      0.78      0.81    322760\n",
            "           2       0.53      0.20      0.29    138387\n",
            "\n",
            "    accuracy                           0.85   1398056\n",
            "   macro avg       0.74      0.65      0.67   1398056\n",
            "weighted avg       0.83      0.85      0.83   1398056\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# show_metrics('rf',rf_clf,x_test,y_test,y_pred_rf,x_train,y_train,5,False,False,True,False)\n",
        "show_metrics('lr',lr_clf,x_test,y_test,y_pred_lr,x_train,y_train,5,False,False,True,False)\n",
        "show_metrics('bag_mnnb',bag_mnnb_clf,x_test,y_test,y_pred_mnnb,x_train,y_train,5,False,False,True,False)\n",
        "show_metrics('xgb',xgb_clf,x_test,y_test_xgb,y_pred_xgb,x_train,y_train_xgb,5,False,False,True,False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
