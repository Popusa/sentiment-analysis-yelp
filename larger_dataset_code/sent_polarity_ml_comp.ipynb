{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\.libs\\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "from nlp_ai_utils import *\n",
    "from chunks_urls import CHUNKS_URLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_ENABLE_ONEDNN_OPTS = 0\n",
    "URLS = CHUNKS_URLS\n",
    "LIMIT = 60\n",
    "LARGER_DATASET_PATH = \"../larger_dataset\"\n",
    "PREPROCESSED_CHUNKS_PATH = LARGER_DATASET_PATH + \"/preprocessed_data_chunks/\"\n",
    "BASE_FILE_NAME = \"chunk_\"\n",
    "FILE_FORMAT = \".csv\"\n",
    "ACTUAL_DATA_SHAPE = 6990280\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_1 already exists.\n",
      "chunk_2 already exists.\n",
      "chunk_3 already exists.\n",
      "chunk_4 already exists.\n",
      "chunk_5 already exists.\n",
      "chunk_6 already exists.\n",
      "chunk_7 already exists.\n",
      "chunk_8 already exists.\n",
      "chunk_9 already exists.\n",
      "chunk_10 already exists.\n",
      "chunk_11 already exists.\n",
      "chunk_12 already exists.\n",
      "chunk_13 already exists.\n",
      "chunk_14 already exists.\n",
      "chunk_15 already exists.\n",
      "chunk_16 already exists.\n",
      "chunk_17 already exists.\n",
      "chunk_18 already exists.\n",
      "chunk_19 already exists.\n",
      "chunk_20 already exists.\n",
      "chunk_21 already exists.\n",
      "chunk_22 already exists.\n",
      "chunk_23 already exists.\n",
      "chunk_24 already exists.\n",
      "chunk_25 already exists.\n",
      "chunk_26 already exists.\n",
      "chunk_27 already exists.\n",
      "chunk_28 already exists.\n",
      "chunk_29 already exists.\n",
      "chunk_30 already exists.\n",
      "chunk_31 already exists.\n",
      "chunk_32 already exists.\n",
      "chunk_33 already exists.\n",
      "chunk_34 already exists.\n",
      "chunk_35 already exists.\n",
      "chunk_36 already exists.\n",
      "chunk_37 already exists.\n",
      "chunk_38 already exists.\n",
      "chunk_39 already exists.\n",
      "chunk_40 already exists.\n",
      "chunk_41 already exists.\n",
      "chunk_42 already exists.\n",
      "chunk_43 already exists.\n",
      "chunk_44 already exists.\n",
      "chunk_45 already exists.\n",
      "chunk_46 already exists.\n",
      "chunk_47 already exists.\n",
      "chunk_48 already exists.\n",
      "chunk_49 already exists.\n",
      "chunk_50 already exists.\n",
      "chunk_51 already exists.\n",
      "chunk_52 already exists.\n",
      "chunk_53 already exists.\n",
      "chunk_54 already exists.\n",
      "chunk_55 already exists.\n",
      "chunk_56 already exists.\n",
      "chunk_57 already exists.\n",
      "chunk_58 already exists.\n",
      "chunk_59 already exists.\n",
      "chunk_60 already exists.\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(LARGER_DATASET_PATH):\n",
    "    os.mkdir(LARGER_DATASET_PATH)\n",
    "if not os.path.exists(PREPROCESSED_CHUNKS_PATH):\n",
    "    os.mkdir(PREPROCESSED_CHUNKS_PATH)\n",
    "get_chunks(URLS,LIMIT,1,BASE_FILE_NAME,PREPROCESSED_CHUNKS_PATH,FILE_FORMAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all names of downloaded files\n",
    "all_file_names = get_all_file_names(BASE_FILE_NAME,LIMIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read all chunks into a list\n",
    "list_dfs = read_chunks(all_file_names,PREPROCESSED_CHUNKS_PATH,FILE_FORMAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate all chunks into a singular df\n",
    "df = group_up_chunks(list_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_data = df[['text', 'stars']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_data.reset_index(inplace = True)\n",
    "review_data.drop(['index'],axis = 1,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_data.rename(columns = {'text':'full_review_text','stars':'star_rating'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_data['full_review_text'] = pd.Series([str(text) for text in review_data['full_review_text']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "full_review_text    44\n",
       "star_rating          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_data.isnull().sum()\n",
    "review_data.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_data['full_review_text'].replace('', np.nan, inplace=True)\n",
    "review_data.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6990192"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(review_data['full_review_text'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparisons"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "sent_polarity_info = [sid.polarity_scores(review) for review in review_data['full_review_text']]\n",
    "\n",
    "sent_polarity_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_sentiment = [classify_sentiment(scores) for scores in sent_polarity_info]\n",
    "\n",
    "sent_polarity = [extract_sent_polarity(scores) for scores in sent_polarity_info]\n",
    "\n",
    "\n",
    "review_data['str_sent'] = review_sentiment\n",
    "\n",
    "review_data['sent_polarity'] = sent_polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_labels = translate_labels(y)\n",
    "y_true_sent = encode_sent(sentiment_labels)\n",
    "y_pred_sent = encode_sent(review_data['str_sent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.accuracy_score(y_true_sent,y_pred_sent))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('../pickle_files'):\n",
    "    os.mkdir('../pickle_files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_model = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"../pickle_files/word2vec_model_sklearn.pickle\"):\n",
    "    print(\"Creating Embedding From Scratch.\")\n",
    "    word2vec_model_sklearn = count_model.fit_transform(X.apply(lambda x: np.str_(x)))\n",
    "    pickle_out = open(\"../pickle_files/word2vec_model_sklearn.pickle\",'wb')\n",
    "    pickle.dump(word2vec_model_sklearn,pickle_out)\n",
    "    pickle_out.close()\n",
    "else:\n",
    "    print(\"Found Pickle File.\")\n",
    "    word2vec_model_sklearn = pickle.load(open(\"../pickle_files/word2vec_model_sklearn.pickle\",'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model_sklearn_array = word2vec_model_sklearn.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = create_train_test_split(word2vec_model_sklearn_array,y_true_sent,test_size=0.2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_resampler = SMOTE(random_state=RANDOM_STATE)\n",
    "x_train,y_train = smote_resampler.fit_resample(x_train,y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier(n_estimators=100,random_state=RANDOM_STATE)\n",
    "svm_clf = SVC(kernel='rbf',random_state=RANDOM_STATE)\n",
    "mnnb_clf = MultinomialNB(random_state = RANDOM_STATE)\n",
    "xgb_clf = XGBClassifier(random_state = RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_xgb,y_test_xgb = adjust_xgb_labels(y_train,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf.fit(x_train,y_train)\n",
    "svm_clf.fit(x_train,y_train)\n",
    "mnnb_clf.fit(x_train,y_train)\n",
    "xgb_clf.fit(x_train,y_train_xgb)\n",
    "y_pred_rf = rf_clf.predict(x_test)\n",
    "y_pred_svm = svm_clf.predict(x_test)\n",
    "y_pred_mnnb = mnnb_clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_metrics(rf_clf,x_test,y_test,y_pred_rf,word2vec_model_sklearn_array,y)\n",
    "show_metrics(svm_clf,x_test,y_test,y_pred_rf,word2vec_model_sklearn_array,y)\n",
    "show_metrics(mnnb_clf,x_test,y_test,y_pred_rf,word2vec_model_sklearn_array,y)\n",
    "show_metrics(xgb_clf,x_test,y_test_xgb,y_pred_rf,word2vec_model_sklearn_array,y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
