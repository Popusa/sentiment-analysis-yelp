{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Goal of This File:\n",
    "\n",
    "##### 1. Import Libraries and Helper Functions ==> Data Sourcing and Munging ==> Utility Functions ==> Loading the Data ==> Merging all Files\n",
    "\n",
    "##### 2. Imbalanced Data Solution ==> Glove Embedding\n",
    "\n",
    "##### 3. Bidirectional LSTM\n",
    "\n",
    "##### 4. Conclusion ==> Results ==> Tensorboard"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries, Helper Functions and Load Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas numpy nltk scikit-learn wordcloud seaborn gensim tensorflow imblearn xgboost matplotlib > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-06 04:26:11.978813: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nlp_ai_utils import *\n",
    "from chunks_urls import CHUNKS_URLS\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_ENABLE_ONEDNN_OPTS = 0\n",
    "URLS = CHUNKS_URLS\n",
    "GLOVE_URL = 'https://f005.backblazeb2.com/file/glove-embeddings-dims/glove.6B.100d.txt'\n",
    "READY_DATA_URL = 'https://f005.backblazeb2.com/file/yelp-dataset-ready-for-models/ready_for_models.rar'\n",
    "CLASS_WEIGHTS_URL = 'https://f005.backblazeb2.com/file/yelp-dataset-ready-for-models/class_weights.pickle'\n",
    "LIMIT = 60\n",
    "LARGER_DATASET_PATH = \"../larger_dataset\"\n",
    "PREPROCESSED_CHUNKS_PATH = LARGER_DATASET_PATH + \"/preprocessed_data_chunks/\"\n",
    "BASE_FILE_NAME = \"chunk_\"\n",
    "FILE_FORMAT = \".csv\"\n",
    "ACTUAL_DATA_SHAPE = 6990280\n",
    "RANDOM_STATE = 42\n",
    "USE_READY_DATA = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Data Sourcing and Munging"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1 Loading The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_1 already exists.\n",
      "chunk_2 already exists.\n",
      "chunk_3 already exists.\n",
      "chunk_4 already exists.\n",
      "chunk_5 already exists.\n",
      "chunk_6 already exists.\n",
      "chunk_7 already exists.\n",
      "chunk_8 already exists.\n",
      "chunk_9 already exists.\n",
      "chunk_10 already exists.\n",
      "chunk_11 already exists.\n",
      "chunk_12 already exists.\n",
      "chunk_13 already exists.\n",
      "chunk_14 already exists.\n",
      "chunk_15 already exists.\n",
      "chunk_16 already exists.\n",
      "chunk_17 already exists.\n",
      "chunk_18 already exists.\n",
      "chunk_19 already exists.\n",
      "chunk_20 already exists.\n",
      "chunk_21 already exists.\n",
      "chunk_22 already exists.\n",
      "chunk_23 already exists.\n",
      "chunk_24 already exists.\n",
      "chunk_25 already exists.\n",
      "chunk_26 already exists.\n",
      "chunk_27 already exists.\n",
      "chunk_28 already exists.\n",
      "chunk_29 already exists.\n",
      "chunk_30 already exists.\n",
      "chunk_31 already exists.\n",
      "chunk_32 already exists.\n",
      "chunk_33 already exists.\n",
      "chunk_34 already exists.\n",
      "chunk_35 already exists.\n",
      "chunk_36 already exists.\n",
      "chunk_37 already exists.\n",
      "chunk_38 already exists.\n",
      "chunk_39 already exists.\n",
      "chunk_40 already exists.\n",
      "chunk_41 already exists.\n",
      "chunk_42 already exists.\n",
      "chunk_43 already exists.\n",
      "chunk_44 already exists.\n",
      "chunk_45 already exists.\n",
      "chunk_46 already exists.\n",
      "chunk_47 already exists.\n",
      "chunk_48 already exists.\n",
      "chunk_49 already exists.\n",
      "chunk_50 already exists.\n",
      "chunk_51 already exists.\n",
      "chunk_52 already exists.\n",
      "chunk_53 already exists.\n",
      "chunk_54 already exists.\n",
      "chunk_55 already exists.\n",
      "chunk_56 already exists.\n",
      "chunk_57 already exists.\n",
      "chunk_58 already exists.\n",
      "chunk_59 already exists.\n",
      "chunk_60 already exists.\n"
     ]
    }
   ],
   "source": [
    "if not USE_READY_DATA:\n",
    "    if not os.path.exists(LARGER_DATASET_PATH):\n",
    "        os.mkdir(LARGER_DATASET_PATH)\n",
    "    if not os.path.exists(PREPROCESSED_CHUNKS_PATH):\n",
    "        os.mkdir(PREPROCESSED_CHUNKS_PATH)\n",
    "    get_chunks(URLS,LIMIT,1,BASE_FILE_NAME,PREPROCESSED_CHUNKS_PATH,FILE_FORMAT)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2 Merging all Individual Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all names of downloaded files\n",
    "if not USE_READY_DATA:\n",
    "    all_file_names = get_all_file_names(BASE_FILE_NAME,LIMIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read all chunks into a list\n",
    "if not USE_READY_DATA:\n",
    "    list_dfs = read_chunks(all_file_names,PREPROCESSED_CHUNKS_PATH,FILE_FORMAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate all chunks into a singular df\n",
    "if not USE_READY_DATA:\n",
    "    df = group_up_chunks(list_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#check how much of the data was actually downloaded\n",
    "if not USE_READY_DATA:\n",
    "    percent_loaded = check_no_missing_data(df.shape[0],ACTUAL_DATA_SHAPE)\n",
    "    percent_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USE_READY_DATA:\n",
    "    review_data = df[['text', 'stars']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USE_READY_DATA:\n",
    "    review_data.reset_index(inplace = True)\n",
    "    review_data.drop(['index'],axis = 1,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USE_READY_DATA:\n",
    "    review_data.rename(columns = {'text':'full_review_text','stars':'star_rating'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "full_review_text    44\n",
       "star_rating          0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if not USE_READY_DATA:\n",
    "    review_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_review_text</th>\n",
       "      <th>star_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>decide eat aware go take 2 hour begin end try ...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ve take lot spin class year nothing compare cl...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>family diner buffet eclectic assortment large ...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wow yummy different delicious favorite lamb cu...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cute interior owner give u tour upcoming patio...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6990275</th>\n",
       "      <td>late addition service iccu apple pay iccu debi...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6990276</th>\n",
       "      <td>spot offer great affordable east weekend paddl...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6990277</th>\n",
       "      <td>home depot need get lot demential lumber seem ...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6990278</th>\n",
       "      <td>m feel like ignore caloriecounting indulge fla...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6990279</th>\n",
       "      <td>locate walk district nashville s bit way u mis...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6990236 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          full_review_text  star_rating\n",
       "0        decide eat aware go take 2 hour begin end try ...          3.0\n",
       "1        ve take lot spin class year nothing compare cl...          5.0\n",
       "2        family diner buffet eclectic assortment large ...          3.0\n",
       "3        wow yummy different delicious favorite lamb cu...          5.0\n",
       "4        cute interior owner give u tour upcoming patio...          4.0\n",
       "...                                                    ...          ...\n",
       "6990275  late addition service iccu apple pay iccu debi...          5.0\n",
       "6990276  spot offer great affordable east weekend paddl...          5.0\n",
       "6990277  home depot need get lot demential lumber seem ...          4.0\n",
       "6990278  m feel like ignore caloriecounting indulge fla...          5.0\n",
       "6990279  locate walk district nashville s bit way u mis...          3.0\n",
       "\n",
       "[6990236 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if not USE_READY_DATA:\n",
    "    review_data.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USE_READY_DATA:\n",
    "    review_data['full_review_text'].replace('', np.nan, inplace=True)\n",
    "    review_data.dropna(inplace = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imbalanced Data Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USE_READY_DATA:\n",
    "    X = review_data['full_review_text']\n",
    "    y = review_data['star_rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USE_READY_DATA:\n",
    "    X = pd.Series([str(text) for text in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USE_READY_DATA:\n",
    "    y = [label - 1 for label in y]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Class Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"../pickle_files\"):\n",
    "    os.mkdir(\"../pickle_files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = compute_class_weight('balanced', classes=[0,1,2,3,4], y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"../pickle_files/class_weights.pickle\"):\n",
    "    pickle_out = open(\"../pickle_files/class_weights.pickle\",'wb')\n",
    "    pickle.dump(class_weights,pickle_out)\n",
    "    pickle_out.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 GloVe + Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('../glove_files'):\n",
    "    os.mkdir('../glove_files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading GloVe File...\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('../glove_files/glove.6B.100d.txt'):\n",
    "    print(\"Downloading GloVe File...\")\n",
    "    r = requests.get(GLOVE_URL)\n",
    "    with open(\"../glove_files/glove.6B.100d.txt\", 'wb') as fd:\n",
    "        for data in r.iter_content():\n",
    "            #save file in the current directory of the notebook\n",
    "            fd.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNIQUE_WORDS = set(' '.join(X).split())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(UNIQUE_WORDS)\n",
    "EMBEDDING_DIM = 100\n",
    "MAX_TEXT_LEN = 200\n",
    "TRUNC_TYPE = 'post'\n",
    "PADDING_TYPE = 'post'\n",
    "OOV_TOKEN = \"<OOV>\"\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 15"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ==> here is a quick explaination of how the dataset will be split using a smaller sample example.\n",
    "##### ==> dataset => 100\n",
    "##### ==> train_set => tr_s (example: 80)\n",
    "##### ==> valid_set => vs (example: 10)\n",
    "##### ==> test_set => te_s (example: 10)\n",
    "##### ==> t = tr_s (80) + vs (10)\n",
    "##### ==> train_set = x[:80]\n",
    "##### ==> valid_set = x[80:t]\n",
    "##### ==> test_set = x[t:] why t? because => vs = ts\n",
    "\n",
    "##### use this guideline if you are confused about how the train-validation-test split was done. Also, this is a future guide for me as well in case I forget."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* train_set_size = 6,990,280 * 0.8 = 5,592,224\n",
    "* valid_set_size = 6,990,280 * 0.1 = 699,028\n",
    "* train_plus_valid = 5,592,224 + 699,028 = 6,291,252\n",
    "\n",
    "==> To Confirm: test_size = 6,990,280 - 6,291,252 = 699,028\n",
    "\n",
    "* train_set = [0:5,592,224]\n",
    "* train_labels = [0:5,592,224]\n",
    "* validation_set = [5,592,224:6,291,252] ==> 699,028\n",
    "* validation_labels = [5,592,224:6,291,252] ==> 699,028\n",
    "* test_set = [6,291,252,6,990,280] ==> 699,028\n",
    "* test_labels = [6,291,252,6,990,280] ==> 699,028"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TRAIN_PERCENT = 0.8\n",
    "VALID_TEST_PERCENT = 0.1\n",
    "TRAIN_SIZE = int(ACTUAL_DATA_SHAPE * TRAIN_PERCENT)\n",
    "VALID_TEST_SIZE = int(ACTUAL_DATA_SHAPE * VALID_TEST_PERCENT)\n",
    "TOTAL_TEST_SIZE = TRAIN_SIZE + VALID_TEST_SIZE\n",
    "train_set = X[:TRAIN_SIZE]\n",
    "train_labels = y[:TRAIN_SIZE]\n",
    "validation_set = X[TRAIN_SIZE:TOTAL_TEST_SIZE]\n",
    "validation_labels = y[TRAIN_SIZE:TOTAL_TEST_SIZE]\n",
    "test_set = X[TOTAL_TEST_SIZE:]\n",
    "test_labels = y[TOTAL_TEST_SIZE:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "READY_DATASET_PATH = \"../larger_dataset/ready_for_models/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"../larger_dataset\"):\n",
    "    os.mkdir(\"../larger_dataset\")\n",
    "    os.mkdir(READY_DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_chunks(READY_DATA_URL,0,1,'ready_for_models','../larger_dataset/ready_for_models/','.rar',False)\n",
    "patoolib.extract_archive(READY_DATASET_PATH + \"ready_for_models.rar\", outdir = READY_DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_padded = pickle.load(open(READY_DATASET_PATH + \"train_set_padded.pickle\", \"rb\"))\n",
    "train_labels = pickle.load(open(READY_DATASET_PATH + \"train_labels.pickle\", \"rb\"))\n",
    "valid_set_padded = pickle.load(open(READY_DATASET_PATH + \"valid_set_padded.pickle\", \"rb\"))\n",
    "validation_labels = pickle.load(open(READY_DATASET_PATH + \"validation_labels.pickle\", \"rb\"))\n",
    "test_set_padded = pickle.load(open(READY_DATASET_PATH + \"test_set_padded.pickle\", \"rb\"))\n",
    "test_labels = pickle.load(open(READY_DATASET_PATH + \"test_labels.pickle\", \"rb\"))\n",
    "EMBEDDING_LAYER = pickle.load(open(READY_DATASET_PATH + \"EMBEDDING_LAYER.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: THIS CELL TAKES A WHILE TO RUN.\n",
    "if not os.path.exists(\"../pickles_files/train_set_padded.pickle\"):\n",
    "    tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token=OOV_TOKEN)\n",
    "    tokenizer.fit_on_texts(train_set)\n",
    "    words_to_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: THIS CELL TAKES A WHILE TO RUN.\n",
    "if not os.path.exists(READY_DATASET_PATH + \"train_set_padded.pickle\"):\n",
    "    train_sequences = tokenizer.texts_to_sequences(train_set)\n",
    "    train_set_padded = pad_sequences(train_sequences, maxlen=MAX_TEXT_LEN, padding=PADDING_TYPE, truncating=TRUNC_TYPE)\n",
    "    \n",
    "if not os.path.exists(READY_DATASET_PATH + \"valid_set_padded.pickle\"):\n",
    "    valid_sequences = tokenizer.texts_to_sequences(validation_set)\n",
    "    valid_set_padded = pad_sequences(valid_sequences, maxlen=MAX_TEXT_LEN, padding=PADDING_TYPE, truncating=TRUNC_TYPE)\n",
    "    \n",
    "if not os.path.exists(READY_DATASET_PATH + \"test_set_padded.pickle\"):\n",
    "    test_sequences = tokenizer.texts_to_sequences(test_set)\n",
    "    test_set_padded = pad_sequences(test_sequences, maxlen=MAX_TEXT_LEN, padding=PADDING_TYPE, truncating=TRUNC_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(READY_DATASET_PATH + \"train_set_padded.pickle\"):\n",
    "    train_set_padded = np.array(train_set_padded)\n",
    "    \n",
    "if not os.path.exists(READY_DATASET_PATH + \"train_labels.pickle\"):\n",
    "    train_labels = np.array(train_labels)\n",
    "    \n",
    "if not os.path.exists(READY_DATASET_PATH + \"valid_set_padded.pickle\"):   \n",
    "    valid_set_padded = np.array(valid_set_padded)\n",
    "\n",
    "if not os.path.exists(READY_DATASET_PATH + \"validation_labels.pickle\"):\n",
    "    validation_labels = np.array(validation_labels)\n",
    "\n",
    "if not os.path.exists(READY_DATASET_PATH + \"test_set_padded.pickle\"):\n",
    "    test_set_padded = np.array(test_set_padded)\n",
    "\n",
    "if not os.path.exists(READY_DATASET_PATH + \"test_labels.pickle\"):\n",
    "    test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_vec_map = read_glove_vector('../glove_files/glove.6B.100d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(READY_DATASET_PATH + \"EMBEDDING_LAYER.pickle\"):\n",
    "    vocab_mapping = len(words_to_index)\n",
    "    embed_vector_len = word_to_vec_map['moon'].shape[0]\n",
    "\n",
    "    emb_matrix = np.zeros((vocab_mapping, embed_vector_len))\n",
    "\n",
    "    for word, index in words_to_index.items():\n",
    "        embedding_vector = word_to_vec_map.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            emb_matrix[index, :] = embedding_vector\n",
    "\n",
    "    EMBEDDING_LAYER = Embedding(input_dim=vocab_mapping,\\\n",
    "                                output_dim=embed_vector_len, input_length=MAX_TEXT_LEN, weights = [emb_matrix], trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(READY_DATASET_PATH + \"train_set_padded.pickle\"):\n",
    "    print(\"Found Pickle File!\")\n",
    "    train_set_padded = pickle.load(open(READY_DATASET_PATH + \"train_set_padded.pickle\", \"rb\"))\n",
    "else:\n",
    "    pickle_out = open(READY_DATASET_PATH + \"train_set_padded.pickle\",'wb')\n",
    "    pickle.dump(train_set_padded,pickle_out)\n",
    "    pickle_out.close()\n",
    "    \n",
    "if os.path.exists(READY_DATASET_PATH + \"train_labels.pickle\"):\n",
    "    print(\"Found Pickle File!\")\n",
    "    train_labels = pickle.load(open(READY_DATASET_PATH + \"train_labels.pickle\", \"rb\"))\n",
    "else:\n",
    "    pickle_out = open(READY_DATASET_PATH + \"train_labels.pickle\",'wb')\n",
    "    pickle.dump(train_labels,pickle_out)\n",
    "    pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(READY_DATASET_PATH + \"train_set_padded.pickle\"):\n",
    "    print(\"Found Pickle File!\")\n",
    "    train_set_padded = pickle.load(open(READY_DATASET_PATH + \"train_set_padded.pickle\", \"rb\"))\n",
    "else:\n",
    "    pickle_out = open(READY_DATASET_PATH + \"train_set_padded.pickle\",'wb')\n",
    "    pickle.dump(train_set_padded,pickle_out)\n",
    "    pickle_out.close()\n",
    "    \n",
    "if os.path.exists(READY_DATASET_PATH + \"train_labels.pickle\"):\n",
    "    print(\"Found Pickle File!\")\n",
    "    train_labels = pickle.load(open(READY_DATASET_PATH + \"train_labels.pickle\", \"rb\"))\n",
    "else:\n",
    "    pickle_out = open(READY_DATASET_PATH + \"train_labels.pickle\",'wb')\n",
    "    pickle.dump(train_labels,pickle_out)\n",
    "    pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(READY_DATASET_PATH + \"valid_set_padded.pickle\"):\n",
    "    print(\"Found Pickle File!\")\n",
    "    valid_set_padded = pickle.load(open(READY_DATASET_PATH + \"valid_set_padded.pickle\", \"rb\"))\n",
    "else:\n",
    "    pickle_out = open(READY_DATASET_PATH + \"valid_set_padded.pickle\",'wb')\n",
    "    pickle.dump(valid_set_padded,pickle_out)\n",
    "    pickle_out.close()\n",
    "    \n",
    "if os.path.exists(READY_DATASET_PATH + \"validation_labels.pickle\"):\n",
    "    print(\"Found Pickle File!\")\n",
    "    validation_labels = pickle.load(open(READY_DATASET_PATH + \"validation_labels.pickle\", \"rb\"))\n",
    "else:\n",
    "    pickle_out = open(READY_DATASET_PATH + \"validation_labels.pickle\",'wb')\n",
    "    pickle.dump(validation_labels,pickle_out)\n",
    "    pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(READY_DATASET_PATH + \"test_set_padded.pickle\"):\n",
    "    print(\"Found Pickle File!\")\n",
    "    test_set_padded = pickle.load(open(READY_DATASET_PATH + \"test_set_padded.pickle\", \"rb\"))\n",
    "else:\n",
    "    pickle_out = open(READY_DATASET_PATH + \"test_set_padded.pickle\",'wb')\n",
    "    pickle.dump(test_set_padded,pickle_out)\n",
    "    pickle_out.close()\n",
    "    \n",
    "if os.path.exists(READY_DATASET_PATH + \"test_labels.pickle\"):\n",
    "    print(\"Found Pickle File!\")\n",
    "    test_labels = pickle.load(open(READY_DATASET_PATH + \"test_labels.pickle\", \"rb\"))\n",
    "else:\n",
    "    pickle_out = open(READY_DATASET_PATH + \"test_labels.pickle\",'wb')\n",
    "    pickle.dump(test_labels,pickle_out)\n",
    "    pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(READY_DATASET_PATH + \"EMBEDDING_LAYER\"):\n",
    "    print(\"Found Pickle File!\")\n",
    "    EMBEDDING_LAYER = pickle.load(open(READY_DATASET_PATH + \"EMBEDDING_LAYER.pickle\", \"rb\"))\n",
    "else:\n",
    "    pickle_out = open(READY_DATASET_PATH + \"EMBEDDING_LAYER.pickle\",'wb')\n",
    "    pickle.dump(EMBEDDING_LAYER,pickle_out)\n",
    "    pickle_out.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LSTM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The requirements to use the cuDNN implementation are:\n",
    "\n",
    "* activation == tanh\n",
    "* recurrent_activation == sigmoid\n",
    "* recurrent_dropout == 0\n",
    "* unroll is False\n",
    "* use_bias is True\n",
    "* Inputs, if use masking, are strictly right-padded.\n",
    "* Eager execution is enabled in the outermost context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-06 04:46:03.899144: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-06 04:46:03.904454: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-06 04:46:03.904847: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-06 04:46:03.906397: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-06 04:46:03.906766: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-06 04:46:03.907113: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-06 04:46:04.402147: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-06 04:46:04.402559: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-06 04:46:04.402985: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-06 04:46:04.403317: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22250 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:21:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(RANDOM_STATE)\n",
    "np.random.seed(RANDOM_STATE)\n",
    "lstm_model = tf.keras.models.Sequential([\n",
    "    EMBEDDING_LAYER,\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,dropout = 0.3,return_sequences = True)),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32,dropout = 0.1)),\n",
    "    tf.keras.layers.Dense(512, activation = \"relu\"),\n",
    "    tf.keras.layers.Dense(5, activation = \"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 200, 100)          100208800 \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 200, 128)         84480     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 64)               41216     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               33280     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5)                 2565      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 100,370,341\n",
      "Trainable params: 161,541\n",
      "Non-trainable params: 100,208,800\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"./logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-06 06:23:40.875426: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 4473779200 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  386/87379 [..............................] - ETA: 36:18:26 - loss: 1.1107 - accuracy: 0.5643 - custom_f1_score: 2.6710"
     ]
    }
   ],
   "source": [
    "lstm_model.compile(optimizer=Adam(), loss = SparseCategoricalCrossentropy(), metrics=['accuracy',custom_f1_score])\n",
    "lstm_model.fit(train_set_padded, train_labels,validation_data = (valid_set_padded,validation_labels),\\\n",
    "              batch_size=BATCH_SIZE, epochs=EPOCHS,class_weight=dict(enumerate(class_weights)),\\\n",
    "              callbacks=[tensorboard_callback,EarlyStopping(patience=3),ReduceLROnPlateau(factor=0.1, patience=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"../saved_models\"):\n",
    "    os.mkdir(\"../saved_models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model.save_weights('../saved_models/lstm_model.h5')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Conclusion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/fit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
