{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vE0B4ngKl2UY"
      },
      "source": [
        "# RNN Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXhvB48zl2UZ"
      },
      "source": [
        "#### Goal of This File:\n",
        "\n",
        "##### 1. Import Libraries, Helper Functions, and Constants ==> Data Sourcing and Munging ==> Utility Functions ==> Loading the Data ==> Merging all Individual Files\n",
        "\n",
        "##### 2. Imbalanced Data Solution ==> Class Weights ==> Glove + Hyperparameters\n",
        "\n",
        "##### 3. SimpleRNN\n",
        "\n",
        "##### 4. Conclusion ==> Results ==> Tensorboard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-8AOgFsl2UZ"
      },
      "source": [
        "## 1. Import Libraries, Helper Functions and Load Constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "tGvCkyMYl2UZ",
        "outputId": "8070511b-0250-465a-c7fe-fe2e2abdb423"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
            "\u001b[0m\r\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\r\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\r\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas numpy nltk scikit-learn wordcloud\\\n",
        "seaborn gensim tensorflow imblearn xgboost matplotlib unrar pyunpack more-itertools patool > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "CPy1BOenl2Ua",
        "outputId": "dd565a84-24f5-4ade-f538-66c22062a224"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "unrar is already the newest version (1:5.6.6-2build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 34 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "rar is already the newest version (2:5.5.0-1build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 34 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "VAST = True\n",
        "\n",
        "if VAST:\n",
        "    !sudo apt-get install unrar\n",
        "    !sudo apt-get install rar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zzRYVKMfl2Ua",
        "outputId": "d28428f2-303c-492e-ec26-3ae4fa6d01e3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-05-08 14:17:49.005807: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nlp_ai_utils import *\n",
        "from chunks_urls import CHUNKS_URLS\n",
        "from updating_values import *\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qDaidkyFl2Ua"
      },
      "outputs": [],
      "source": [
        "TF_ENABLE_ONEDNN_OPTS = 0\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "DATA_URLS = CHUNKS_URLS\n",
        "GLOVE_URL = 'https://f005.backblazeb2.com/file/glove-embeddings-dims/glove.6B.100d.txt'\n",
        "READY_DATA_URL = 'https://f005.backblazeb2.com/file/yelp-dataset-ready-for-models/ready_for_models.rar'\n",
        "CLASS_WEIGHTS_URL = 'https://f005.backblazeb2.com/file/yelp-dataset-ready-for-models/class_weights.pickle'\n",
        "UNIQUE_WORDS_URL = 'https://f005.backblazeb2.com/file/yelp-dataset-ready-for-models/unique_words.pickle'\n",
        "LIMIT = DATA_LIMIT\n",
        "TRAINED_MODELS_COUNT = TRAINED_MODELS\n",
        "LARGER_DATASET_PATH = \"../larger_dataset\"\n",
        "PREPROCESSED_CHUNKS_PATH = LARGER_DATASET_PATH + \"/preprocessed_data_chunks/\"\n",
        "BASE_FILE_NAME = \"chunk_\"\n",
        "FILE_FORMAT = \".csv\"\n",
        "DATA_IN_CHUNK = 116505\n",
        "if LIMIT == 60:\n",
        "    ACTUAL_DATA_SHAPE = DATA_IN_CHUNK * LIMIT + 20\n",
        "else:\n",
        "    ACTUAL_DATA_SHAPE = DATA_IN_CHUNK * LIMIT\n",
        "RANDOM_STATE = CONST_RANDOM_STATE\n",
        "READY_DATASET_PATH = \"../larger_dataset/ready_for_models/\"\n",
        "USE_READY_DATA = False\n",
        "USE_MODIFIED_LABELS = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPxZrAnxl2Ub"
      },
      "source": [
        "### 1.1 Data Sourcing and Munging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-CV6kT3l2Ub"
      },
      "source": [
        "#### 1.1.1 Loading The Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "edHNsXNll2Ub",
        "outputId": "446bd9e9-a32f-4b63-c598-876d13846be0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "chunk_1 already exists.\n",
            "chunk_2 already exists.\n",
            "chunk_3 already exists.\n",
            "chunk_4 already exists.\n",
            "chunk_5 already exists.\n",
            "chunk_6 already exists.\n"
          ]
        }
      ],
      "source": [
        "if not USE_READY_DATA:\n",
        "    if not os.path.exists(LARGER_DATASET_PATH):\n",
        "        os.mkdir(LARGER_DATASET_PATH)\n",
        "    if not os.path.exists(PREPROCESSED_CHUNKS_PATH):\n",
        "        os.mkdir(PREPROCESSED_CHUNKS_PATH)\n",
        "    get_chunks(DATA_URLS,LIMIT,1,BASE_FILE_NAME,PREPROCESSED_CHUNKS_PATH,FILE_FORMAT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wESDxHPal2Ub"
      },
      "source": [
        "#### 1.1.2 Merging all Individual Files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8g29Ch7nl2Ub"
      },
      "outputs": [],
      "source": [
        "#get all names of downloaded files\n",
        "if not USE_READY_DATA:\n",
        "    all_file_names = get_all_file_names(BASE_FILE_NAME,LIMIT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TRBnsQAwl2Ub"
      },
      "outputs": [],
      "source": [
        "#read all chunks into a list\n",
        "if not USE_READY_DATA:\n",
        "    list_dfs = read_chunks(all_file_names,PREPROCESSED_CHUNKS_PATH,FILE_FORMAT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nVaEMUkMl2Uc"
      },
      "outputs": [],
      "source": [
        "#concatenate all chunks into a singular df\n",
        "if not USE_READY_DATA:\n",
        "    df = group_up_chunks(list_dfs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0bzsLn28l2Uc"
      },
      "outputs": [],
      "source": [
        "#check how much of the data was actually downloaded\n",
        "if not USE_READY_DATA:\n",
        "    percent_loaded = check_no_missing_data(df.shape[0],ACTUAL_DATA_SHAPE)\n",
        "    percent_loaded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aSga54J-l2Uc"
      },
      "outputs": [],
      "source": [
        "if not USE_READY_DATA:\n",
        "    review_data = df[['text', 'stars']].copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q3UKMWgsl2Uc"
      },
      "outputs": [],
      "source": [
        "if not USE_READY_DATA:\n",
        "    review_data.reset_index(inplace = True)\n",
        "    review_data.drop(['index'],axis = 1,inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nVFXlXLAl2Uc"
      },
      "outputs": [],
      "source": [
        "if not USE_READY_DATA:\n",
        "    review_data.rename(columns = {'text':'full_review_text','stars':'star_rating'}, inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RzKu-v7Tl2Uc"
      },
      "outputs": [],
      "source": [
        "if not USE_READY_DATA:\n",
        "    review_data.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bx4GPYACl2Uc"
      },
      "outputs": [],
      "source": [
        "if not USE_READY_DATA:\n",
        "    review_data.dropna(inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-q0SgJFl2Uc"
      },
      "outputs": [],
      "source": [
        "if not USE_READY_DATA:\n",
        "    review_data['full_review_text'].replace('', np.nan, inplace=True)\n",
        "    review_data.dropna(inplace = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CF7jU0rml2Uc"
      },
      "source": [
        "## 2. Imbalanced Data Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gv20zYscl2Ud"
      },
      "outputs": [],
      "source": [
        "if not USE_READY_DATA:\n",
        "    X = review_data['full_review_text']\n",
        "    y = review_data['star_rating']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frZZjWrol2Ud"
      },
      "outputs": [],
      "source": [
        "if not USE_READY_DATA:\n",
        "    X = pd.Series([str(text) for text in X])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f846g2mfl2Ud"
      },
      "outputs": [],
      "source": [
        "if USE_MODIFIED_LABELS:\n",
        "    translated_labels = translate_labels(y)\n",
        "    y = pd.Series(encode_sent(translated_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57qIfyDFl2Ud"
      },
      "outputs": [],
      "source": [
        "if not USE_READY_DATA:\n",
        "    y = [label - 1 for label in y]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTx2-xWEl2Ud"
      },
      "source": [
        "### 2.1 Class Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PALYzkR8l2Ud"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(\"../pickle_files\"):\n",
        "    os.mkdir(\"../pickle_files\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "vVg2zoccl2Ud",
        "outputId": "fd53caa5-34bb-43ed-9259-77d7b2e28102"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "class_weights already exists.\n"
          ]
        }
      ],
      "source": [
        "if USE_READY_DATA:\n",
        "    get_chunks([CLASS_WEIGHTS_URL],0,1,'class_weights','../pickle_files/','.pickle',False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uTOCH4HMl2Ue"
      },
      "outputs": [],
      "source": [
        "if USE_READY_DATA:\n",
        "    class_weights = pickle.load(open(\"../pickle_files/class_weights.pickle\", \"rb\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-oNVxWCl2Ue"
      },
      "outputs": [],
      "source": [
        "if not USE_READY_DATA:\n",
        "    y_dict = y.value_counts().to_dict()\n",
        "    class_weights = get_class_weights(y_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpk0tkIal2Uf"
      },
      "source": [
        "### 2.2 GloVe + Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PGOyiARjl2Uf"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists('../glove_files'):\n",
        "    os.mkdir('../glove_files')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "PwyRbPENl2Uf",
        "outputId": "bf7a8a1e-91ab-431d-f963-3564e3c0aa47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "glove.6B.100d already exists.\n"
          ]
        }
      ],
      "source": [
        "if not USE_READY_DATA:\n",
        "    get_chunks([GLOVE_URL],0,1,\"glove.6B.100d\",'../glove_files/','.txt',False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p5aIofmyl2Uf"
      },
      "outputs": [],
      "source": [
        "if not USE_READY_DATA:\n",
        "    UNIQUE_WORDS = set(' '.join(X).split())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KfwXO8V-l2Uf",
        "outputId": "8146dd38-e675-4bdd-8e0d-931f34449742"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "unique_words already exists.\n"
          ]
        }
      ],
      "source": [
        "if not USE_READY_DATA:\n",
        "    get_chunks([UNIQUE_WORDS_URL],0,1,'unique_words',\"../pickle_files/\",'.pickle',False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8e_AN2SDl2Uf"
      },
      "outputs": [],
      "source": [
        "if not USE_READY_DATA:\n",
        "    UNIQUE_WORDS = pickle.load(open(\"../pickle_files/class_weights.pickle\", \"rb\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzn3I8Eal2Uf"
      },
      "source": [
        "##### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_vYAJNzl2Uf"
      },
      "outputs": [],
      "source": [
        "VOCAB_SIZE = len(UNIQUE_WORDS)\n",
        "RNN_UNITS = 64\n",
        "DROPOUT_PERCENT = 0.3\n",
        "DENSE_UNITS = 512\n",
        "LABELS_COUNT = len(y.unique())\n",
        "EMBEDDING_DIM = 100\n",
        "MAX_TEXT_LEN = 200\n",
        "TRUNC_TYPE = 'post'\n",
        "PADDING_TYPE = 'post'\n",
        "OOV_TOKEN = \"<OOV>\"\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 15"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dX2VXSaQl2Ug"
      },
      "source": [
        "##### ==> here is a quick explaination of how the dataset will be split using a smaller sample example.\n",
        "##### ==> dataset => 100\n",
        "##### ==> train_set => tr_s (example: 80)\n",
        "##### ==> valid_set => vs (example: 10)\n",
        "##### ==> test_set => te_s (example: 10)\n",
        "##### ==> t = tr_s (80) + vs (10)\n",
        "##### ==> train_set = x[:80]\n",
        "##### ==> valid_set = x[80:t]\n",
        "##### ==> test_set = x[t:] why t? because => vs = ts\n",
        "\n",
        "##### use this guideline if you are confused about how the train-validation-test split was done. Also, this is a future guide for me as well in case I forget."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpcAg3IJl2Ug"
      },
      "source": [
        "* train_set_size = 6,990,280 * 0.8 = 5,592,224\n",
        "* valid_set_size = 6,990,280 * 0.1 = 699,028\n",
        "* train_plus_valid = 5,592,224 + 699,028 = 6,291,252\n",
        "\n",
        "==> To Confirm: test_size = 6,990,280 - 6,291,252 = 699,028\n",
        "\n",
        "* train_set = [0:5,592,224]\n",
        "* train_labels = [0:5,592,224]\n",
        "* validation_set = [5,592,224:6,291,252] ==> 699,028\n",
        "* validation_labels = [5,592,224:6,291,252] ==> 699,028\n",
        "* test_set = [6,291,252,6,990,280] ==> 699,028\n",
        "* test_labels = [6,291,252,6,990,280] ==> 699,028"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "RVPwH6i3l2Ug"
      },
      "outputs": [],
      "source": [
        "if not USE_READY_DATA:\n",
        "    TRAIN_PERCENT = 0.8\n",
        "    VALID_TEST_PERCENT = 0.1\n",
        "    TRAIN_SIZE = int(ACTUAL_DATA_SHAPE * TRAIN_PERCENT)\n",
        "    VALID_TEST_SIZE = int(ACTUAL_DATA_SHAPE * VALID_TEST_PERCENT)\n",
        "    TOTAL_TEST_SIZE = TRAIN_SIZE + VALID_TEST_SIZE\n",
        "    train_set = X[:TRAIN_SIZE]\n",
        "    train_labels = y[:TRAIN_SIZE]\n",
        "    validation_set = X[TRAIN_SIZE:TOTAL_TEST_SIZE]\n",
        "    validation_labels = y[TRAIN_SIZE:TOTAL_TEST_SIZE]\n",
        "    test_set = X[TOTAL_TEST_SIZE:]\n",
        "    test_labels = y[TOTAL_TEST_SIZE:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mhmJiFX7l2Ug"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(\"../larger_dataset\"):\n",
        "    os.mkdir(\"../larger_dataset\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JXNr3rgGl2Ug"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(READY_DATASET_PATH):\n",
        "    os.mkdir(READY_DATASET_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "U6M-v3vNl2Ug"
      },
      "outputs": [],
      "source": [
        "if USE_READY_DATA:\n",
        "    get_chunks([READY_DATA_URL],0,1,'ready_for_models',READY_DATASET_PATH,'.rar',False)\n",
        "    Archive(os.path.join(READY_DATASET_PATH,\"ready_for_models.rar\")).extractall(READY_DATASET_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Dj_JfnTl2Uh"
      },
      "outputs": [],
      "source": [
        "if USE_READY_DATA:\n",
        "    train_set_padded = pickle.load(open(READY_DATASET_PATH + \"train_set_padded.pickle\", \"rb\"))\n",
        "    train_labels = pickle.load(open(READY_DATASET_PATH + \"train_labels.pickle\", \"rb\"))\n",
        "    valid_set_padded = pickle.load(open(READY_DATASET_PATH + \"valid_set_padded.pickle\", \"rb\"))\n",
        "    validation_labels = pickle.load(open(READY_DATASET_PATH + \"validation_labels.pickle\", \"rb\"))\n",
        "    test_set_padded = pickle.load(open(READY_DATASET_PATH + \"test_set_padded.pickle\", \"rb\"))\n",
        "    test_labels = pickle.load(open(READY_DATASET_PATH + \"test_labels.pickle\", \"rb\"))\n",
        "    EMBEDDING_LAYER = pickle.load(open(READY_DATASET_PATH + \"EMBEDDING_LAYER.pickle\", \"rb\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6jCjFMjOl2Uh"
      },
      "outputs": [],
      "source": [
        "# NOTE: THIS CELL TAKES A WHILE TO RUN.\n",
        "if not os.path.exists(READY_DATASET_PATH + \"train_set_padded.pickle\"):\n",
        "    tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token=OOV_TOKEN)\n",
        "    tokenizer.fit_on_texts(train_set)\n",
        "    words_to_index = tokenizer.word_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tNAnCDlFl2Uh"
      },
      "outputs": [],
      "source": [
        "# NOTE: THIS CELL TAKES A WHILE TO RUN.\n",
        "if not os.path.exists(READY_DATASET_PATH + \"train_set_padded.pickle\"):\n",
        "    train_sequences = tokenizer.texts_to_sequences(train_set)\n",
        "    train_set_padded = pad_sequences(train_sequences, maxlen=MAX_TEXT_LEN, padding=PADDING_TYPE, truncating=TRUNC_TYPE)\n",
        "    \n",
        "if not os.path.exists(READY_DATASET_PATH + \"valid_set_padded.pickle\"):\n",
        "    valid_sequences = tokenizer.texts_to_sequences(validation_set)\n",
        "    valid_set_padded = pad_sequences(valid_sequences, maxlen=MAX_TEXT_LEN, padding=PADDING_TYPE, truncating=TRUNC_TYPE)\n",
        "    \n",
        "if not os.path.exists(READY_DATASET_PATH + \"test_set_padded.pickle\"):\n",
        "    test_sequences = tokenizer.texts_to_sequences(test_set)\n",
        "    test_set_padded = pad_sequences(test_sequences, maxlen=MAX_TEXT_LEN, padding=PADDING_TYPE, truncating=TRUNC_TYPE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMsbSaLLl2Uh"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(READY_DATASET_PATH + \"train_set_padded.pickle\"):\n",
        "    train_set_padded = np.array(train_set_padded)\n",
        "    \n",
        "if not os.path.exists(READY_DATASET_PATH + \"train_labels.pickle\"):\n",
        "    train_labels = np.array(train_labels)\n",
        "    \n",
        "if not os.path.exists(READY_DATASET_PATH + \"valid_set_padded.pickle\"):   \n",
        "    valid_set_padded = np.array(valid_set_padded)\n",
        "\n",
        "if not os.path.exists(READY_DATASET_PATH + \"validation_labels.pickle\"):\n",
        "    validation_labels = np.array(validation_labels)\n",
        "\n",
        "if not os.path.exists(READY_DATASET_PATH + \"test_set_padded.pickle\"):\n",
        "    test_set_padded = np.array(test_set_padded)\n",
        "\n",
        "if not os.path.exists(READY_DATASET_PATH + \"test_labels.pickle\"):\n",
        "    test_labels = np.array(test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vzD579Ll2Uh"
      },
      "outputs": [],
      "source": [
        "if not USE_READY_DATA:\n",
        "    word_to_vec_map = read_glove_vector('../glove_files/glove.6B.100d.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HOqgiVf3l2Uh"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(READY_DATASET_PATH + \"EMBEDDING_LAYER.pickle\"):\n",
        "    vocab_mapping = len(words_to_index)\n",
        "    embed_vector_len = word_to_vec_map['moon'].shape[0]\n",
        "\n",
        "    emb_matrix = np.zeros((vocab_mapping, embed_vector_len))\n",
        "\n",
        "    for word, index in words_to_index.items():\n",
        "        embedding_vector = word_to_vec_map.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            emb_matrix[index, :] = embedding_vector\n",
        "\n",
        "    EMBEDDING_LAYER = Embedding(input_dim=vocab_mapping,\\\n",
        "                                output_dim=embed_vector_len, input_length=MAX_TEXT_LEN, weights = [emb_matrix], trainable=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OeH8hh01l2Uh",
        "outputId": "a1d6c02b-57c4-4c46-f18f-a521fbfed87a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found Pickle File!\n",
            "Found Pickle File!\n"
          ]
        }
      ],
      "source": [
        "if os.path.exists(READY_DATASET_PATH + \"train_set_padded.pickle\"):\n",
        "    print(\"Found Pickle File!\")\n",
        "    train_set_padded = pickle.load(open(READY_DATASET_PATH + \"train_set_padded.pickle\", \"rb\"))\n",
        "else:\n",
        "    pickle_out = open(READY_DATASET_PATH + \"train_set_padded.pickle\",'wb')\n",
        "    pickle.dump(train_set_padded,pickle_out)\n",
        "    pickle_out.close()\n",
        "    \n",
        "if os.path.exists(READY_DATASET_PATH + \"train_labels.pickle\"):\n",
        "    print(\"Found Pickle File!\")\n",
        "    train_labels = pickle.load(open(READY_DATASET_PATH + \"train_labels.pickle\", \"rb\"))\n",
        "else:\n",
        "    pickle_out = open(READY_DATASET_PATH + \"train_labels.pickle\",'wb')\n",
        "    pickle.dump(train_labels,pickle_out)\n",
        "    pickle_out.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vh43HeCXl2Uh",
        "outputId": "d56a9f56-308d-42a0-abb7-e936dcad43ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found Pickle File!\n",
            "Found Pickle File!\n"
          ]
        }
      ],
      "source": [
        "if os.path.exists(READY_DATASET_PATH + \"valid_set_padded.pickle\"):\n",
        "    print(\"Found Pickle File!\")\n",
        "    valid_set_padded = pickle.load(open(READY_DATASET_PATH + \"valid_set_padded.pickle\", \"rb\"))\n",
        "else:\n",
        "    pickle_out = open(READY_DATASET_PATH + \"valid_set_padded.pickle\",'wb')\n",
        "    pickle.dump(valid_set_padded,pickle_out)\n",
        "    pickle_out.close()\n",
        "    \n",
        "if os.path.exists(READY_DATASET_PATH + \"validation_labels.pickle\"):\n",
        "    print(\"Found Pickle File!\")\n",
        "    validation_labels = pickle.load(open(READY_DATASET_PATH + \"validation_labels.pickle\", \"rb\"))\n",
        "else:\n",
        "    pickle_out = open(READY_DATASET_PATH + \"validation_labels.pickle\",'wb')\n",
        "    pickle.dump(validation_labels,pickle_out)\n",
        "    pickle_out.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DkubCpukl2Ui",
        "outputId": "fc04a2e5-d831-43d2-bcaa-2517c5e3c1b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found Pickle File!\n",
            "Found Pickle File!\n"
          ]
        }
      ],
      "source": [
        "if os.path.exists(READY_DATASET_PATH + \"test_set_padded.pickle\"):\n",
        "    print(\"Found Pickle File!\")\n",
        "    test_set_padded = pickle.load(open(READY_DATASET_PATH + \"test_set_padded.pickle\", \"rb\"))\n",
        "else:\n",
        "    pickle_out = open(READY_DATASET_PATH + \"test_set_padded.pickle\",'wb')\n",
        "    pickle.dump(test_set_padded,pickle_out)\n",
        "    pickle_out.close()\n",
        "    \n",
        "if os.path.exists(READY_DATASET_PATH + \"test_labels.pickle\"):\n",
        "    print(\"Found Pickle File!\")\n",
        "    test_labels = pickle.load(open(READY_DATASET_PATH + \"test_labels.pickle\", \"rb\"))\n",
        "else:\n",
        "    pickle_out = open(READY_DATASET_PATH + \"test_labels.pickle\",'wb')\n",
        "    pickle.dump(test_labels,pickle_out)\n",
        "    pickle_out.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-oFRFeREl2Ui",
        "outputId": "90dafa93-dc92-4f7c-d502-508bd5e438a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found Pickle File!\n"
          ]
        }
      ],
      "source": [
        "if os.path.exists(READY_DATASET_PATH + \"EMBEDDING_LAYER.pickle\"):\n",
        "    print(\"Found Pickle File!\")\n",
        "    EMBEDDING_LAYER = pickle.load(open(READY_DATASET_PATH + \"EMBEDDING_LAYER.pickle\", \"rb\"))\n",
        "else:\n",
        "    pickle_out = open(READY_DATASET_PATH + \"EMBEDDING_LAYER.pickle\",'wb')\n",
        "    pickle.dump(EMBEDDING_LAYER,pickle_out)\n",
        "    pickle_out.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYtVE04cl2Ui"
      },
      "source": [
        "## 3. RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVfXTWxyl2Ui"
      },
      "source": [
        "The requirements to use the cuDNN implementation are:\n",
        "\n",
        "* activation == tanh\n",
        "* recurrent_activation == sigmoid\n",
        "* recurrent_dropout == 0\n",
        "* unroll is False\n",
        "* use_bias is True\n",
        "* Inputs, if use masking, are strictly right-padded.\n",
        "* Eager execution is enabled in the outermost context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6nRdESOl2Ui",
        "outputId": "ada0e8e8-2cb1-4329-a8df-3f2560999622"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-05-08 14:18:04.026640: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-05-08 14:18:04.033467: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-05-08 14:18:04.033999: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
          ]
        }
      ],
      "source": [
        "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
        "for device in gpu_devices:\n",
        "    tf.config.experimental.set_memory_growth(device, True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "TybFmkPvl2Ui",
        "outputId": "5a394f5b-4c85-4356-931b-c12a1977ea4f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-05-08 14:18:04.042303: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-05-08 14:18:04.042757: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-05-08 14:18:04.043113: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-05-08 14:18:04.476012: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-05-08 14:18:04.476476: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-05-08 14:18:04.476835: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-05-08 14:18:04.477161: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22250 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:21:00.0, compute capability: 8.6\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tmp/ipykernel_1101/3820615806.py:4: The name tf.keras.backend.set_session is deprecated. Please use tf.compat.v1.keras.backend.set_session instead.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-05-08 14:18:04.721389: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-05-08 14:18:04.721831: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-05-08 14:18:04.722180: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-05-08 14:18:04.722576: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-05-08 14:18:04.722924: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-05-08 14:18:04.723237: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22250 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:21:00.0, compute capability: 8.6\n"
          ]
        }
      ],
      "source": [
        "configproto = tf.compat.v1.ConfigProto() \n",
        "configproto.gpu_options.allow_growth = True\n",
        "sess = tf.compat.v1.Session(config=configproto) \n",
        "tf.compat.v1.keras.backend.set_session(sess)\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "tf.random.set_seed(RANDOM_STATE)\n",
        "np.random.seed(RANDOM_STATE)\n",
        "rnn_model = tf.keras.models.Sequential([\n",
        "    EMBEDDING_LAYER,\n",
        "    tf.keras.layers.Conv1D(filters=CONV_FILTERS, kernel_size=CONV_KERNEL_SIZE, padding='same', activation='relu'),\n",
        "    tf.keras.layers.GlobalMaxPooling1D(keepdims = True),\n",
        "    tf.keras.layers.SimpleRNN(RNN_UNITS),\n",
        "    tf.keras.layers.Dense(DENSE_UNITS, activation = \"relu\"),\n",
        "    tf.keras.layers.Dense(LABELS_COUNT, activation = \"softmax\")\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "SB4H9J-ql2Uj",
        "outputId": "63984786-db01-4b16-8086-f10ce4525df7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 200, 100)          21927500  \n",
            "                                                                 \n",
            " simple_rnn (SimpleRNN)      (None, 60)                9660      \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               31232     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 5)                 2565      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 21,970,957\n",
            "Trainable params: 21,970,957\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "rnn_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92DdUn9Jl2Uj"
      },
      "outputs": [],
      "source": [
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"./logs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wy0NqHQAl2Uj"
      },
      "outputs": [],
      "source": [
        "metrics_callback = MetricsCallback(test_data = valid_set_padded, y_true = validation_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "g_wk5zYll2Uj",
        "outputId": "fc6ae068-0eda-44a4-d8d8-9bf89358b2b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-05-08 14:18:06.348324: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:637] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
            "2023-05-08 14:18:06.382852: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x7fdd880143c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2023-05-08 14:18:06.382874: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA GeForce RTX 3090, Compute Capability 8.6\n",
            "2023-05-08 14:18:06.386981: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "2023-05-08 14:18:06.493682: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8600\n",
            "2023-05-08 14:18:06.600161: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2185/2185 [==============================] - 31s 14ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.34      0.03      0.05     13127\n",
            "         1.0       0.11      0.22      0.15      5186\n",
            "         2.0       0.11      0.12      0.12      5858\n",
            "         3.0       0.00      0.00      0.00     11981\n",
            "         4.0       0.53      0.82      0.64     33751\n",
            "\n",
            "    accuracy                           0.43     69903\n",
            "   macro avg       0.22      0.24      0.19     69903\n",
            "weighted avg       0.34      0.43      0.34     69903\n",
            "\n",
            "17476/17476 [==============================] - 1538s 88ms/step - loss: 1.5989 - accuracy: 0.2691 - val_loss: 1.6019 - val_accuracy: 0.4271 - lr: 0.0010\n",
            "Epoch 2/15\n",
            "2185/2185 [==============================] - 31s 14ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.34      0.04      0.08     13127\n",
            "         1.0       0.21      0.00      0.00      5186\n",
            "         2.0       0.23      0.00      0.00      5858\n",
            "         3.0       0.17      0.98      0.29     11981\n",
            "         4.0       0.00      0.00      0.00     33751\n",
            "\n",
            "    accuracy                           0.18     69903\n",
            "   macro avg       0.19      0.20      0.08     69903\n",
            "weighted avg       0.13      0.18      0.06     69903\n",
            "\n",
            "17476/17476 [==============================] - 1514s 87ms/step - loss: 1.5973 - accuracy: 0.2741 - val_loss: 1.6236 - val_accuracy: 0.1756 - lr: 0.0010\n",
            "Epoch 3/15\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2185/2185 [==============================] - 31s 14ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.34      0.04      0.08     13127\n",
            "         1.0       0.16      0.00      0.01      5186\n",
            "         2.0       0.05      0.00      0.00      5858\n",
            "         3.0       0.17      0.98      0.29     11981\n",
            "         4.0       0.00      0.00      0.00     33751\n",
            "\n",
            "    accuracy                           0.18     69903\n",
            "   macro avg       0.14      0.20      0.07     69903\n",
            "weighted avg       0.11      0.18      0.06     69903\n",
            "\n",
            "17476/17476 [==============================] - 1513s 87ms/step - loss: 1.6001 - accuracy: 0.2212 - val_loss: 1.6096 - val_accuracy: 0.1755 - lr: 1.0000e-04\n",
            "Epoch 4/15\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2185/2185 [==============================] - 31s 14ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.34      0.04      0.08     13127\n",
            "         1.0       0.16      0.00      0.01      5186\n",
            "         2.0       0.09      0.00      0.00      5858\n",
            "         3.0       0.17      0.98      0.29     11981\n",
            "         4.0       0.00      0.00      0.00     33751\n",
            "\n",
            "    accuracy                           0.18     69903\n",
            "   macro avg       0.15      0.20      0.07     69903\n",
            "weighted avg       0.11      0.18      0.06     69903\n",
            "\n",
            "17476/17476 [==============================] - 1519s 87ms/step - loss: 1.6001 - accuracy: 0.2220 - val_loss: 1.6123 - val_accuracy: 0.1755 - lr: 1.0000e-05\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fdfe1873c70>"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rnn_model.compile(optimizer=Adam(), loss = SparseCategoricalCrossentropy(), metrics=['accuracy'])\n",
        "rnn_model.fit(train_set_padded, train_labels,validation_data = (valid_set_padded,validation_labels),\\\n",
        "              batch_size=BATCH_SIZE, epochs=EPOCHS,class_weight=class_weights,\\\n",
        "              callbacks=[tensorboard_callback,metrics_callback,EarlyStopping(patience=3),ReduceLROnPlateau(factor=0.1, patience=1)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7zpdK53l2Uj"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(\"../saved_models\"):\n",
        "    os.mkdir(\"../saved_models\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pyG_cJhAl2Uj"
      },
      "outputs": [],
      "source": [
        "TRAINED_MODELS_COUNT += 1\n",
        "rnn_model.save_weights(\"../saved_models/rnn_model_\" + str(TRAINED_MODELS_COUNT) + \".h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6jNahUlol2Uj"
      },
      "outputs": [],
      "source": [
        "pickle_out = open(\"../saved_models/rnn_model_params_\" + str(TRAINED_MODELS_COUNT) + \".pickle\",'wb')\n",
        "pickle.dump({'EMBEDDING_DIM':EMBEDDING_DIM,'MAX_TEXT_LEN':200,'BATCH_SIZE':BATCH_SIZE,'EPOCHS':EPOCHS,'train_set_size':len(train_set),'chunks_used':(len(train_set) + len(validation_set) + len(test_set)) // DATA_IN_CHUNK},pickle_out)\n",
        "pickle_out.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQpMlKE0l2Uj"
      },
      "outputs": [],
      "source": [
        "!tar -czf rnn_logs.tar.gz logs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BW-aBrKIl2Uj"
      },
      "source": [
        "## 4. Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wz1p9RKdl2Uk"
      },
      "source": [
        "### 4.1 Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKyWJ1J4l2Uk"
      },
      "source": [
        "### 4.2 Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AycBRnGml2Uk",
        "outputId": "06fe4131-95bd-4817-e123-2ab1e08a3da3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "      <iframe id=\"tensorboard-frame-55ba4b5abf8524aa\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
              "      </iframe>\n",
              "      <script>\n",
              "        (function() {\n",
              "          const frame = document.getElementById(\"tensorboard-frame-55ba4b5abf8524aa\");\n",
              "          const url = new URL(\"/\", window.location);\n",
              "          const port = 6006;\n",
              "          if (port) {\n",
              "            url.port = port;\n",
              "          }\n",
              "          frame.src = url;\n",
              "        })();\n",
              "      </script>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs/fit"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}