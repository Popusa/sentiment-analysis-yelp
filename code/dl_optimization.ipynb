{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Optimization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Goal of This File:\n",
    "\n",
    "##### 1. Import Libraries and Helper Functions ==> Data Sourcing and Munging ==> Utility Functions ==> Data Preparation and Pre-Processing\n",
    "\n",
    "##### 2. Confirm Findings From ML File ==> Sklearn Embeddings vs Gensim Embeddings ==> Sentiment Polarity Final Score ==> 3 labels vs 5 labels\n",
    "\n",
    "##### 3. XGBoost\n",
    "\n",
    "##### 4. Simple RNN\n",
    "\n",
    "##### 5. LSTM ==> Bidirectional LSTM\n",
    "\n",
    "##### 6. Sequence-to-Vector Transformers\n",
    "\n",
    "##### 7. Conclusion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "from nlp_ai_utils import *\n",
    "from chunks_urls import CHUNKS_URLS\n",
    "URLS = CHUNKS_URLS\n",
    "LIMIT = 60"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Data Sourcing and Munging"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1 Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks(urls):\n",
    "  #downloads all data from their urls\n",
    "  for i,url in enumerate(urls):\n",
    "    file_name = \"chunk_\" + str(i + 1)\n",
    "    #checks if file already exists\n",
    "    if os.path.exists(file_name):\n",
    "      continue\n",
    "    r = requests.get(url)\n",
    "    print(f\"Downloading {file_name}...\")\n",
    "    with open(file_name, 'wb') as fd:\n",
    "      for chunk in r.iter_content():\n",
    "        #save file in the current directory of the notebook\n",
    "        fd.write(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_chunks(files):\n",
    "    #reads chunks csvs and converts them to a dataframe format\n",
    "    final_df = []\n",
    "    for file in files:\n",
    "        df = pd.read_csv(file)\n",
    "        final_df.append(df)\n",
    "    #function returns a list of dfs, or one df if only one input is given\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_up_chunks(dfs):\n",
    "    #adds up all dataframes together\n",
    "    return pd.concat(dfs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Data Preparation and Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
