{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlp_ai_utils import *\n",
    "from chunks_urls import CHUNKS_URLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_ENABLE_ONEDNN_OPTS = 0\n",
    "URLS = CHUNKS_URLS\n",
    "LIMIT = 60\n",
    "LARGER_DATASET_PATH = \"../larger_dataset\"\n",
    "PREPROCESSED_CHUNKS_PATH = LARGER_DATASET_PATH + \"/preprocessed_data_chunks/\"\n",
    "BASE_FILE_NAME = \"chunk_\"\n",
    "FILE_FORMAT = \".csv\"\n",
    "ACTUAL_DATA_SHAPE = 6990280\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks(urls,limit=0,verbose = 1,base_name = \"\",file_path=\"\",file_format='.csv'):\n",
    "    #downloads all data from their urls\n",
    "    for i,url in enumerate(urls):\n",
    "        if limit:\n",
    "            if i == limit:\n",
    "                return\n",
    "        file_name = base_name + str(i + 1)\n",
    "        #checks if file already exists\n",
    "        if os.path.exists(file_path + file_name + file_format):\n",
    "            print(f\"{file_name} already exists.\")\n",
    "            continue\n",
    "        if i % verbose == 0:\n",
    "            print(f\"Downloading {file_name}...\")\n",
    "        r = requests.get(url)\n",
    "        with open(file_path + file_name + file_format, 'wb') as fd:\n",
    "            for chunk in r.iter_content():\n",
    "                #save file in the current directory of the notebook\n",
    "                fd.write(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_file_names(base_name,limit_num):\n",
    "    return [base_name + str(num) for num in range(1,limit_num + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_chunks(files,file_path = \"\",file_format = \".csv\"):\n",
    "    #reads chunks csvs and converts them to a dataframe format\n",
    "    final_df = []\n",
    "    for file in files:\n",
    "        df = pd.read_csv(file_path + file + file_format)\n",
    "        final_df.append(df)\n",
    "    #function returns a list of dfs\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all names of downloaded files\n",
    "all_file_names = get_all_file_names(BASE_FILE_NAME,LIMIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read all chunks into a list\n",
    "list_dfs = read_chunks(all_file_names,PREPROCESSED_CHUNKS_PATH,FILE_FORMAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_up_chunks(dfs):\n",
    "    #adds up all dataframes together\n",
    "    return pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate all chunks into a singular df\n",
    "df = group_up_chunks(list_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_data = df[['text', 'stars']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_data.reset_index(inplace = True)\n",
    "review_data.drop(['index'],axis = 1,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_data.rename(columns = {'text':'full_review_text','stars':'star_rating'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_data.isnull().sum()\n",
    "review_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = review_data['full_review_text']\n",
    "y = review_data['star_rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('../pickle_files'):\n",
    "    os.mkdir('../pickle_files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"../pickle_files/word2vec_model_sklearn.pickle\"):\n",
    "    print(\"Creating Embedding From Scratch.\")\n",
    "    count_model = CountVectorizer()\n",
    "    word2vec_model_sklearn = count_model.fit_transform(X.apply(lambda x: np.str_(x)))\n",
    "    pickle_out = open(\"../pickle_files/word2vec_model_sklearn.pickle\",'wb')\n",
    "    pickle.dump(word2vec_model_sklearn,pickle_out)\n",
    "    pickle_out.close()\n",
    "else:\n",
    "    print(\"Found Pickle File.\")\n",
    "    word2vec_model_sklearn = pickle.load(open(\"../pickle_files/word2vec_model_sklearn.pickle\",'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model_gensim = Word2Vec(X, min_count=1, vector_size=100)\n",
    "word2vec_model_gensim.save('../pickle_files/word2vec_model_gensim.model')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
