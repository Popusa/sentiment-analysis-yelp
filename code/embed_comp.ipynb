{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\.libs\\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "from nlp_ai_utils import *\n",
    "from chunks_urls import CHUNKS_URLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_ENABLE_ONEDNN_OPTS = 0\n",
    "URLS = CHUNKS_URLS\n",
    "LIMIT = 60\n",
    "LARGER_DATASET_PATH = \"../larger_dataset\"\n",
    "PREPROCESSED_CHUNKS_PATH = LARGER_DATASET_PATH + \"/preprocessed_data_chunks/\"\n",
    "BASE_FILE_NAME = \"chunk_\"\n",
    "FILE_FORMAT = \".csv\"\n",
    "ACTUAL_DATA_SHAPE = 6990280\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks(urls,limit=0,verbose = 1,base_name = \"\",file_path=\"\",file_format='.csv'):\n",
    "    #downloads all data from their urls\n",
    "    for i,url in enumerate(urls):\n",
    "        if limit:\n",
    "            if i == limit:\n",
    "                return\n",
    "        file_name = base_name + str(i + 1)\n",
    "        #checks if file already exists\n",
    "        if os.path.exists(file_path + file_name + file_format):\n",
    "            print(f\"{file_name} already exists.\")\n",
    "            continue\n",
    "        if i % verbose == 0:\n",
    "            print(f\"Downloading {file_name}...\")\n",
    "        r = requests.get(url)\n",
    "        with open(file_path + file_name + file_format, 'wb') as fd:\n",
    "            for chunk in r.iter_content():\n",
    "                #save file in the current directory of the notebook\n",
    "                fd.write(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_file_names(base_name,limit_num):\n",
    "    return [base_name + str(num) for num in range(1,limit_num + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_chunks(files,file_path = \"\",file_format = \".csv\"):\n",
    "    #reads chunks csvs and converts them to a dataframe format\n",
    "    final_df = []\n",
    "    for file in files:\n",
    "        df = pd.read_csv(file_path + file + file_format)\n",
    "        final_df.append(df)\n",
    "    #function returns a list of dfs\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all names of downloaded files\n",
    "all_file_names = get_all_file_names(BASE_FILE_NAME,LIMIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read all chunks into a list\n",
    "list_dfs = read_chunks(all_file_names,PREPROCESSED_CHUNKS_PATH,FILE_FORMAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_up_chunks(dfs):\n",
    "    #adds up all dataframes together\n",
    "    return pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate all chunks into a singular df\n",
    "df = group_up_chunks(list_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_data = df[['text', 'stars']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_data.reset_index(inplace = True)\n",
    "review_data.drop(['index'],axis = 1,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_data.rename(columns = {'text':'full_review_text','stars':'star_rating'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_review_text</th>\n",
       "      <th>star_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>decide eat aware go take 2 hour begin end try ...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ve take lot spin class year nothing compare cl...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>family diner buffet eclectic assortment large ...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wow yummy different delicious favorite lamb cu...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cute interior owner give u tour upcoming patio...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6990275</th>\n",
       "      <td>late addition service iccu apple pay iccu debi...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6990276</th>\n",
       "      <td>spot offer great affordable east weekend paddl...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6990277</th>\n",
       "      <td>home depot need get lot demential lumber seem ...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6990278</th>\n",
       "      <td>m feel like ignore caloriecounting indulge fla...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6990279</th>\n",
       "      <td>locate walk district nashville s bit way u mis...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6990236 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          full_review_text  star_rating\n",
       "0        decide eat aware go take 2 hour begin end try ...          3.0\n",
       "1        ve take lot spin class year nothing compare cl...          5.0\n",
       "2        family diner buffet eclectic assortment large ...          3.0\n",
       "3        wow yummy different delicious favorite lamb cu...          5.0\n",
       "4        cute interior owner give u tour upcoming patio...          4.0\n",
       "...                                                    ...          ...\n",
       "6990275  late addition service iccu apple pay iccu debi...          5.0\n",
       "6990276  spot offer great affordable east weekend paddl...          5.0\n",
       "6990277  home depot need get lot demential lumber seem ...          4.0\n",
       "6990278  m feel like ignore caloriecounting indulge fla...          5.0\n",
       "6990279  locate walk district nashville s bit way u mis...          3.0\n",
       "\n",
       "[6990236 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_data.isnull().sum()\n",
    "review_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = review_data['full_review_text']\n",
    "y = review_data['star_rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.Series([str(text) for text in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "full_review_text    44\n",
       "star_rating          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_review_text</th>\n",
       "      <th>star_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>decide eat aware go take 2 hour begin end try ...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ve take lot spin class year nothing compare cl...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>family diner buffet eclectic assortment large ...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wow yummy different delicious favorite lamb cu...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cute interior owner give u tour upcoming patio...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6990275</th>\n",
       "      <td>late addition service iccu apple pay iccu debi...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6990276</th>\n",
       "      <td>spot offer great affordable east weekend paddl...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6990277</th>\n",
       "      <td>home depot need get lot demential lumber seem ...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6990278</th>\n",
       "      <td>m feel like ignore caloriecounting indulge fla...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6990279</th>\n",
       "      <td>locate walk district nashville s bit way u mis...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6990236 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          full_review_text  star_rating\n",
       "0        decide eat aware go take 2 hour begin end try ...          3.0\n",
       "1        ve take lot spin class year nothing compare cl...          5.0\n",
       "2        family diner buffet eclectic assortment large ...          3.0\n",
       "3        wow yummy different delicious favorite lamb cu...          5.0\n",
       "4        cute interior owner give u tour upcoming patio...          4.0\n",
       "...                                                    ...          ...\n",
       "6990275  late addition service iccu apple pay iccu debi...          5.0\n",
       "6990276  spot offer great affordable east weekend paddl...          5.0\n",
       "6990277  home depot need get lot demential lumber seem ...          4.0\n",
       "6990278  m feel like ignore caloriecounting indulge fla...          5.0\n",
       "6990279  locate walk district nashville s bit way u mis...          3.0\n",
       "\n",
       "[6990236 rows x 2 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('../pickle_files'):\n",
    "    os.mkdir('../pickle_files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found Pickle File.\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(\"../pickle_files/word2vec_model_sklearn.pickle\"):\n",
    "    print(\"Creating Embedding From Scratch.\")\n",
    "    count_model = CountVectorizer()\n",
    "    word2vec_model_sklearn = count_model.fit_transform(X.apply(lambda x: np.str_(x)))\n",
    "    pickle_out = open(\"../pickle_files/word2vec_model_sklearn.pickle\",'wb')\n",
    "    pickle.dump(word2vec_model_sklearn,pickle_out)\n",
    "    pickle_out.close()\n",
    "else:\n",
    "    print(\"Found Pickle File.\")\n",
    "    word2vec_model_sklearn = pickle.load(open(\"../pickle_files/word2vec_model_sklearn.pickle\",'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Embedding From Scratch.\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(\"../pickle_files/word2vec_model_gensim.model\"):\n",
    "    print(\"Creating Embedding From Scratch.\")\n",
    "    word2vec_model_gensim = Word2Vec(X, min_count=1, vector_size=100)\n",
    "    word2vec_model_gensim.save('../pickle_files/word2vec_model_gensim.model')\n",
    "else:\n",
    "    print(\"Found Embedding File.\")\n",
    "    word2vec_model_gensim = Word2Vec.load(\"../pickle_files/word2vec_model_gensim.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating SVD model From Scratch.\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 5.73 GiB for an array with shape (110, 6990280) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32md:\\new pc stuff\\E-learning\\Year 3\\Graduation Project\\sentiment-analysis-yelp\\code\\embed_comp.ipynb Cell 21\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/new%20pc%20stuff/E-learning/Year%203/Graduation%20Project/sentiment-analysis-yelp/code/embed_comp.ipynb#X24sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mCreating SVD model From Scratch.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/new%20pc%20stuff/E-learning/Year%203/Graduation%20Project/sentiment-analysis-yelp/code/embed_comp.ipynb#X24sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m svd_model \u001b[39m=\u001b[39m TruncatedSVD(n_components\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, random_state\u001b[39m=\u001b[39mRANDOM_STATE)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/new%20pc%20stuff/E-learning/Year%203/Graduation%20Project/sentiment-analysis-yelp/code/embed_comp.ipynb#X24sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m svd_model_sklearn \u001b[39m=\u001b[39m svd_model\u001b[39m.\u001b[39;49mfit_transform(word2vec_model_sklearn)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/new%20pc%20stuff/E-learning/Year%203/Graduation%20Project/sentiment-analysis-yelp/code/embed_comp.ipynb#X24sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m word_vectors_sklearn \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\u001b[39mzip\u001b[39m(count_model\u001b[39m.\u001b[39mget_feature_names(), svd_model\u001b[39m.\u001b[39mcomponents_\u001b[39m.\u001b[39mtranspose()))\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/new%20pc%20stuff/E-learning/Year%203/Graduation%20Project/sentiment-analysis-yelp/code/embed_comp.ipynb#X24sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m pickle_out \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m../pickle_files/svd_model_sklearn.pickle\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:239\u001b[0m, in \u001b[0;36mTruncatedSVD.fit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    231\u001b[0m     n_features \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[0;32m    232\u001b[0m     check_scalar(\n\u001b[0;32m    233\u001b[0m         k,\n\u001b[0;32m    234\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mn_components\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    237\u001b[0m         max_val\u001b[39m=\u001b[39mn_features,\n\u001b[0;32m    238\u001b[0m     )\n\u001b[1;32m--> 239\u001b[0m     U, Sigma, VT \u001b[39m=\u001b[39m randomized_svd(\n\u001b[0;32m    240\u001b[0m         X,\n\u001b[0;32m    241\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_components,\n\u001b[0;32m    242\u001b[0m         n_iter\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_iter,\n\u001b[0;32m    243\u001b[0m         n_oversamples\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_oversamples,\n\u001b[0;32m    244\u001b[0m         power_iteration_normalizer\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpower_iteration_normalizer,\n\u001b[0;32m    245\u001b[0m         random_state\u001b[39m=\u001b[39;49mrandom_state,\n\u001b[0;32m    246\u001b[0m     )\n\u001b[0;32m    247\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    248\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39munknown algorithm \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39malgorithm)\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\extmath.py:415\u001b[0m, in \u001b[0;36mrandomized_svd\u001b[1;34m(M, n_components, n_oversamples, n_iter, power_iteration_normalizer, transpose, flip_sign, random_state)\u001b[0m\n\u001b[0;32m    413\u001b[0m \u001b[39mif\u001b[39;00m flip_sign:\n\u001b[0;32m    414\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m transpose:\n\u001b[1;32m--> 415\u001b[0m         U, Vt \u001b[39m=\u001b[39m svd_flip(U, Vt)\n\u001b[0;32m    416\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    417\u001b[0m         \u001b[39m# In case of transpose u_based_decision=false\u001b[39;00m\n\u001b[0;32m    418\u001b[0m         \u001b[39m# to actually flip based on u and not v.\u001b[39;00m\n\u001b[0;32m    419\u001b[0m         U, Vt \u001b[39m=\u001b[39m svd_flip(U, Vt, u_based_decision\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\extmath.py:742\u001b[0m, in \u001b[0;36msvd_flip\u001b[1;34m(u, v, u_based_decision)\u001b[0m\n\u001b[0;32m    710\u001b[0m \u001b[39m\"\"\"Sign correction to ensure deterministic output from SVD.\u001b[39;00m\n\u001b[0;32m    711\u001b[0m \n\u001b[0;32m    712\u001b[0m \u001b[39mAdjusts the columns of u and the rows of v such that the loadings in the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    738\u001b[0m \n\u001b[0;32m    739\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    740\u001b[0m \u001b[39mif\u001b[39;00m u_based_decision:\n\u001b[0;32m    741\u001b[0m     \u001b[39m# columns of u, rows of v\u001b[39;00m\n\u001b[1;32m--> 742\u001b[0m     max_abs_cols \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49margmax(np\u001b[39m.\u001b[39;49mabs(u), axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[0;32m    743\u001b[0m     signs \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msign(u[max_abs_cols, \u001b[39mrange\u001b[39m(u\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m])])\n\u001b[0;32m    744\u001b[0m     u \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m signs\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36margmax\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\fromnumeric.py:1216\u001b[0m, in \u001b[0;36margmax\u001b[1;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[0;32m   1129\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1130\u001b[0m \u001b[39mReturns the indices of the maximum values along an axis.\u001b[39;00m\n\u001b[0;32m   1131\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1213\u001b[0m \u001b[39m(2, 1, 4)\u001b[39;00m\n\u001b[0;32m   1214\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1215\u001b[0m kwds \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mkeepdims\u001b[39m\u001b[39m'\u001b[39m: keepdims} \u001b[39mif\u001b[39;00m keepdims \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39m_NoValue \u001b[39melse\u001b[39;00m {}\n\u001b[1;32m-> 1216\u001b[0m \u001b[39mreturn\u001b[39;00m _wrapfunc(a, \u001b[39m'\u001b[39m\u001b[39margmax\u001b[39m\u001b[39m'\u001b[39m, axis\u001b[39m=\u001b[39maxis, out\u001b[39m=\u001b[39mout, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\fromnumeric.py:57\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapit(obj, method, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m     56\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 57\u001b[0m     \u001b[39mreturn\u001b[39;00m bound(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m     58\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m     59\u001b[0m     \u001b[39m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[0;32m     60\u001b[0m     \u001b[39m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[39m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[0;32m     65\u001b[0m     \u001b[39m# exception has a traceback chain.\u001b[39;00m\n\u001b[0;32m     66\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapit(obj, method, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 5.73 GiB for an array with shape (110, 6990280) and data type float64"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(\"../pickle_files/svd_model_sklearn.pickle\"):\n",
    "    print(\"Creating SVD model From Scratch.\")\n",
    "    svd_model = TruncatedSVD(n_components=100, random_state=RANDOM_STATE)\n",
    "    svd_model_sklearn = svd_model.fit_transform(word2vec_model_sklearn)\n",
    "    word_vectors_sklearn = dict(zip(count_model.get_feature_names(), svd_model.components_.transpose()))\n",
    "    pickle_out = open(\"../pickle_files/svd_model_sklearn.pickle\",'wb')\n",
    "    pickle.dump(word2vec_model_sklearn,pickle_out)\n",
    "    pickle_out.close()\n",
    "else:\n",
    "    print(\"Found Pickle File.\")\n",
    "    word2vec_model_sklearn = pickle.load(open(\"../pickle_files/svd_model_sklearn.pickle\",'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word1 = 'sentiment'\n",
    "word2 = 'sarcasm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine similarity using scikit-learn model\n",
    "vec1_sklearn = word_vectors_sklearn[word1]\n",
    "vec2_sklearn = word_vectors_sklearn[word2]\n",
    "similarity_sklearn = np.dot(vec1_sklearn, vec2_sklearn) / (np.linalg.norm(vec1_sklearn) * np.linalg.norm(vec2_sklearn))\n",
    "print(f\"Similarity between '{word1}' and '{word2}' using scikit-learn model: {similarity_sklearn:.4f}\")\n",
    "\n",
    "# Cosine similarity using gensim model\n",
    "similarity_gensim = word2vec_model_gensim.wv.similarity(word1, word2)\n",
    "print(f\"Similarity between '{word1}' and '{word2}' using gensim model: {similarity_gensim:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model_sklearn_array = word2vec_model_sklearn.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_sklearn,x_test_sklearn,y_train_sklearn,y_test_sklearn = create_train_test_split(word2vec_model_sklearn_array,y,test_size=0.2)\n",
    "x_train_gensim,x_test_gensim,y_train_gensim,y_test_gensim = create_train_test_split(word2vec_model_gensim,y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_resampler = SMOTE(random_state=RANDOM_STATE)\n",
    "x_train_sklearn,y_train_sklearn = smote_resampler.fit_resample(x_train_sklearn,y_train_sklearn)\n",
    "x_train_gensim,y_train_gensim = smote_resampler.fit_resample(x_train_gensim,y_train_gensim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf_sklearn = RandomForestClassifier(n_estimators=100,random_state=RANDOM_STATE)\n",
    "rf_clf_gensim = RandomForestClassifier(n_estimators=100,random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf_sklearn.fit(x_train_sklearn,y_train_sklearn)\n",
    "rf_clf_gensim.fit(x_train_gensim,y_train_gensim)\n",
    "\n",
    "y_pred_sklearn = rf_clf_sklearn.predict(x_test_sklearn)\n",
    "y_pred_gensim = rf_clf_gensim.predict(x_test_gensim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\" Sklearn Embedding with RF Accuracy Score: {metrics.accuracy_score(y_test_sklearn,y_pred_sklearn)}\")\n",
    "print(f\" Gensim Embedding with RF Accuracy Score: {metrics.accuracy_score(y_test_gensim,y_pred_gensim)}\")\n",
    "print(f\" Sklearn Embedding with RF Accuracy Score: {metrics.f1_score(y_test_sklearn,y_pred_sklearn)}\")\n",
    "print(f\" Gensim Embedding with RF Accuracy Score: {metrics.f1_score(y_test_gensim,y_pred_gensim)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_learning_curve(rf_clf_sklearn,word2vec_model_sklearn_array,y,5)\n",
    "print('\\n')\n",
    "show_learning_curve(rf_clf_gensim,word2vec_model_gensim,y,5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
