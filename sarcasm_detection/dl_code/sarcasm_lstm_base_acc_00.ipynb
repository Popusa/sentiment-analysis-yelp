{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "VARIANT_NUM = 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Model for Sarcasm Detection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Goal of This File:\n",
    "\n",
    "##### 1. Import Libraries, Helper Functions, and Constants ==> Data Sourcing and Munging ==> Utility Functions ==> Loading the Data ==> Merging all Individual Files\n",
    "\n",
    "##### 2. Data Processing\n",
    "\n",
    "##### 3. Bidirectional LSTM\n",
    "\n",
    "##### 4. Conclusion ==> Results ==> Tensorboard"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries, Helper Functions and Load Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas numpy nltk scikit-learn wordcloud\\\n",
    "seaborn gensim tensorflow imblearn xgboost matplotlib unrar pyunpack more-itertools patool keras-tqdm > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "unrar is already the newest version (1:5.6.6-2build1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 46 not upgraded.\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "rar is already the newest version (2:5.5.0-1build1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 46 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "VAST = True\n",
    "\n",
    "if VAST:\n",
    "    !sudo apt-get install unrar\n",
    "    !sudo apt-get install rar\n",
    "\n",
    "GDRIVE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "UTILS_URL = 'https://f005.backblazeb2.com/file/gp-support-files/archived_nlp_ai_utils.py'\n",
    "UPDATING_VALUES_URL = 'https://f005.backblazeb2.com/file/gp-support-files/archived_updating_values.py'\n",
    "ALL_LIBS_URL = 'https://f005.backblazeb2.com/file/gp-support-files/archived_all_libs_dl.py'\n",
    "CHUNKS_URLS_FILE_URL = 'https://f005.backblazeb2.com/file/gp-support-files/chunks_urls.py'\n",
    "\n",
    "UTILS_FILE_NAME = 'archived_nlp_ai_utils'\n",
    "UPDATING_VALUES_FILE_NAME = 'archived_updating_values'\n",
    "ALL_LIBS_FILE_NAME = 'archived_all_libs_dl'\n",
    "CHUNKS_URLS_FILE_NAME = 'sd_chunks_urls'\n",
    "\n",
    "DEP_FILE_EXT = '.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "def get_dependencies(url,file_name,file_extension):\n",
    "    if os.path.exists(file_name + file_extension):\n",
    "        return print(file_name + \" already exists.\")\n",
    "    else:\n",
    "        print(f\"downloading {file_name}...\")\n",
    "        r = requests.get(url)\n",
    "        with open(file_name + file_extension, 'wb') as fd:\n",
    "            for chunk in r.iter_content():\n",
    "                fd.write(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "archived_nlp_ai_utils already exists.\n",
      "archived_updating_values already exists.\n",
      "archived_all_libs_dl already exists.\n",
      "sd_chunks_urls already exists.\n"
     ]
    }
   ],
   "source": [
    "get_dependencies(UTILS_URL,UTILS_FILE_NAME,DEP_FILE_EXT)\n",
    "get_dependencies(UPDATING_VALUES_URL,UPDATING_VALUES_FILE_NAME,DEP_FILE_EXT)\n",
    "get_dependencies(ALL_LIBS_URL,ALL_LIBS_FILE_NAME,DEP_FILE_EXT)\n",
    "get_dependencies(CHUNKS_URLS_FILE_URL,CHUNKS_URLS_FILE_NAME,DEP_FILE_EXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "2023-05-28 05:46:28.414690: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-05-28 05:46:28.441760: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from archived_nlp_ai_utils import *\n",
    "from sd_chunks_urls import SD_CHUNKS_URLS\n",
    "from archived_updating_values import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_ENABLE_ONEDNN_OPTS = 0\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "DATA_URLS = SD_CHUNKS_URLS\n",
    "GLOVE_URL = 'https://f005.backblazeb2.com/file/glove-embeddings-dims/glove.6B.100d.txt'\n",
    "LIMIT = 10\n",
    "TRAINED_MODELS_COUNT = TRAINED_MODELS\n",
    "\n",
    "if GDRIVE:\n",
    "    PICKLES_DIR = 'sd_pickle_files'\n",
    "    DATA_PATH = 'datasets'\n",
    "    GLOVE_FILES_DIR = 'glove_files'\n",
    "    SAVED_MODELS_DIR = 'sd_saved_models'\n",
    "else:\n",
    "    PICKLES_DIR = '../sd_pickle_files'\n",
    "    DATA_PATH = '../datasets'\n",
    "    GLOVE_FILES_DIR = '../glove_files'\n",
    "    SAVED_MODELS_DIR = '../sd_saved_models'\n",
    "\n",
    "PREPROCESSED_CHUNKS_PATH = DATA_PATH + \"/preprocessed_sd_data_chunks/\"\n",
    "PROCESSED_DATA_DIR = DATA_PATH + \"/processed_data/\"\n",
    "BASE_FILE_NAME = \"sd_chunk_\"\n",
    "FILE_FORMAT = \".csv\"\n",
    "DATA_IN_CHUNK = 99744\n",
    "if LIMIT == 10:\n",
    "    ACTUAL_DATA_SHAPE = DATA_IN_CHUNK * LIMIT - 2\n",
    "else:\n",
    "    ACTUAL_DATA_SHAPE = DATA_IN_CHUNK * LIMIT\n",
    "RANDOM_STATE = CONST_RANDOM_STATE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Data Sourcing and Munging"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1 Loading The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sd_chunk_1 already exists.\n",
      "sd_chunk_2 already exists.\n",
      "sd_chunk_3 already exists.\n",
      "sd_chunk_4 already exists.\n",
      "sd_chunk_5 already exists.\n",
      "sd_chunk_6 already exists.\n",
      "sd_chunk_7 already exists.\n",
      "sd_chunk_8 already exists.\n",
      "sd_chunk_9 already exists.\n",
      "sd_chunk_10 already exists.\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(DATA_PATH):\n",
    "    os.mkdir(DATA_PATH)\n",
    "if not os.path.exists(PREPROCESSED_CHUNKS_PATH):\n",
    "    os.mkdir(PREPROCESSED_CHUNKS_PATH)\n",
    "get_chunks(DATA_URLS,LIMIT,1,BASE_FILE_NAME,PREPROCESSED_CHUNKS_PATH,FILE_FORMAT)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2 Merging all Individual Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all names of downloaded files\n",
    "all_file_names = get_all_file_names(BASE_FILE_NAME,LIMIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read all chunks into a list\n",
    "list_dfs = read_chunks(all_file_names,PREPROCESSED_CHUNKS_PATH,FILE_FORMAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate all chunks into a singular df\n",
    "df = group_up_chunks(list_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check how much of the data was actually downloaded\n",
    "percent_loaded = check_no_missing_data(df.shape[0],ACTUAL_DATA_SHAPE)\n",
    "percent_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(inplace = True)\n",
    "df.drop(['index'],axis = 1,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text      9488\n",
       "labels       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'].replace('', np.nan, inplace=True)\n",
    "df.dropna(inplace = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['text']\n",
    "y = df['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.Series([str(text) for text in X])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 GloVe + Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(GLOVE_FILES_DIR):\n",
    "    os.mkdir(GLOVE_FILES_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove.6B.100d already exists.\n"
     ]
    }
   ],
   "source": [
    "get_chunks([GLOVE_URL],0,1,\"glove.6B.100d\",GLOVE_FILES_DIR + '/','.txt',False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Unique Words...\n"
     ]
    }
   ],
   "source": [
    "print('Getting Unique Words...')\n",
    "UNIQUE_WORDS = set(' '.join(X).split())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(UNIQUE_WORDS)\n",
    "LSTM_UNITS = 256\n",
    "ATTENTION_UNITS = 64\n",
    "DENSE_UNITS = 1024\n",
    "LABELS_COUNT = 1\n",
    "EMBEDDING_DIM = 100\n",
    "MAX_TEXT_LEN = 150\n",
    "CONV_FILTERS = 60\n",
    "CONV_KERNEL_SIZE = 3\n",
    "DROPOUT_VAL = 0.2\n",
    "TRUNC_TYPE = 'post'\n",
    "PADDING_TYPE = 'post'\n",
    "OOV_TOKEN = \"<OOV>\"\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 25"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ==> here is a quick explaination of how the dataset will be split using a smaller sample example.\n",
    "##### ==> dataset => 100\n",
    "##### ==> train_set => tr_s (example: 80)\n",
    "##### ==> valid_set => vs (example: 10)\n",
    "##### ==> test_set => te_s (example: 10)\n",
    "##### ==> t = tr_s (80) + vs (10)\n",
    "##### ==> train_set = x[:80]\n",
    "##### ==> valid_set = x[80:t]\n",
    "##### ==> test_set = x[t:] why t? because => vs = ts\n",
    "\n",
    "##### use this guideline if you are confused about how the train-validation-test split was done. Also, this is a future guide for me as well in case I forget."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* train_set_size = 6,990,280 * 0.8 = 5,592,224\n",
    "* valid_set_size = 6,990,280 * 0.1 = 699,028\n",
    "* train_plus_valid = 5,592,224 + 699,028 = 6,291,252\n",
    "\n",
    "==> To Confirm: test_size = 6,990,280 - 6,291,252 = 699,028\n",
    "\n",
    "* train_set = [0:5,592,224]\n",
    "* train_labels = [0:5,592,224]\n",
    "* validation_set = [5,592,224:6,291,252] ==> 699,028\n",
    "* validation_labels = [5,592,224:6,291,252] ==> 699,028\n",
    "* test_set = [6,291,252,6,990,280] ==> 699,028\n",
    "* test_labels = [6,291,252,6,990,280] ==> 699,028"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TRAIN_PERCENT = 0.8\n",
    "VALID_TEST_PERCENT = 0.1\n",
    "TRAIN_SIZE = int(ACTUAL_DATA_SHAPE * TRAIN_PERCENT)\n",
    "VALID_TEST_SIZE = int(ACTUAL_DATA_SHAPE * VALID_TEST_PERCENT)\n",
    "TOTAL_TEST_SIZE = TRAIN_SIZE + VALID_TEST_SIZE\n",
    "train_set = X[:TRAIN_SIZE]\n",
    "train_labels = y[:TRAIN_SIZE]\n",
    "validation_set = X[TRAIN_SIZE:TOTAL_TEST_SIZE]\n",
    "validation_labels = y[TRAIN_SIZE:TOTAL_TEST_SIZE]\n",
    "test_set = X[TOTAL_TEST_SIZE:]\n",
    "test_labels = y[TOTAL_TEST_SIZE:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_set.reset_index(drop=True,inplace=True)\n",
    "test_set.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(PROCESSED_DATA_DIR):\n",
    "    os.mkdir(PROCESSED_DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: THIS CELL TAKES A WHILE TO RUN.\n",
    "if not os.path.exists(PROCESSED_DATA_DIR + \"train_set_padded.pickle\"):\n",
    "    print(\"Tokenizing the Dataset...\")\n",
    "    tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token=OOV_TOKEN)\n",
    "    tokenizer.fit_on_texts(train_set)\n",
    "    words_to_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: THIS CELL TAKES A WHILE TO RUN.\n",
    "if not os.path.exists(PROCESSED_DATA_DIR + \"train_set_padded.pickle\"):\n",
    "    print(\"Padding the Training Set.\")\n",
    "    train_sequences = tokenizer.texts_to_sequences(train_set)\n",
    "    train_set_padded = pad_sequences(train_sequences, maxlen=MAX_TEXT_LEN, padding=PADDING_TYPE, truncating=TRUNC_TYPE)\n",
    "    \n",
    "if not os.path.exists(PROCESSED_DATA_DIR + \"valid_set_padded.pickle\"):\n",
    "    print(\"Padding the Validation Set.\")\n",
    "    valid_sequences = tokenizer.texts_to_sequences(validation_set)\n",
    "    valid_set_padded = pad_sequences(valid_sequences, maxlen=MAX_TEXT_LEN, padding=PADDING_TYPE, truncating=TRUNC_TYPE)\n",
    "    \n",
    "if not os.path.exists(PROCESSED_DATA_DIR + \"test_set_padded.pickle\"):\n",
    "    print(\"Padding the Testing Set.\")\n",
    "    test_sequences = tokenizer.texts_to_sequences(test_set)\n",
    "    test_set_padded = pad_sequences(test_sequences, maxlen=MAX_TEXT_LEN, padding=PADDING_TYPE, truncating=TRUNC_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(PROCESSED_DATA_DIR + \"train_set_padded.pickle\"):\n",
    "    print(\"Re-formatting Train Set Shape.\")\n",
    "    train_set_padded = np.array(train_set_padded)\n",
    "    \n",
    "if not os.path.exists(PROCESSED_DATA_DIR + \"train_labels.pickle\"):\n",
    "    print(\"Re-formatting Train Labels Shape.\")\n",
    "    train_labels = np.array(train_labels)\n",
    "    \n",
    "if not os.path.exists(PROCESSED_DATA_DIR + \"valid_set_padded.pickle\"): \n",
    "    print(\"Re-formatting Validation Set Shape.\")\n",
    "    valid_set_padded = np.array(valid_set_padded)\n",
    "\n",
    "if not os.path.exists(PROCESSED_DATA_DIR + \"validation_labels.pickle\"):\n",
    "    print(\"Re-formatting Validation Labels Shape.\")\n",
    "    validation_labels = np.array(validation_labels)\n",
    "\n",
    "if not os.path.exists(PROCESSED_DATA_DIR + \"test_set_padded.pickle\"):\n",
    "    print(\"Re-formatting Testing Set Shape.\")\n",
    "    test_set_padded = np.array(test_set_padded)\n",
    "\n",
    "if not os.path.exists(PROCESSED_DATA_DIR + \"test_labels.pickle\"):\n",
    "    print(\"Re-formatting Testing Labels Shape.\")\n",
    "    test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_vec_map = read_glove_vector(GLOVE_FILES_DIR + '/glove.6B.100d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(PROCESSED_DATA_DIR + \"EMBEDDING_LAYER.pickle\") :\n",
    "    print(\"Generating Embedding From Scratch.\")\n",
    "    vocab_mapping = len(words_to_index)\n",
    "    embed_vector_len = word_to_vec_map['moon'].shape[0]\n",
    "\n",
    "    emb_matrix = np.zeros((vocab_mapping, embed_vector_len))\n",
    "\n",
    "    for word, index in words_to_index.items():\n",
    "        embedding_vector = word_to_vec_map.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            emb_matrix[index, :] = embedding_vector\n",
    "\n",
    "    EMBEDDING_LAYER = Embedding(input_dim=vocab_mapping,\\\n",
    "                                output_dim=embed_vector_len, input_length=MAX_TEXT_LEN, weights = [emb_matrix], trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Pickle File!\n",
      "Using Pickle File!\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(PROCESSED_DATA_DIR + \"train_set_padded.pickle\") :\n",
    "    print(\"Using Pickle File!\")\n",
    "    train_set_padded = pickle.load(open(PROCESSED_DATA_DIR + \"train_set_padded.pickle\", \"rb\"))\n",
    "else:\n",
    "    pickle_out = open(PROCESSED_DATA_DIR + \"train_set_padded.pickle\",'wb')\n",
    "    pickle.dump(train_set_padded,pickle_out)\n",
    "    pickle_out.close()\n",
    "    \n",
    "if os.path.exists(PROCESSED_DATA_DIR + \"train_labels.pickle\") :\n",
    "    print(\"Using Pickle File!\")\n",
    "    train_labels = pickle.load(open(PROCESSED_DATA_DIR + \"train_labels.pickle\", \"rb\"))\n",
    "else:\n",
    "    pickle_out = open(PROCESSED_DATA_DIR + \"train_labels.pickle\",'wb')\n",
    "    pickle.dump(train_labels,pickle_out)\n",
    "    pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Pickle File!\n",
      "Using Pickle File!\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(PROCESSED_DATA_DIR + \"valid_set_padded.pickle\") :\n",
    "    print(\"Using Pickle File!\")\n",
    "    valid_set_padded = pickle.load(open(PROCESSED_DATA_DIR + \"valid_set_padded.pickle\", \"rb\"))\n",
    "else:\n",
    "    pickle_out = open(PROCESSED_DATA_DIR + \"valid_set_padded.pickle\",'wb')\n",
    "    pickle.dump(valid_set_padded,pickle_out)\n",
    "    pickle_out.close()\n",
    "    \n",
    "if os.path.exists(PROCESSED_DATA_DIR + \"validation_labels.pickle\") :\n",
    "    print(\"Using Pickle File!\")\n",
    "    validation_labels = pickle.load(open(PROCESSED_DATA_DIR + \"validation_labels.pickle\", \"rb\"))\n",
    "else:\n",
    "    pickle_out = open(PROCESSED_DATA_DIR + \"validation_labels.pickle\",'wb')\n",
    "    pickle.dump(validation_labels,pickle_out)\n",
    "    pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Pickle File!\n",
      "Using Pickle File!\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(PROCESSED_DATA_DIR + \"test_set_padded.pickle\") :\n",
    "    print(\"Using Pickle File!\")\n",
    "    test_set_padded = pickle.load(open(PROCESSED_DATA_DIR + \"test_set_padded.pickle\", \"rb\"))\n",
    "else:\n",
    "    pickle_out = open(PROCESSED_DATA_DIR + \"test_set_padded.pickle\",'wb')\n",
    "    pickle.dump(test_set_padded,pickle_out)\n",
    "    pickle_out.close()\n",
    "    \n",
    "if os.path.exists(PROCESSED_DATA_DIR + \"test_labels.pickle\") :\n",
    "    print(\"Using Pickle File!\")\n",
    "    test_labels = pickle.load(open(PROCESSED_DATA_DIR + \"test_labels.pickle\", \"rb\"))\n",
    "else:\n",
    "    pickle_out = open(PROCESSED_DATA_DIR + \"test_labels.pickle\",'wb')\n",
    "    pickle.dump(test_labels,pickle_out)\n",
    "    pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Pickle File!\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(PROCESSED_DATA_DIR + \"EMBEDDING_LAYER.pickle\") :\n",
    "    print(\"Using Pickle File!\")\n",
    "    EMBEDDING_LAYER = pickle.load(open(PROCESSED_DATA_DIR + \"EMBEDDING_LAYER.pickle\", \"rb\"))\n",
    "else:\n",
    "    print(\"Pickling Embedding Layer!\")\n",
    "    pickle_out = open(PROCESSED_DATA_DIR + \"EMBEDDING_LAYER.pickle\",'wb')\n",
    "    pickle.dump(EMBEDDING_LAYER,pickle_out)\n",
    "    pickle_out.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LSTM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The requirements to use the cuDNN implementation are:\n",
    "\n",
    "* activation == tanh\n",
    "* recurrent_activation == sigmoid\n",
    "* recurrent_dropout == 0\n",
    "* unroll is False\n",
    "* use_bias is True\n",
    "* Inputs, if use masking, are strictly right-padded.\n",
    "* Eager execution is enabled in the outermost context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(Attention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
    "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-28 05:46:34.858021: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-28 05:46:34.871959: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-28 05:46:34.872218: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_13645/733994204.py:4: The name tf.keras.backend.set_session is deprecated. Please use tf.compat.v1.keras.backend.set_session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-28 05:46:35.225834: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-28 05:46:35.226001: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-28 05:46:35.226145: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-28 05:46:35.226235: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22077 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_13645/733994204.py:15: The name tf.keras.layers.CuDNNLSTM is deprecated. Please use tf.compat.v1.keras.layers.CuDNNLSTM instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-28 05:46:35.418365: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-28 05:46:35.418515: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-28 05:46:35.418590: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-28 05:46:35.418748: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-28 05:46:35.418828: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-28 05:46:35.418897: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-28 05:46:35.418997: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-28 05:46:35.419069: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-28 05:46:35.419121: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22077 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "('Keyword argument not understood:', 'activation')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m x \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mGlobalMaxPooling1D(keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(x)\n\u001b[1;32m     14\u001b[0m x \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mBatchNormalization(synchronized\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(x)\n\u001b[0;32m---> 15\u001b[0m x \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mBidirectional(\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mv1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCuDNNLSTM\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mLSTM_UNITS\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtanh\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)(x)\n\u001b[1;32m     16\u001b[0m x, attention_weights \u001b[38;5;241m=\u001b[39m Attention(ATTENTION_UNITS)(x, x)\n\u001b[1;32m     17\u001b[0m x \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDropout(DROPOUT_VAL)(x)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/cudnn_lstm.py:97\u001b[0m, in \u001b[0;36mCuDNNLSTM.__init__\u001b[0;34m(self, units, kernel_initializer, recurrent_initializer, bias_initializer, unit_forget_bias, kernel_regularizer, recurrent_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, recurrent_constraint, bias_constraint, return_sequences, return_state, go_backwards, stateful, **kwargs)\u001b[0m\n\u001b[1;32m     95\u001b[0m cell_spec \u001b[38;5;241m=\u001b[39m collections\u001b[38;5;241m.\u001b[39mnamedtuple(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcell\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate_size\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cell \u001b[38;5;241m=\u001b[39m cell_spec(state_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munits, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munits))\n\u001b[0;32m---> 97\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_sequences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgo_backwards\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgo_backwards\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstateful\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstateful\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_initializer \u001b[38;5;241m=\u001b[39m initializers\u001b[38;5;241m.\u001b[39mget(kernel_initializer)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurrent_initializer \u001b[38;5;241m=\u001b[39m initializers\u001b[38;5;241m.\u001b[39mget(recurrent_initializer)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/base_cudnn_rnn.py:55\u001b[0m, in \u001b[0;36m_CuDNNRNN.__init__\u001b[0;34m(self, return_sequences, return_state, go_backwards, stateful, time_major, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     46\u001b[0m     return_sequences\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;66;03m# We invoke the base layer's initializer directly here because we do not\u001b[39;00m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;66;03m# want to create RNN cell instance.\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mRNN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_sequences \u001b[38;5;241m=\u001b[39m return_sequences\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_state \u001b[38;5;241m=\u001b[39m return_state\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/trackable/base.py:205\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m previous_value  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py:341\u001b[0m, in \u001b[0;36mLayer.__init__\u001b[0;34m(self, trainable, name, dtype, dynamic, **kwargs)\u001b[0m\n\u001b[1;32m    330\u001b[0m allowed_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_dim\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_shape\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimplementation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    339\u001b[0m }\n\u001b[1;32m    340\u001b[0m \u001b[38;5;66;03m# Validate optional keyword arguments.\u001b[39;00m\n\u001b[0;32m--> 341\u001b[0m \u001b[43mgeneric_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallowed_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;66;03m# Mutable properties\u001b[39;00m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# Indicates whether the layer's weights are updated during training\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# and whether the layer's updates are run during training.\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\n\u001b[1;32m    347\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(trainable, \u001b[38;5;28mbool\u001b[39m)\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    351\u001b[0m     )\n\u001b[1;32m    352\u001b[0m ):\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/generic_utils.py:514\u001b[0m, in \u001b[0;36mvalidate_kwargs\u001b[0;34m(kwargs, allowed_kwargs, error_message)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m kwarg \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m kwarg \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m allowed_kwargs:\n\u001b[0;32m--> 514\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(error_message, kwarg)\n",
      "\u001b[0;31mTypeError\u001b[0m: ('Keyword argument not understood:', 'activation')"
     ]
    }
   ],
   "source": [
    "configproto = tf.compat.v1.ConfigProto() \n",
    "configproto.gpu_options.allow_growth = True\n",
    "sess = tf.compat.v1.Session(config=configproto) \n",
    "tf.compat.v1.keras.backend.set_session(sess)\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(RANDOM_STATE)\n",
    "np.random.seed(RANDOM_STATE)\n",
    "inputs = tf.keras.Input(shape=(MAX_TEXT_LEN,))\n",
    "x = EMBEDDING_LAYER(inputs)\n",
    "x = tf.keras.layers.Normalization()(x)\n",
    "x = tf.keras.layers.Conv1D(filters=CONV_FILTERS, kernel_size=CONV_KERNEL_SIZE, padding='same', activation='tanh')(x)\n",
    "x = tf.keras.layers.GlobalMaxPooling1D(keepdims=True)(x)\n",
    "x = tf.keras.layers.BatchNormalization(synchronized=True)(x)\n",
    "x = tf.keras.layers.Bidirectional(tf.compat.v1.keras.layers.CuDNNLSTM(int(LSTM_UNITS),activation='tanh'))(x)\n",
    "x, attention_weights = Attention(ATTENTION_UNITS)(x, x)\n",
    "x = tf.keras.layers.Dropout(DROPOUT_VAL)(x)\n",
    "x = tf.keras.layers.Dense(DENSE_UNITS, activation='tanh')(x)\n",
    "outputs = tf.keras.layers.Dense(LABELS_COUNT, activation=\"sigmoid\")(x)\n",
    "\n",
    "lstm_model = tf.keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"sd_logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"./logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lstm_model.compile(optimizer=Adam(learning_rate = 0.01,amsgrad=True), loss = BinaryCrossentropy(), metrics=['accuracy'])\n",
    "history = lstm_model.fit(train_set_padded, train_labels,validation_data = (valid_set_padded,validation_labels),\\\n",
    "              batch_size=BATCH_SIZE, epochs=EPOCHS,\\\n",
    "              callbacks=[tensorboard_callback,EarlyStopping(patience=6),ReduceLROnPlateau(factor=0.1, patience=3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(SAVED_MODELS_DIR):\n",
    "    os.mkdir(SAVED_MODELS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINED_MODELS_COUNT = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "if VARIANT_NUM == 0:\n",
    "    lstm_model.save_weights(SAVED_MODELS_DIR + \"/sd_lstm_model_base\" + \".h5\")\n",
    "else:\n",
    "    lstm_model.save_weights(SAVED_MODELS_DIR + \"/sd_lstm_model_variant_\" + str(VARIANT_NUM) + \".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if VARIANT_NUM == 0:\n",
    "    pickle_out = open(SAVED_MODELS_DIR + \"/sd_lstm_model_params_base\" + \".pickle\",'wb')\n",
    "else:\n",
    "    pickle_out = open(SAVED_MODELS_DIR + \"/sd_lstm_model_params_\" + str(VARIANT_NUM) + \".pickle\",'wb')\n",
    "pickle.dump({'EMBEDDING_DIM':EMBEDDING_DIM,'MAX_TEXT_LEN':MAX_TEXT_LEN,'BATCH_SIZE':BATCH_SIZE,'EPOCHS':EPOCHS,\\\n",
    "             'train_set_size':len(train_set),'optimizer':str(lstm_model.optimizer),\\\n",
    "             'learning_rate':str(lstm_model.optimizer.learning_rate),\\\n",
    "             'conv_filters':CONV_FILTERS,'conv_kernel':CONV_KERNEL_SIZE,\\\n",
    "             'chunks_used':(len(train_set) + len(validation_set) + len(test_set)) // DATA_IN_CHUNK,\\\n",
    "            'LSTM_UNITS':LSTM_UNITS,'DENSE_UNITS':DENSE_UNITS,'LABELS_COUNT':LABELS_COUNT},pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(SAVED_MODELS_DIR + \"/\" + 'sd_lstm_model_' + str(VARIANT_NUM) + '.npy',history.history)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if VAST:\n",
    "    !tar -czf sd_lstm_base_logs.tar.gz logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = lstm_model.evaluate(test_set_padded, test_labels, batch_size=BATCH_SIZE)\n",
    "print(\"test loss, test acc:\", results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir logs/fit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
