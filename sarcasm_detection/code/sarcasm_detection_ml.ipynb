{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sarcasm Detection using Machine Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Goal of This File:\n",
    "\n",
    "##### 1. Import Libraries, Helper Functions, and Constants ==> Data Sourcing and Munging ==> Utility Functions ==> Loading the Data ==> Merging all Individual Files\n",
    "\n",
    "##### 2. Word Embeddings \n",
    "\n",
    "##### 3. Machine Learning Models ==> Small Model Test\n",
    "\n",
    "##### 4. Conclusion ==> Results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries, Helper Functions and Load Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The system cannot find the path specified.\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas numpy nltk scikit-learn wordcloud\\\n",
    "seaborn gensim tensorflow imblearn xgboost matplotlib unrar pyunpack more-itertools patool keras-tqdm > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sudo' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'sudo' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "VAST = False\n",
    "\n",
    "if VAST:\n",
    "    !sudo apt-get install unrar\n",
    "    !sudo apt-get install rar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "UTILS_URL = 'https://f005.backblazeb2.com/file/gp-support-files/archived_nlp_ai_utils.py'\n",
    "UPDATING_VALUES_URL = 'https://f005.backblazeb2.com/file/gp-support-files/archived_updating_values.py'\n",
    "ALL_LIBS_URL = 'https://f005.backblazeb2.com/file/gp-support-files/archived_all_libs_dl.py'\n",
    "CHUNKS_URLS_FILE_URL = 'https://f005.backblazeb2.com/file/gp-support-files/chunks_urls.py'\n",
    "\n",
    "UTILS_FILE_NAME = 'archived_nlp_ai_utils'\n",
    "UPDATING_VALUES_FILE_NAME = 'archived_updating_values'\n",
    "ALL_LIBS_FILE_NAME = 'archived_all_libs_dl'\n",
    "CHUNKS_URLS_FILE_NAME = 'chunks_urls'\n",
    "\n",
    "DEP_FILE_EXT = '.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "def get_dependencies(url,file_name,file_extension):\n",
    "    if os.path.exists(file_name + file_extension):\n",
    "        return print(file_name + \" already exists.\")\n",
    "    else:\n",
    "        print(f\"downloading {file_name}...\")\n",
    "        r = requests.get(url)\n",
    "        with open(file_name + file_extension, 'wb') as fd:\n",
    "            for chunk in r.iter_content():\n",
    "                fd.write(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "archived_nlp_ai_utils already exists.\n",
      "archived_updating_values already exists.\n",
      "archived_all_libs_dl already exists.\n",
      "chunks_urls already exists.\n"
     ]
    }
   ],
   "source": [
    "get_dependencies(UTILS_URL,UTILS_FILE_NAME,DEP_FILE_EXT)\n",
    "get_dependencies(UPDATING_VALUES_URL,UPDATING_VALUES_FILE_NAME,DEP_FILE_EXT)\n",
    "get_dependencies(ALL_LIBS_URL,ALL_LIBS_FILE_NAME,DEP_FILE_EXT)\n",
    "get_dependencies(CHUNKS_URLS_FILE_URL,CHUNKS_URLS_FILE_NAME,DEP_FILE_EXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from archived_nlp_ai_utils import *\n",
    "from chunks_urls import SD_CHUNKS_URLS\n",
    "from archived_updating_values import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_ENABLE_ONEDNN_OPTS = 0\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "DATA_URLS = SD_CHUNKS_URLS\n",
    "GLOVE_URL = 'https://f005.backblazeb2.com/file/glove-embeddings-dims/glove.6B.100d.txt'\n",
    "LIMIT = 10\n",
    "TRAINED_MODELS_COUNT = TRAINED_MODELS\n",
    "DATASET_PATH = \"../datasets\"\n",
    "PICKLES_DIR = \"../sd_pickle_files\"\n",
    "PREPROCESSED_CHUNKS_PATH = DATASET_PATH + \"/preprocessed_sd_data_chunks/\"\n",
    "BASE_FILE_NAME = \"sd_chunk_\"\n",
    "FILE_FORMAT = \".csv\"\n",
    "DATA_IN_CHUNK = 99744\n",
    "if LIMIT == 10:\n",
    "    ACTUAL_DATA_SHAPE = DATA_IN_CHUNK * LIMIT - 2\n",
    "else:\n",
    "    ACTUAL_DATA_SHAPE = DATA_IN_CHUNK * LIMIT\n",
    "RANDOM_STATE = CONST_RANDOM_STATE\n",
    "READY_DATASET_PATH = \"../larger_dataset/ready_for_models/\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Data Sourcing and Munging"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1 Loading The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../datasets'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATASET_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../datasets/preprocessed_sd_data_chunks/'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PREPROCESSED_CHUNKS_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sd_chunk_1 already exists.\n",
      "sd_chunk_2 already exists.\n",
      "sd_chunk_3 already exists.\n",
      "sd_chunk_4 already exists.\n",
      "sd_chunk_5 already exists.\n",
      "sd_chunk_6 already exists.\n",
      "sd_chunk_7 already exists.\n",
      "sd_chunk_8 already exists.\n",
      "sd_chunk_9 already exists.\n",
      "sd_chunk_10 already exists.\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(DATASET_PATH):\n",
    "    os.mkdir(DATASET_PATH)\n",
    "if not os.path.exists(PREPROCESSED_CHUNKS_PATH):\n",
    "    os.mkdir(PREPROCESSED_CHUNKS_PATH)\n",
    "get_chunks(DATA_URLS,LIMIT,1,BASE_FILE_NAME,PREPROCESSED_CHUNKS_PATH,FILE_FORMAT)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2 Merging all Individual Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all names of downloaded files\n",
    "all_file_names = get_all_file_names(BASE_FILE_NAME,LIMIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read all chunks into a list\n",
    "list_dfs = read_chunks(all_file_names,PREPROCESSED_CHUNKS_PATH,FILE_FORMAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate all chunks into a singular df\n",
    "df = group_up_chunks(list_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check how much of the data was actually downloaded\n",
    "percent_loaded = check_no_missing_data(df.shape[0],ACTUAL_DATA_SHAPE)\n",
    "percent_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(inplace = True)\n",
    "df.drop(['index'],axis = 1,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['text']\n",
    "y = df['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.Series([str(text) for text in X])\n",
    "\n",
    "y = pd.Series([int(label) for label in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(PICKLES_DIR):\n",
    "    os.mkdir(PICKLES_DIR)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Word Embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_tokens = [nltk.word_tokenize(text) for text in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=reviews_tokens, vector_size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_W2V = np.zeros((len(reviews_tokens), model.vector_size))\n",
    "for i, review in enumerate(reviews_tokens):\n",
    "    for word in review:\n",
    "        if word in model.wv:\n",
    "            X_W2V[i] += model.wv[word]\n",
    "    X_W2V[i] /= len(reviews_tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 TF-IDF"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 FastText"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Machine Learning Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Word2Vec Train-Validation-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PERCENT = 0.8\n",
    "VALID_TEST_PERCENT = 0.1\n",
    "TRAIN_SIZE = int(ACTUAL_DATA_SHAPE * TRAIN_PERCENT)\n",
    "VALID_TEST_SIZE = int(ACTUAL_DATA_SHAPE * VALID_TEST_PERCENT)\n",
    "TOTAL_TEST_SIZE = TRAIN_SIZE + VALID_TEST_SIZE\n",
    "train_set = X_W2V[:TRAIN_SIZE]\n",
    "train_labels = y[:TRAIN_SIZE]\n",
    "validation_set = X_W2V[TRAIN_SIZE:TOTAL_TEST_SIZE]\n",
    "validation_labels = y[TRAIN_SIZE:TOTAL_TEST_SIZE]\n",
    "test_set = X_W2V[TOTAL_TEST_SIZE:]\n",
    "test_labels = y[TOTAL_TEST_SIZE:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 TF-IDF Train-Validation-Test Split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 FastText Train-Validation-Test Split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Naive Bayes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 SVM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf_w2v = RandomForestClassifier(random_state=RANDOM_STATE)\n",
    "rf_clf_w2v.fit(train_set, train_labels)\n",
    "validation_predictions = rf_clf_w2v.predict(validation_set)\n",
    "accuracy = metrics.accuracy_score(validation_labels,validation_predictions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 XGBoost"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Small Model Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_review = \"I totally would love to lend some money to someone I don't know at all!\"\n",
    "test_token = nltk.word_tokenize(test_review)\n",
    "test_vector = np.zeros(model.vector_size)\n",
    "for word in test_token:\n",
    "    if word in model.wv:\n",
    "        test_vector += model.wv[word]\n",
    "test_vector /= len(test_token)\n",
    "predicted_label = clf.predict([test_vector])[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Conclusion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
