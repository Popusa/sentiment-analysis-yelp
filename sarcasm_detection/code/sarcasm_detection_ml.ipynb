{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sarcasm Detection using Machine Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Goal of This File:\n",
    "\n",
    "##### 1. Import Libraries, Helper Functions, and Constants ==> Data Sourcing and Munging ==> Utility Functions ==> Loading the Data ==> Merging all Individual Files\n",
    "\n",
    "##### 2. Word Embeddings \n",
    "\n",
    "##### 3. Machine Learning Models ==> Small Model Test\n",
    "\n",
    "##### 4. Conclusion ==> Results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries, Helper Functions and Load Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The system cannot find the path specified.\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas numpy nltk scikit-learn wordcloud\\\n",
    "seaborn gensim tensorflow imblearn xgboost matplotlib unrar pyunpack more-itertools patool keras-tqdm > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "VAST = False\n",
    "\n",
    "if VAST:\n",
    "    !sudo apt-get install unrar\n",
    "    !sudo apt-get install rar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "UTILS_URL = 'https://f005.backblazeb2.com/file/gp-support-files/archived_nlp_ai_utils.py'\n",
    "UPDATING_VALUES_URL = 'https://f005.backblazeb2.com/file/gp-support-files/archived_updating_values.py'\n",
    "ALL_LIBS_URL = 'https://f005.backblazeb2.com/file/gp-support-files/archived_all_libs_dl.py'\n",
    "CHUNKS_URLS_FILE_URL = 'https://f005.backblazeb2.com/file/gp-support-files/chunks_urls.py'\n",
    "\n",
    "UTILS_FILE_NAME = 'archived_nlp_ai_utils'\n",
    "UPDATING_VALUES_FILE_NAME = 'archived_updating_values'\n",
    "ALL_LIBS_FILE_NAME = 'archived_all_libs_dl'\n",
    "CHUNKS_URLS_FILE_NAME = 'chunks_urls'\n",
    "\n",
    "DEP_FILE_EXT = '.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "def get_dependencies(url,file_name,file_extension):\n",
    "    if os.path.exists(file_name + file_extension):\n",
    "        return print(file_name + \" already exists.\")\n",
    "    else:\n",
    "        print(f\"downloading {file_name}...\")\n",
    "        r = requests.get(url)\n",
    "        with open(file_name + file_extension, 'wb') as fd:\n",
    "            for chunk in r.iter_content():\n",
    "                fd.write(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "archived_nlp_ai_utils already exists.\n",
      "archived_updating_values already exists.\n",
      "archived_all_libs_dl already exists.\n",
      "chunks_urls already exists.\n"
     ]
    }
   ],
   "source": [
    "get_dependencies(UTILS_URL,UTILS_FILE_NAME,DEP_FILE_EXT)\n",
    "get_dependencies(UPDATING_VALUES_URL,UPDATING_VALUES_FILE_NAME,DEP_FILE_EXT)\n",
    "get_dependencies(ALL_LIBS_URL,ALL_LIBS_FILE_NAME,DEP_FILE_EXT)\n",
    "get_dependencies(CHUNKS_URLS_FILE_URL,CHUNKS_URLS_FILE_NAME,DEP_FILE_EXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\.libs\\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from archived_nlp_ai_utils import *\n",
    "from chunks_urls import SD_CHUNKS_URLS\n",
    "from archived_updating_values import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_ENABLE_ONEDNN_OPTS = 0\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "DATA_URLS = SD_CHUNKS_URLS\n",
    "GLOVE_URL = 'https://f005.backblazeb2.com/file/glove-embeddings-dims/glove.6B.100d.txt'\n",
    "LIMIT = 10\n",
    "TRAINED_MODELS_COUNT = TRAINED_MODELS\n",
    "DATASET_PATH = \"../datasets\"\n",
    "PICKLES_DIR = \"../sd_pickle_files\"\n",
    "PREPROCESSED_CHUNKS_PATH = DATASET_PATH + \"/preprocessed_sd_data_chunks/\"\n",
    "BASE_FILE_NAME = \"sd_chunk_\"\n",
    "FILE_FORMAT = \".csv\"\n",
    "DATA_IN_CHUNK = 99744\n",
    "if LIMIT == 10:\n",
    "    ACTUAL_DATA_SHAPE = DATA_IN_CHUNK * LIMIT - 2\n",
    "else:\n",
    "    ACTUAL_DATA_SHAPE = DATA_IN_CHUNK * LIMIT\n",
    "RANDOM_STATE = CONST_RANDOM_STATE\n",
    "READY_DATASET_PATH = \"../larger_dataset/ready_for_models/\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Data Sourcing and Munging"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1 Loading The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../datasets'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATASET_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../datasets/preprocessed_sd_data_chunks/'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PREPROCESSED_CHUNKS_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sd_chunk_1 already exists.\n",
      "sd_chunk_2 already exists.\n",
      "sd_chunk_3 already exists.\n",
      "sd_chunk_4 already exists.\n",
      "sd_chunk_5 already exists.\n",
      "sd_chunk_6 already exists.\n",
      "sd_chunk_7 already exists.\n",
      "sd_chunk_8 already exists.\n",
      "sd_chunk_9 already exists.\n",
      "sd_chunk_10 already exists.\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(DATASET_PATH):\n",
    "    os.mkdir(DATASET_PATH)\n",
    "if not os.path.exists(PREPROCESSED_CHUNKS_PATH):\n",
    "    os.mkdir(PREPROCESSED_CHUNKS_PATH)\n",
    "get_chunks(DATA_URLS,LIMIT,1,BASE_FILE_NAME,PREPROCESSED_CHUNKS_PATH,FILE_FORMAT)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2 Merging all Individual Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all names of downloaded files\n",
    "all_file_names = get_all_file_names(BASE_FILE_NAME,LIMIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read all chunks into a list\n",
    "list_dfs = read_chunks(all_file_names,PREPROCESSED_CHUNKS_PATH,FILE_FORMAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate all chunks into a singular df\n",
    "df = group_up_chunks(list_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check how much of the data was actually downloaded\n",
    "percent_loaded = check_no_missing_data(df.shape[0],ACTUAL_DATA_SHAPE)\n",
    "percent_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(inplace = True)\n",
    "df.drop(['index'],axis = 1,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['text']\n",
    "y = df['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.Series([str(text) for text in X])\n",
    "\n",
    "y = pd.Series([int(label) for label in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(PICKLES_DIR):\n",
    "    os.mkdir(PICKLES_DIR)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Word Embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokens = [nltk.word_tokenize(text) for text in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_tokens = text_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(sentences = w2v_tokens, vector_size = 100, window = 5, min_count=1, workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_W2V = np.zeros((len(text_tokens), w2v_model.vector_size))\n",
    "for i, text in enumerate(w2v_tokens):\n",
    "    for word in text:\n",
    "        if word in w2v_model.wv:\n",
    "            X_W2V[i] += w2v_model.wv[word]\n",
    "    X_W2V[i] /= len(w2v_tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_tokens = text_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(tfidf_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(token) for token in tfidf_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_model = TfidfModel(bow_corpus)\n",
    "\n",
    "tfidf_embedding = tfidf_model[bow_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_21620\\3403743208.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  X_TFIDF = np.array([np.array(token)[:, 1] for token in tfidf_embedding])\n"
     ]
    }
   ],
   "source": [
    "X_TFIDF = np.array([np.array(token)[:, 1] for token in tfidf_embedding])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_tokens = text_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model = FastText(sentences = fasttext_tokens, vector_size = 100, window = 5, min_count = 1, workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_FT = np.zeros((len(text_tokens), ft_model.vector_size))\n",
    "for i, text in enumerate(fasttext_tokens):\n",
    "    for word in text:\n",
    "        if word in ft_model.wv:\n",
    "            X_FT[i] += ft_model.wv[word]\n",
    "    X_FT[i] /= len(text_tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Machine Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_W2V = preprocessing.normalize(X_W2V)\n",
    "X_TFIDF = preprocessing.normalize(X_TFIDF)\n",
    "X_FT = preprocessing.normalize(X_FT)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Word2Vec Train-Validation-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PERCENT = 0.8\n",
    "VALID_TEST_PERCENT = 0.1\n",
    "TRAIN_SIZE = int(ACTUAL_DATA_SHAPE * TRAIN_PERCENT)\n",
    "VALID_TEST_SIZE = int(ACTUAL_DATA_SHAPE * VALID_TEST_PERCENT)\n",
    "TOTAL_TEST_SIZE = TRAIN_SIZE + VALID_TEST_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_w2v = X_W2V[:TRAIN_SIZE]\n",
    "train_labels_w2v = y[:TRAIN_SIZE]\n",
    "validation_set_w2v = X_W2V[TRAIN_SIZE:TOTAL_TEST_SIZE]\n",
    "validation_labels_w2v = y[TRAIN_SIZE:TOTAL_TEST_SIZE]\n",
    "test_set_w2v = X_W2V[TOTAL_TEST_SIZE:]\n",
    "test_labels_w2v = y[TOTAL_TEST_SIZE:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 TF-IDF Train-Validation-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_tfidf = X_TFIDF[:TRAIN_SIZE]\n",
    "train_labels_tfidf = y[:TRAIN_SIZE]\n",
    "validation_set_tfidf = X_TFIDF[TRAIN_SIZE:TOTAL_TEST_SIZE]\n",
    "validation_labels_tfidf = y[TRAIN_SIZE:TOTAL_TEST_SIZE]\n",
    "test_set_tfidf = X_TFIDF[TOTAL_TEST_SIZE:]\n",
    "test_labels_tfidf = y[TOTAL_TEST_SIZE:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 FastText Train-Validation-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_ft = X_FT[:TRAIN_SIZE]\n",
    "train_labels_ft = y[:TRAIN_SIZE]\n",
    "validation_set_ft = X_FT[TRAIN_SIZE:TOTAL_TEST_SIZE]\n",
    "validation_labels_ft = y[TRAIN_SIZE:TOTAL_TEST_SIZE]\n",
    "test_set_ft = X_FT[TOTAL_TEST_SIZE:]\n",
    "test_labels_ft = y[TOTAL_TEST_SIZE:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Naive Bayes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.1 Word2Vec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_clf_w2v = GaussianNB()\n",
    "nb_clf_w2v.fit(train_set_w2v, train_labels_w2v)\n",
    "validation_predictions_nb_w2v = nb_clf_w2v.predict(validation_set_w2v)\n",
    "accuracy = metrics.accuracy_score(validation_labels_w2v,validation_predictions_nb_w2v)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.2 TF-IDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_clf_tfidf = GaussianNB()\n",
    "nb_clf_tfidf.fit(train_set_tfidf, train_labels_tfidf)\n",
    "validation_predictions_nb_tfidf = nb_clf_w2v.predict(validation_set_tfidf)\n",
    "accuracy = metrics.accuracy_score(validation_labels_tfidf,validation_predictions_nb_tfidf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.3 FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_clf_ft = GaussianNB()\n",
    "nb_clf_ft.fit(train_set_ft, train_labels_ft)\n",
    "validation_predictions_nb_ft = nb_clf_w2v.predict(validation_set_ft)\n",
    "accuracy = metrics.accuracy_score(validation_labels_ft,validation_predictions_nb_ft)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 SVM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.1 Word2Vec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clf_w2v = SVC(kernel='rbf',random_state=RANDOM_STATE)\n",
    "svm_clf_w2v.fit(train_set_w2v, train_labels_w2v)\n",
    "validation_predictions_svm_w2v = svm_clf_w2v.predict(validation_set_w2v)\n",
    "accuracy = metrics.accuracy_score(validation_labels_w2v,validation_predictions_svm_w2v)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.2 TF-IDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clf_tfidf = SVC(kernel='rbf',random_state=RANDOM_STATE)\n",
    "svm_clf_tfidf.fit(train_set_tfidf, train_labels_tfidf)\n",
    "validation_predictions_svm_tfidf = svm_clf_tfidf.predict(validation_set_tfidf)\n",
    "accuracy = metrics.accuracy_score(validation_labels_tfidf,validation_predictions_svm_tfidf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.3 FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clf_ft = SVC(kernel='rbf',random_state=RANDOM_STATE)\n",
    "svm_clf_ft.fit(train_set_ft, train_labels_ft)\n",
    "validation_predictions_svm_ft = svm_clf_ft.predict(validation_set_ft)\n",
    "accuracy = metrics.accuracy_score(validation_labels_ft,validation_predictions_svm_ft)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Random Forest"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6.1 Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf_w2v = RandomForestClassifier(n_estimators=50,random_state=RANDOM_STATE)\n",
    "rf_clf_w2v.fit(train_set_w2v, train_labels_w2v)\n",
    "validation_predictions_rf_w2v = rf_clf_w2v.predict(validation_set_w2v)\n",
    "accuracy = metrics.accuracy_score(validation_labels_w2v,validation_predictions_rf_w2v)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6.2 TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf_tfidf = RandomForestClassifier(n_estimators=50,random_state=RANDOM_STATE)\n",
    "rf_clf_tfidf.fit(train_set_tfidf, train_labels_tfidf)\n",
    "validation_predictions_rf_tfidf = rf_clf_tfidf.predict(validation_set_tfidf)\n",
    "accuracy = metrics.accuracy_score(validation_labels_tfidf,validation_predictions_rf_tfidf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6.3 FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf_ft = RandomForestClassifier(n_estimators=50,random_state=RANDOM_STATE)\n",
    "rf_clf_ft.fit(train_set_ft, train_labels_ft)\n",
    "validation_predictions_rf_ft = rf_clf_ft.predict(validation_set_ft)\n",
    "accuracy = metrics.accuracy_score(validation_labels_ft,validation_predictions_rf_ft)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 XGBoost"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.7.1 Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_xgb_w2v,validation_labels_xgb_w2v = adjust_xgb_labels(train_labels_w2v,validation_labels_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_clf_w2v = XGBClassifier(random_state=RANDOM_STATE)\n",
    "xgb_clf_w2v.fit(train_set_w2v, train_labels_xgb_w2v)\n",
    "validation_predictions_xgb_w2v = xgb_clf_w2v.predict(validation_set_w2v)\n",
    "accuracy = metrics.accuracy_score(validation_labels_xgb_w2v,validation_predictions_xgb_w2v)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.7.2 TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_xgb_tfidf,validation_labels_xgb_tfidf = adjust_xgb_labels(train_labels_tfidf,validation_labels_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_clf_tfidf = XGBClassifier(random_state=RANDOM_STATE)\n",
    "xgb_clf_tfidf.fit(train_set_tfidf, train_labels_xgb_tfidf)\n",
    "validation_predictions_xgb_tfidf = xgb_clf_tfidf.predict(validation_set_tfidf)\n",
    "accuracy = metrics.accuracy_score(validation_labels_xgb_tfidf,validation_predictions_xgb_tfidf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.7.3 FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_xgb_ft,validation_labels_xgb_ft = adjust_xgb_labels(train_labels_ft,validation_labels_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_clf_ft = XGBClassifier(random_state=RANDOM_STATE)\n",
    "xgb_clf_ft.fit(train_set_ft, train_labels_xgb_ft)\n",
    "validation_predictions_xgb_ft = xgb_clf_ft.predict(validation_set_ft)\n",
    "accuracy = metrics.accuracy_score(validation_labels_xgb_ft,validation_predictions_xgb_ft)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8 Small Model Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(sentence,embedding,clf):\n",
    "    test_token = nltk.word_tokenize(sentence)\n",
    "    test_vector = np.zeros(embedding.vector_size)\n",
    "    for word in test_token:\n",
    "        if word in embedding.wv:\n",
    "            test_vector += embedding.wv[word]\n",
    "    test_vector /= len(test_token)\n",
    "    predicted_label = clf.predict([test_vector])[0]\n",
    "    return predicted_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLF_TAG = '_clf'\n",
    "clf_name_flag = ['nb','svm','rf','_xgb']\n",
    "word_embed_name_flag = ['_w2v','_tfidf','_ft']\n",
    "FILE_EXT_PICKLE = '.pickle'\n",
    "VALID_BASE_NAME = 'validation_predictions_'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.9 Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_models_pickles(PICKLES_DIR,clf_name_flag,word_embed_name_flag,FILE_EXT_PICKLE,VALID_BASE_NAME,clf_count,embed_count):\n",
    "    for i in range(clf_count):\n",
    "        for j in range(embed_count):\n",
    "            saved_file_name = clf_name_flag[i] + word_embed_name_flag[j]\n",
    "            saved_valid_key_name = VALID_BASE_NAME + clf_name_flag[i] + word_embed_name_flag[j]\n",
    "            pickle_out = open(PICKLES_DIR + '/' + saved_file_name + FILE_EXT_PICKLE,'wb')\n",
    "            pickle.dump(\n",
    "                {saved_file_name:saved_file_name,\\\n",
    "                 saved_valid_key_name:saved_valid_key_name},pickle_out)\n",
    "            pickle_out.close()\n",
    "            print(\"saved file: \" + clf_name_flag[i] + word_embed_name_flag[j] + FILE_EXT_PICKLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models_pickles(PICKLES_DIR,clf_name_flag,word_embed_name_flag,FILE_EXT_PICKLE,VALID_BASE_NAME,clf_count,embed_count):\n",
    "    all_clfs_names = []\n",
    "    all_valid_preds = []\n",
    "    for i in range(clf_count):\n",
    "        for j in range(embed_count):\n",
    "            saved_file_name = clf_name_flag[i] + word_embed_name_flag[j]\n",
    "            saved_valid_key_name = VALID_BASE_NAME + clf_name_flag[i] + word_embed_name_flag[j]\n",
    "            if not os.path.exists(saved_file_name + FILE_EXT_PICKLE):\n",
    "                print(\"file already exists.\")\n",
    "                continue\n",
    "            else:\n",
    "                file = pickle.load(open(PICKLES_DIR + '/' + saved_file_name + FILE_EXT_PICKLE,'rb'))\n",
    "                all_clfs_names.append(file[saved_file_name])\n",
    "                all_valid_preds.append(file[saved_valid_key_name])\n",
    "                \n",
    "    return all_clfs_names,all_valid_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_models_pickles(PICKLES_DIR,clf_name_flag,word_embed_name_flag,FILE_EXT_PICKLE,VALID_BASE_NAME,4,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_clfs,all_validation_predictions = load_models_pickles(PICKLES_DIR,clf_name_flag,word_embed_name_flag,FILE_EXT_PICKLE,VALID_BASE_NAME,4,3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Conclusion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALID_SET_VAR_BASE_NAME = 'validation_set'\n",
    "VALID_LABELS_VAR_BASE_NAME = 'validation_labels'\n",
    "X_BASE_NAME = 'X'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    for j in range(3):\n",
    "        clf = clf_name_flag[i] + '_clf' + word_embed_name_flag[j]\n",
    "        show_metrics(clf,clf,VALID_SET_VAR_BASE_NAME + word_embed_name_flag[j],VALID_LABELS_VAR_BASE_NAME + word_embed_name_flag[j]\\\n",
    "                     ,VALID_BASE_NAME + word_embed_name_flag[j],X_BASE_NAME + (word_embed_name_flag[j]).upper(),y,5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
