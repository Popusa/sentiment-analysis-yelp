{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "I7c7ILKCl5JV"
      },
      "source": [
        "# Sarcasm Detection using Machine Learning"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "mXvrRVhDl5JV"
      },
      "source": [
        "#### (0) Goal of This File:\n",
        "\n",
        "##### 1. Import Libraries, Helper Functions, and Constants ==> Data Sourcing and Munging ==> Utility Functions ==> Loading the Data ==> Merging all Individual Files\n",
        "\n",
        "##### 2. Word Embeddings ==> Train-Validation-Test Split\n",
        "\n",
        "##### 3. Machine Learning Models ==> Models Tests\n",
        "\n",
        "##### 4. Conclusion ==> Results"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "UZX5ZZ_Fl5JW"
      },
      "source": [
        "## (1) Import Libraries, Helper Functions and Load Constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mNZ-tFPhl5JW",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "!pip install pandas numpy nltk scikit-learn wordcloud\\\n",
        "seaborn gensim tensorflow imblearn xgboost matplotlib unrar pyunpack more-itertools patool > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JoY8Pp8Pl5JX",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "VAST = False\n",
        "\n",
        "if VAST:\n",
        "    !sudo apt-get install unrar\n",
        "    !sudo apt-get install rar\n",
        "\n",
        "GDRIVE = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SN7JtZ_fl5JX"
      },
      "outputs": [],
      "source": [
        "UTILS_URL = 'https://f005.backblazeb2.com/file/gp-support-files/archived_nlp_ai_utils.py'\n",
        "UPDATING_VALUES_URL = 'https://f005.backblazeb2.com/file/gp-support-files/archived_updating_values.py'\n",
        "ALL_LIBS_URL = 'https://f005.backblazeb2.com/file/gp-support-files/archived_all_libs_dl.py'\n",
        "CHUNKS_URLS_FILE_URL = 'https://f005.backblazeb2.com/file/gp-support-files/chunks_urls.py'\n",
        "\n",
        "UTILS_FILE_NAME = 'archived_nlp_ai_utils'\n",
        "UPDATING_VALUES_FILE_NAME = 'archived_updating_values'\n",
        "ALL_LIBS_FILE_NAME = 'archived_all_libs_dl'\n",
        "CHUNKS_URLS_FILE_NAME = 'chunks_urls'\n",
        "\n",
        "DEP_FILE_EXT = '.py'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8MYfVj5jl5JX"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import os\n",
        "\n",
        "def get_dependencies(url,file_name,file_extension):\n",
        "    if os.path.exists(file_name + file_extension):\n",
        "        return print(file_name + \" already exists.\")\n",
        "    else:\n",
        "        print(f\"downloading {file_name}...\")\n",
        "        r = requests.get(url)\n",
        "        with open(file_name + file_extension, 'wb') as fd:\n",
        "            for chunk in r.iter_content():\n",
        "                fd.write(chunk)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPXfSxsal5JX",
        "outputId": "35cc5261-2213-466a-ca3b-265e4acf4d86"
      },
      "outputs": [],
      "source": [
        "get_dependencies(UTILS_URL,UTILS_FILE_NAME,DEP_FILE_EXT)\n",
        "get_dependencies(UPDATING_VALUES_URL,UPDATING_VALUES_FILE_NAME,DEP_FILE_EXT)\n",
        "get_dependencies(ALL_LIBS_URL,ALL_LIBS_FILE_NAME,DEP_FILE_EXT)\n",
        "get_dependencies(CHUNKS_URLS_FILE_URL,CHUNKS_URLS_FILE_NAME,DEP_FILE_EXT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3bJepxul5JX",
        "outputId": "e43c36e1-1681-4778-ea17-49137cfddc63"
      },
      "outputs": [],
      "source": [
        "from archived_nlp_ai_utils import *\n",
        "from chunks_urls import SD_CHUNKS_URLS\n",
        "from archived_updating_values import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ns_Pbmbql5JY"
      },
      "outputs": [],
      "source": [
        "TF_ENABLE_ONEDNN_OPTS = 0\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "DATA_URLS = SD_CHUNKS_URLS\n",
        "LIMIT = SD_DATA_LIMIT\n",
        "if GDRIVE:\n",
        "    DATASET_PATH = \"datasets\"\n",
        "    PICKLES_DIR = \"sd_pickle_files\"\n",
        "else:\n",
        "    DATASET_PATH = \"../datasets\"\n",
        "    PICKLES_DIR = \"../sd_pickle_files\"\n",
        "\n",
        "PREPROCESSED_CHUNKS_PATH = DATASET_PATH + \"/preprocessed_sd_data_chunks/\"\n",
        "BASE_FILE_NAME = \"sd_chunk_\"\n",
        "FILE_FORMAT = \".csv\"\n",
        "DATA_IN_CHUNK = 99744\n",
        "if LIMIT == 10:\n",
        "    ACTUAL_DATA_SHAPE = DATA_IN_CHUNK * LIMIT - 2\n",
        "else:\n",
        "    ACTUAL_DATA_SHAPE = DATA_IN_CHUNK * LIMIT\n",
        "RANDOM_STATE = CONST_RANDOM_STATE\n",
        "np.random.seed(CONST_RANDOM_STATE)\n",
        "\n",
        "CV = 3\n",
        "USE_SUBSET = 10000\n",
        "USE_PICKLES = False\n",
        "PRINT_METRICS = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_embedding(text, model, dim):\n",
        "    text = text.lower().split()\n",
        "    words = Counter(text)\n",
        "    total = len(text)\n",
        "    vectors = np.zeros((len(words), dim))\n",
        "    \n",
        "    for i,word in enumerate(words):\n",
        "            v = model.wv[word]\n",
        "            vectors[i] = v*(words[word]/total)\n",
        "    \n",
        "    if vectors.any():\n",
        "        vector = np.average(vectors, axis=0)\n",
        "    else:\n",
        "        vector = np.zeros((dim))\n",
        "    \n",
        "    return vector"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "o64oqsFel5JY"
      },
      "source": [
        "### 1.1 Data Sourcing and Munging"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "CgvrBd5-l5JY"
      },
      "source": [
        "#### 1.1.1 Loading The Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ofkCbKZBl5JY",
        "outputId": "2ce9fdc4-3620-4572-ca51-e1a45051d081"
      },
      "outputs": [],
      "source": [
        "DATASET_PATH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "mhV6oZagl5JY",
        "outputId": "2478b2d1-cea0-454a-cb92-57ab10b012b6"
      },
      "outputs": [],
      "source": [
        "PREPROCESSED_CHUNKS_PATH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4OgA1E7Kl5JY",
        "outputId": "5b018df1-324a-4aa3-b665-69e86c6be381",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(DATASET_PATH):\n",
        "    os.mkdir(DATASET_PATH)\n",
        "if not os.path.exists(PREPROCESSED_CHUNKS_PATH):\n",
        "    os.mkdir(PREPROCESSED_CHUNKS_PATH)\n",
        "get_chunks(DATA_URLS,LIMIT,1,BASE_FILE_NAME,PREPROCESSED_CHUNKS_PATH,FILE_FORMAT)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "k_yzumQel5JZ"
      },
      "source": [
        "#### 1.1.2 Merging all Individual Files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tBG9E1G5l5JZ"
      },
      "outputs": [],
      "source": [
        "#get all names of downloaded files\n",
        "all_file_names = get_all_file_names(BASE_FILE_NAME,LIMIT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yhXKIqaKl5JZ"
      },
      "outputs": [],
      "source": [
        "#read all chunks into a list\n",
        "list_dfs = read_chunks(all_file_names,PREPROCESSED_CHUNKS_PATH,FILE_FORMAT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75eTnzcrl5JZ"
      },
      "outputs": [],
      "source": [
        "#concatenate all chunks into a singular df\n",
        "df = group_up_chunks(list_dfs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2fiKDh5YHEep"
      },
      "outputs": [],
      "source": [
        "if USE_SUBSET:\n",
        "    df = df[:USE_SUBSET]\n",
        "    ACTUAL_DATA_SHAPE = USE_SUBSET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UD7e50qnl5JZ",
        "outputId": "812aa099-8427-4aad-85c4-577fc11a4c06"
      },
      "outputs": [],
      "source": [
        "#check how much of the data was actually downloaded\n",
        "percent_loaded = check_no_missing_data(df.shape[0],ACTUAL_DATA_SHAPE)\n",
        "percent_loaded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AaEFioxol5JZ"
      },
      "outputs": [],
      "source": [
        "df.reset_index(inplace = True)\n",
        "df.drop(['index'],axis = 1,inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QVidO0ull5JZ"
      },
      "outputs": [],
      "source": [
        "X = df['text']\n",
        "y = df['labels']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yrHi0XDbl5JZ"
      },
      "outputs": [],
      "source": [
        "X = pd.Series([str(text) for text in X])\n",
        "\n",
        "y = pd.Series([int(label) for label in y])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ASsTelql5JZ"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(PICKLES_DIR):\n",
        "    os.mkdir(PICKLES_DIR)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "fLnoo0APl5Ja"
      },
      "source": [
        "## (2) Word Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WsPw9HPDHEeq"
      },
      "outputs": [],
      "source": [
        "TRAIN_PERCENT = 0.8\n",
        "VALID_TEST_PERCENT = 0.1\n",
        "TRAIN_SIZE = int(ACTUAL_DATA_SHAPE * TRAIN_PERCENT)\n",
        "VALID_TEST_SIZE = int(ACTUAL_DATA_SHAPE * VALID_TEST_PERCENT)\n",
        "TOTAL_TEST_SIZE = TRAIN_SIZE + VALID_TEST_SIZE"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "uKIffQELl5Ja"
      },
      "source": [
        "### 2.1 Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_W2V = X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "w2v_model = Word2Vec(sentences=[nltk.word_tokenize(text) for text in X_W2V], vector_size=100, window=5, min_count=1, workers=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "word_index_w2v = {}\n",
        "\n",
        "for i, word in enumerate(w2v_model.wv.key_to_index):\n",
        "    word_index_w2v[word] = i"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vocab_size = len(word_index_w2v)\n",
        "embedding_dim_w2v = w2v_model.vector_size\n",
        "\n",
        "embedding_matrix_w2v = np.zeros((vocab_size, embedding_dim_w2v))\n",
        "\n",
        "for word, i in word_index_w2v.items():\n",
        "    if word in w2v_model.wv.key_to_index:\n",
        "        embedding_matrix_w2v[i] = w2v_model.wv.get_vector(word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_text_w2v = np.zeros((X_W2V.shape[0], 100))\n",
        "\n",
        "for i, text in enumerate(X_W2V.values):\n",
        "    X_text_w2v[i] = get_embedding(text, w2v_model, 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_set_w2v = X_text_w2v[:TRAIN_SIZE]\n",
        "validation_set_w2v = X_text_w2v[TRAIN_SIZE:TOTAL_TEST_SIZE]\n",
        "test_set_w2v = X_text_w2v[TOTAL_TEST_SIZE:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_labels_w2v = y[:TRAIN_SIZE]\n",
        "validation_labels_w2v = y[TRAIN_SIZE:TOTAL_TEST_SIZE]\n",
        "test_labels_w2v = y[TOTAL_TEST_SIZE:]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Ch6nrOc4l5Ja"
      },
      "source": [
        "### 2.2 TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vE2f_m2hl5Ja"
      },
      "outputs": [],
      "source": [
        "X_TFIDF = X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tfidf_vect = TfidfVectorizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_TFIDF = tfidf_vect.fit_transform(X_TFIDF)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_set_tfidf = X_TFIDF[:TRAIN_SIZE].toarray()\n",
        "validation_set_tfidf = X_TFIDF[TRAIN_SIZE:TOTAL_TEST_SIZE].toarray()\n",
        "test_set_tfidf = X_TFIDF[TOTAL_TEST_SIZE:].toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_labels_tfidf = y[:TRAIN_SIZE]\n",
        "validation_labels_tfidf = y[TRAIN_SIZE:TOTAL_TEST_SIZE]\n",
        "test_labels_tfidf = y[TOTAL_TEST_SIZE:]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ybuKIVtLl5Jb"
      },
      "source": [
        "### 2.3 FastText"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPTbzoH2l5Jb"
      },
      "outputs": [],
      "source": [
        "X_FT = X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBO1mYGDl5Jj"
      },
      "outputs": [],
      "source": [
        "ft_model = FastText(sentences=[nltk.word_tokenize(text) for text in X_FT], vector_size=100, window=5, min_count=1, workers=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1jKxWw2Hhmk"
      },
      "outputs": [],
      "source": [
        "word_index_ft = {}\n",
        "\n",
        "for i, word in enumerate(ft_model.wv.key_to_index):\n",
        "    word_index_ft[word] = i"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vntRmT70Hhmk"
      },
      "outputs": [],
      "source": [
        "vocab_size = len(word_index_ft)\n",
        "embedding_dim_ft = ft_model.vector_size\n",
        "\n",
        "embedding_matrix_ft = np.zeros((vocab_size, embedding_dim_ft))\n",
        "\n",
        "for word, i in word_index_ft.items():\n",
        "    if word in ft_model.wv.key_to_index:\n",
        "        embedding_matrix_ft[i] = ft_model.wv.get_vector(word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V5lWeg_BHhml"
      },
      "outputs": [],
      "source": [
        "X_text_ft = np.zeros((X_FT.shape[0], 100))\n",
        "\n",
        "for i, text in enumerate(X_FT.values):\n",
        "    X_text_ft[i] = get_embedding(text, ft_model, 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D49-w9crHhml"
      },
      "outputs": [],
      "source": [
        "train_set_ft = X_text_ft[:TRAIN_SIZE]\n",
        "validation_set_ft = X_text_ft[TRAIN_SIZE:TOTAL_TEST_SIZE]\n",
        "test_set_ft = X_text_ft[TOTAL_TEST_SIZE:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_labels_ft = y[:TRAIN_SIZE]\n",
        "validation_labels_ft = y[TRAIN_SIZE:TOTAL_TEST_SIZE]\n",
        "test_labels_ft = y[TOTAL_TEST_SIZE:]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "H8eTc5x_l5Jj"
      },
      "source": [
        "## (3) Machine Learning Models"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Normalization and Data Scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3EpARsJGl5Jj"
      },
      "outputs": [],
      "source": [
        "train_set_w2v = preprocessing.normalize(train_set_w2v)\n",
        "validation_set_w2v = preprocessing.normalize(validation_set_w2v)\n",
        "test_set_w2v = preprocessing.normalize(test_set_w2v)\n",
        "train_set_tfidf = preprocessing.normalize(train_set_tfidf)\n",
        "validation_set_tfidf = preprocessing.normalize(validation_set_tfidf)\n",
        "test_set_tfidf = preprocessing.normalize(test_set_tfidf)\n",
        "train_set_ft = preprocessing.normalize(train_set_ft)\n",
        "validation_set_ft = preprocessing.normalize(validation_set_ft)\n",
        "test_set_ft = preprocessing.normalize(test_set_ft)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5N1dpQKzHEe0"
      },
      "outputs": [],
      "source": [
        "sc_w2v_ft = StandardScaler()\n",
        "sc_tfidf_train = StandardScaler()\n",
        "sc_tfidf_valid = StandardScaler()\n",
        "train_set_w2v = sc_w2v_ft.fit_transform(train_set_w2v)\n",
        "validation_set_w2v = sc_w2v_ft.transform(validation_set_w2v)\n",
        "test_set_w2v = sc_w2v_ft.transform(test_set_w2v)\n",
        "train_set_tfidf = sc_tfidf_train.fit_transform(train_set_tfidf)\n",
        "validation_set_tfidf = sc_tfidf_valid.fit_transform(validation_set_tfidf)\n",
        "test_set_tfidf = sc_tfidf_valid.fit_transform(test_set_tfidf)\n",
        "train_set_ft = sc_w2v_ft.fit_transform(train_set_ft)\n",
        "validation_set_ft = sc_w2v_ft.transform(validation_set_ft)\n",
        "test_set_ft = sc_w2v_ft.transform(test_set_ft)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_XWFrMzyl5Jk"
      },
      "source": [
        "### 3.2 Naive Bayes"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "UaBEXle-l5Jk"
      },
      "source": [
        "#### 3.2.1 Word2Vec "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ajw2T1uql5Jk",
        "outputId": "fa9a2957-6bb6-4df8-a9c3-c879450f803b"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(PICKLES_DIR + '/' + 'all_nb' + '.pickle') or not USE_PICKLES:\n",
        "    print(\"Training Model...\")\n",
        "    nb_clf_w2v = GaussianNB()\n",
        "    nb_clf_w2v.fit(train_set_w2v, train_labels_w2v)\n",
        "    validation_predictions_nb_w2v = nb_clf_w2v.predict(validation_set_w2v)\n",
        "    nb_w2v_accuracy = metrics.accuracy_score(validation_labels_w2v,validation_predictions_nb_w2v)\n",
        "else:\n",
        "    print(\"Found Pickle File.\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "col7Sutkl5Jk"
      },
      "source": [
        "#### 3.2.2 TF-IDF "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXvIlqd5l5Jk",
        "outputId": "275acb9f-42e1-4705-a6b3-7696c10b04e2"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(PICKLES_DIR + '/' + 'all_nb' + '.pickle') or not USE_PICKLES:\n",
        "    print(\"Training Model...\")\n",
        "    nb_clf_tfidf = GaussianNB()\n",
        "    nb_clf_tfidf.fit(train_set_tfidf, train_labels_tfidf)\n",
        "    validation_predictions_nb_tfidf = nb_clf_tfidf.predict(validation_set_tfidf)\n",
        "    nb_tfidf_accuracy = metrics.accuracy_score(validation_labels_tfidf,validation_predictions_nb_tfidf)\n",
        "else:\n",
        "    print(\"Found Pickle File.\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "oVrRQmbml5Jk"
      },
      "source": [
        "#### 3.2.3 FastText"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7EDYchWl5Jk",
        "outputId": "fbf5b16d-8574-4748-d508-de00b9cad115"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(PICKLES_DIR + '/' + 'all_nb' + '.pickle') or not USE_PICKLES:\n",
        "    print(\"Training Model...\")\n",
        "    nb_clf_ft = GaussianNB()\n",
        "    nb_clf_ft.fit(train_set_ft, train_labels_ft)\n",
        "    validation_predictions_nb_ft = nb_clf_ft.predict(validation_set_ft)\n",
        "    nb_ft_accuracy = metrics.accuracy_score(validation_labels_ft,validation_predictions_nb_ft)\n",
        "else:\n",
        "    print(\"Found Pickle File.\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "CZJXuO5ol5Jl"
      },
      "source": [
        "### 3.3 Logistic Regression"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_ui2iPLKl5Jl"
      },
      "source": [
        "#### 3.3.1 Word2Vec "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qgebCgPal5Jl",
        "outputId": "ed55c1e4-5858-407d-e002-c634ecd56d67"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(PICKLES_DIR + '/' + 'all_lr' + '.pickle') or not USE_PICKLES:\n",
        "    print(\"Training Model...\")\n",
        "    lr_clf_w2v = LogisticRegression(random_state=RANDOM_STATE)\n",
        "    lr_clf_w2v.fit(train_set_w2v, train_labels_w2v)\n",
        "    validation_predictions_lr_w2v = lr_clf_w2v.predict(validation_set_w2v)\n",
        "    lr_w2v_accuracy = metrics.accuracy_score(validation_labels_w2v,validation_predictions_lr_w2v)\n",
        "else:\n",
        "    print(\"Found Pickle File.\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "sVaZp1Szl5Jl"
      },
      "source": [
        "#### 3.3.2 TF-IDF "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYHG0xm1l5Jl"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(PICKLES_DIR + '/' + 'all_lr' + '.pickle') or not USE_PICKLES:\n",
        "    lr_clf_tfidf = LogisticRegression(random_state=RANDOM_STATE,max_iter=1000)\n",
        "    lr_clf_tfidf.fit(train_set_tfidf, train_labels_tfidf)\n",
        "    validation_predictions_lr_tfidf = lr_clf_tfidf.predict(validation_set_tfidf)\n",
        "    lr_tfidf_accuracy = metrics.accuracy_score(validation_labels_tfidf,validation_predictions_lr_tfidf)\n",
        "else:\n",
        "    print(\"Found Pickle File.\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "GtGHa9Ewl5Jl"
      },
      "source": [
        "#### 3.3.3 FastText"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t19R6Z_Yl5Jl",
        "outputId": "09c97718-6117-456d-bacb-c94f1f7bd4e7"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(PICKLES_DIR + '/' + 'all_lr' + '.pickle') or not USE_PICKLES:\n",
        "    print(\"Training Model...\")\n",
        "    lr_clf_ft = LogisticRegression(random_state=RANDOM_STATE,max_iter=1000)\n",
        "    lr_clf_ft.fit(train_set_ft, train_labels_ft)\n",
        "    validation_predictions_lr_ft = lr_clf_ft.predict(validation_set_ft)\n",
        "    lr_ft_accuracy = metrics.accuracy_score(validation_labels_ft,validation_predictions_lr_ft)\n",
        "else:\n",
        "    print(\"Found Pickle File.\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "gONmcQEOl5Jl"
      },
      "source": [
        "### 3.4 Random Forest"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "krP76FBWl5Jl"
      },
      "source": [
        "#### 3.4.1 Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojcT-gRAl5Jm",
        "outputId": "6b7fbb06-d31d-418f-ccd7-696724db4ef1"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(PICKLES_DIR + '/' + 'all_rf' + '.pickle') or not USE_PICKLES:\n",
        "    print(\"Training Model...\")\n",
        "    rf_clf_w2v = RandomForestClassifier(n_estimators=50,random_state=RANDOM_STATE)\n",
        "    rf_clf_w2v.fit(train_set_w2v, train_labels_w2v)\n",
        "    validation_predictions_rf_w2v = rf_clf_w2v.predict(validation_set_w2v)\n",
        "    rf_w2v_accuracy = metrics.accuracy_score(validation_labels_w2v,validation_predictions_rf_w2v)\n",
        "else:\n",
        "    print(\"Found Pickle File.\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ilGrAKKZl5Jm"
      },
      "source": [
        "#### 3.4.2 TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9VrrIg4l5Jm",
        "outputId": "93176127-a7da-4ccd-e341-a99c743c44cf"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(PICKLES_DIR + '/' + 'all_rf' + '.pickle') or not USE_PICKLES:\n",
        "    print(\"Training Model...\")\n",
        "    rf_clf_tfidf = RandomForestClassifier(n_estimators=50,random_state=RANDOM_STATE)\n",
        "    rf_clf_tfidf.fit(train_set_tfidf, train_labels_tfidf)\n",
        "    validation_predictions_rf_tfidf = rf_clf_tfidf.predict(validation_set_tfidf)\n",
        "    rf_tfidf_accuracy = metrics.accuracy_score(validation_labels_tfidf,validation_predictions_rf_tfidf)\n",
        "else:\n",
        "    print(\"Found Pickle File.\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "eRn1mF94l5Jm"
      },
      "source": [
        "#### 3.4.3 FastText"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOVGwQFal5Jm",
        "outputId": "4c258d3b-1ae0-45dd-9454-fd8eabf91900"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(PICKLES_DIR + '/' + 'all_rf' + '.pickle') or not USE_PICKLES:\n",
        "    print(\"Training Model...\")\n",
        "    rf_clf_ft = RandomForestClassifier(n_estimators=50,random_state=RANDOM_STATE)\n",
        "    rf_clf_ft.fit(train_set_ft, train_labels_ft)\n",
        "    validation_predictions_rf_ft = rf_clf_ft.predict(validation_set_ft)\n",
        "    rf_ft_accuracy = metrics.accuracy_score(validation_labels_ft,validation_predictions_rf_ft)\n",
        "else:\n",
        "    print(\"Found Pickle File.\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "fjnbdgfhl5Jm"
      },
      "source": [
        "### 3.5 XGBoost"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "YvK9xicvl5Jm"
      },
      "source": [
        "#### 3.5.1 Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_Yt6NsPl5Jm",
        "outputId": "8cc4f843-32fa-46d3-9fbb-fe9319c02706"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(PICKLES_DIR + '/' + 'all_xgb' + '.pickle') or not USE_PICKLES:\n",
        "    print(\"Training Model...\")\n",
        "    xgb_clf_w2v = XGBClassifier(random_state=RANDOM_STATE)\n",
        "    xgb_clf_w2v.fit(train_set_w2v, train_labels_w2v)\n",
        "    validation_predictions_xgb_w2v = xgb_clf_w2v.predict(validation_set_w2v)\n",
        "    xgb_w2v_accuracy = metrics.accuracy_score(validation_labels_w2v,validation_predictions_xgb_w2v)\n",
        "else:\n",
        "    print(\"Found Pickle File.\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "cCmWxbK0l5Jn"
      },
      "source": [
        "#### 3.5.2 TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7FckmxFl5Jn"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(PICKLES_DIR + '/' + 'all_xgb' + '.pickle') or not USE_PICKLES:\n",
        "    xgb_clf_tfidf = XGBClassifier(random_state=RANDOM_STATE)\n",
        "    xgb_clf_tfidf.fit(train_set_tfidf, train_labels_tfidf)\n",
        "    validation_predictions_xgb_tfidf = xgb_clf_tfidf.predict(validation_set_tfidf)\n",
        "    xgb_tfidf_accuracy = metrics.accuracy_score(validation_labels_tfidf,validation_predictions_xgb_tfidf)\n",
        "else:\n",
        "    print(\"Found Pickle File.\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6yGrj0wul5Jn"
      },
      "source": [
        "#### 3.5.3 FastText"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PPPw7jDNl5Jn",
        "outputId": "877e4d94-241d-4aad-fb63-c0e51f48edcc"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(PICKLES_DIR + '/' + 'all_xgb' + '.pickle') or not USE_PICKLES:\n",
        "    print(\"Training Model...\")\n",
        "    xgb_clf_ft = XGBClassifier(random_state=RANDOM_STATE)\n",
        "    xgb_clf_ft.fit(train_set_ft, train_labels_ft)\n",
        "    validation_predictions_xgb_ft = xgb_clf_ft.predict(validation_set_ft)\n",
        "    xgb_ft_accuracy = metrics.accuracy_score(validation_labels_ft,validation_predictions_xgb_ft)\n",
        "else:\n",
        "    print(\"Found Pickle File.\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Jub1MfRCl5Jn"
      },
      "source": [
        "### 3.6 Small Model Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DWDbD4D_l5Jn"
      },
      "outputs": [],
      "source": [
        "def test_model(sentence,embedding,clf):\n",
        "    test_token = nltk.word_tokenize(sentence)\n",
        "    test_vector = np.zeros(embedding.vector_size)\n",
        "    for word in test_token:\n",
        "        if word in embedding.wv:\n",
        "            test_vector += embedding.wv[word]\n",
        "    test_vector /= len(test_token)\n",
        "    predicted_label = clf.predict([test_vector])[0]\n",
        "    return predicted_label"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "dBjYJjinl5Jo"
      },
      "source": [
        "#### 3.7 Save Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VRX2SHSzHhmq"
      },
      "outputs": [],
      "source": [
        "if USE_PICKLES:\n",
        "    all_models = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8y_rIgQNl5Jo",
        "outputId": "b0643899-b42d-45fa-bcb0-c09c450baa6f"
      },
      "outputs": [],
      "source": [
        "if os.path.exists(PICKLES_DIR + '/' + \"all_nb.pickle\"):\n",
        "    if USE_PICKLES:\n",
        "        print(\"Found Pickle File for nb.\")\n",
        "        all_nb = pickle.load(open(PICKLES_DIR + '/' + 'all_nb.pickle','rb'))\n",
        "else:\n",
        "    pickle_out = open(PICKLES_DIR + '/' + 'all_nb' + '.pickle','wb')\n",
        "    nb_dict = {'nb_clf_w2v':nb_clf_w2v,'nb_clf_w2v_preds':validation_predictions_nb_w2v,'nb_clf_tfidf':nb_clf_tfidf,\\\n",
        "        'nb_clf_tfidf_preds':validation_predictions_nb_tfidf,'nb_clf_ft':nb_clf_ft,'nb_clf_ft_preds':validation_predictions_nb_ft,}\n",
        "    pickle.dump(nb_dict,pickle_out)\n",
        "    pickle_out.close()\n",
        "    all_nb = nb_dict\n",
        "    print(\"Created Pickle File for nb.\")\n",
        "    \n",
        "if USE_PICKLES:\n",
        "    all_models.append(all_nb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KFYD-Z1BHhmq",
        "outputId": "3354160a-3dd0-4e20-ea30-f6be4dbf1cd1"
      },
      "outputs": [],
      "source": [
        "if os.path.exists(PICKLES_DIR + '/' + \"all_lr.pickle\"):\n",
        "    if USE_PICKLES:\n",
        "        print(\"Found Pickle File for lr.\")\n",
        "        all_lr = pickle.load(open(PICKLES_DIR + '/' + 'all_lr.pickle','rb'))\n",
        "else:\n",
        "    pickle_out = open(PICKLES_DIR + '/' + 'all_lr' + '.pickle','wb')\n",
        "    lr_dict = {'lr_clf_w2v':lr_clf_w2v,'lr_clf_w2v_preds':validation_predictions_lr_w2v,'lr_clf_tfidf':lr_clf_tfidf,\\\n",
        "        'lr_clf_tfidf_preds':validation_predictions_lr_tfidf,'lr_clf_ft':lr_clf_ft,'lr_clf_ft_preds':validation_predictions_lr_ft}\n",
        "    pickle.dump(lr_dict,pickle_out)\n",
        "    pickle_out.close()\n",
        "    all_lr = lr_dict\n",
        "    print(\"Created Pickle File for lr.\")\n",
        "\n",
        "if USE_PICKLES:\n",
        "    all_models.append(all_lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdbnC4gZHhmq",
        "outputId": "e5e25b93-a85c-4dfe-89be-be75c0952782"
      },
      "outputs": [],
      "source": [
        "if os.path.exists(PICKLES_DIR + '/' + \"all_rf.pickle\"):\n",
        "    if USE_PICKLES:\n",
        "        print(\"Found Pickle File for rf.\")\n",
        "        all_rf = pickle.load(open(PICKLES_DIR + '/' + 'all_rf.pickle','rb'))\n",
        "else:\n",
        "    pickle_out = open(PICKLES_DIR + '/' + 'all_rf' + '.pickle','wb')\n",
        "    rf_dict = {'rf_clf_w2v':rf_clf_w2v,'rf_clf_w2v_preds':validation_predictions_rf_w2v,'rf_clf_tfidf':rf_clf_tfidf,\\\n",
        "        'rf_clf_tfidf_preds':validation_predictions_rf_tfidf,'rf_clf_ft':rf_clf_ft,'rf_clf_ft_preds':validation_predictions_rf_ft}\n",
        "    pickle.dump(rf_dict,pickle_out)\n",
        "    pickle_out.close()\n",
        "    all_rf = rf_dict\n",
        "    print(\"Created Pickle File for rf.\")\n",
        "    \n",
        "if USE_PICKLES:\n",
        "    all_models.append(all_rf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjgeWaWDHhmr",
        "outputId": "2203fc0e-e0d3-42ce-e3ac-7912920a4878"
      },
      "outputs": [],
      "source": [
        "if os.path.exists(PICKLES_DIR + \"/\" + \"all_xgb.pickle\"):\n",
        "    if USE_PICKLES:\n",
        "        print(\"Found Pickle File for xgb.\")\n",
        "        all_xgb = pickle.load(open(PICKLES_DIR + '/' + 'all_xgb.pickle','rb'))\n",
        "else:\n",
        "    pickle_out = open(PICKLES_DIR + '/' + 'all_xgb' + '.pickle','wb')\n",
        "    xgb_dict = {'xgb_clf_w2v':xgb_clf_w2v,'xgb_clf_w2v_preds':validation_predictions_xgb_w2v,'xgb_clf_tfidf':xgb_clf_tfidf,\\\n",
        "        'xgb_clf_tfidf_preds':validation_predictions_xgb_tfidf,'xgb_clf_ft':xgb_clf_ft,'xgb_clf_ft_preds':validation_predictions_xgb_ft,}\n",
        "    pickle.dump(xgb_dict,pickle_out)\n",
        "    pickle_out.close()\n",
        "    all_xgb = xgb_dict\n",
        "    print(\"Created Pickle File for xgb.\")\n",
        "    \n",
        "if USE_PICKLES:    \n",
        "    all_models.append(all_xgb)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Om-vRHUKl5Jo"
      },
      "source": [
        "## (4) Conclusion"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "PfNk3V8xl5Jo"
      },
      "source": [
        "### 4.1 Results"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "cSoSy40SHhmr"
      },
      "source": [
        "### DISCLAIMER: VISUALIZATIONS CAN TAKE A WHILE TO RUN. IF YOU DO NOT WANT TO WAIT A LONG TIME TO RUN THE WHOLE FILE, SET THE \"PRINT_METRICS\" FLAG TO FALSE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GRcvwFyul5Jo",
        "outputId": "dcdad1b1-7271-4a0b-e55e-1f6227173bac"
      },
      "outputs": [],
      "source": [
        "if PRINT_METRICS:\n",
        "    if USE_PICKLES:\n",
        "        show_metrics('Gaussian Naive Bayes/Word2Vec',all_models[0]['nb_clf_w2v'],validation_set_w2v,validation_labels_w2v,all_models[0]['nb_clf_w2v_preds'],\\\n",
        "                    np.concatenate([train_set_w2v,validation_set_w2v,test_set_w2v]),y,5)\n",
        "        show_metrics('Gaussian Naive Bayes/TFIDF',all_models[0]['nb_clf_tfidf'],validation_set_tfidf,validation_labels_tfidf,all_models[0]['nb_clf_tfidf_preds'],\\\n",
        "                    np.concatenate([train_set_tfidf,validation_set_tfidf,test_set_tfidf]),y,5)\n",
        "        show_metrics('Gaussian Naive Bayes/FastText',all_models[0]['nb_clf_ft'],validation_set_ft,validation_labels_ft,all_models[0]['nb_clf_ft_preds'],\\\n",
        "                    np.concatenate([train_set_ft,validation_set_ft,test_set_ft]),y,5)\n",
        "        \n",
        "    else:\n",
        "        show_metrics('Gaussian Naive Bayes/Word2Vec',nb_clf_w2v,validation_set_w2v,validation_labels_w2v,validation_predictions_nb_w2v,\\\n",
        "                    np.concatenate([train_set_w2v,validation_set_w2v,test_set_w2v]),y,5)\n",
        "        show_metrics('Gaussian Naive Bayes/TFIDF',nb_clf_tfidf,validation_set_tfidf,validation_labels_tfidf,validation_predictions_nb_tfidf,\\\n",
        "                    np.concatenate([train_set_tfidf,validation_set_tfidf,test_set_tfidf]),y,5)\n",
        "        show_metrics('Gaussian Naive Bayes/FastText',nb_clf_ft,validation_set_ft,validation_labels_ft,validation_predictions_nb_ft,\\\n",
        "                    np.concatenate([train_set_ft,validation_set_ft,test_set_ft]),y,5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QnSY-BLFHhmr",
        "outputId": "1d5a1ff2-09e4-4a86-8cc5-10453f66c8a3"
      },
      "outputs": [],
      "source": [
        "if PRINT_METRICS:\n",
        "    if USE_PICKLES:\n",
        "        show_metrics('Logistic Regression/Word2Vec',all_models[1]['lr_clf_w2v'],validation_set_w2v,validation_labels_w2v,all_models[1]['lr_clf_w2v_preds'],\\\n",
        "                    np.concatenate([train_set_w2v,validation_set_w2v,test_set_w2v]),y,5)\n",
        "        show_metrics('Logistic Regression/TFIDF',all_models[1]['lr_clf_tfidf'],validation_set_tfidf,validation_labels_tfidf,all_models[1]['lr_clf_tfidf_preds'],\\\n",
        "                    np.concatenate([train_set_tfidf,validation_set_tfidf,test_set_tfidf]),y,5)\n",
        "        show_metrics('Logistic Regression/FastText',all_models[1]['lr_clf_ft'],validation_set_ft,validation_labels_ft,all_models[1]['lr_clf_ft_preds'],\\\n",
        "                    np.concatenate([train_set_ft,validation_set_ft,test_set_ft]),y,5)\n",
        "\n",
        "    else:\n",
        "        show_metrics('Logistic Regression/Word2Vec',lr_clf_w2v,validation_set_w2v,validation_labels_w2v,validation_predictions_lr_w2v,\\\n",
        "                    np.concatenate([train_set_w2v,validation_set_w2v,test_set_w2v]),y,5)\n",
        "        show_metrics('Logistic Regression/TFIDF',lr_clf_tfidf,validation_set_tfidf,validation_labels_tfidf,validation_predictions_lr_tfidf,\\\n",
        "                    np.concatenate([train_set_tfidf,validation_set_tfidf,test_set_tfidf]),y,5)\n",
        "        show_metrics('Logistic Regression/FastText',lr_clf_ft,validation_set_ft,validation_labels_ft,validation_predictions_lr_ft,\\\n",
        "                    np.concatenate([train_set_ft,validation_set_ft,test_set_ft]),y,5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SxKOCjVFHhmr",
        "outputId": "d57ea23a-eecb-4ff5-da70-9a5872121ea4"
      },
      "outputs": [],
      "source": [
        "if PRINT_METRICS:\n",
        "    if USE_PICKLES:\n",
        "        show_metrics('Random Forest/Word2Vec',all_models[2]['rf_clf_w2v'],validation_set_w2v,validation_labels_w2v,all_models[2]['rf_clf_w2v_preds'],X_W2V,y,CV)\n",
        "        show_metrics('Random Forest/TFIDF',all_models[2]['rf_clf_tfidf'],validation_set_tfidf,validation_labels_tfidf,all_models[2]['rf_clf_tfidf_preds'],\\\n",
        "                    np.concatenate([train_set_tfidf,validation_set_tfidf,test_set_tfidf]),y,CV)\n",
        "        show_metrics('Random Forest/FastText',all_models[2]['rf_clf_ft'],validation_set_ft,validation_labels_ft,all_models[2]['rf_clf_ft_preds'],X_FT,y,CV)\n",
        "\n",
        "    else:\n",
        "        show_metrics('Random Forest/Word2Vec',rf_clf_w2v,validation_set_w2v,validation_labels_w2v,validation_predictions_rf_w2v,\\\n",
        "                    np.concatenate([train_set_w2v,validation_set_w2v,test_set_w2v]),y,CV)\n",
        "        show_metrics('Random Forest/TFIDF',rf_clf_tfidf,validation_set_tfidf,validation_labels_tfidf,validation_predictions_rf_tfidf,\\\n",
        "                    np.concatenate([train_set_tfidf,validation_set_tfidf,test_set_tfidf]),y,CV)\n",
        "        show_metrics('Random Forest/FastText',rf_clf_ft,validation_set_ft,validation_labels_ft,validation_predictions_rf_ft,\\\n",
        "                    np.concatenate([train_set_ft,validation_set_ft,test_set_ft]),y,CV)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 709
        },
        "id": "OxMwMEseHhms",
        "outputId": "aa7a084f-2d8f-4cd9-b90a-a0de61e3d4c2"
      },
      "outputs": [],
      "source": [
        "if PRINT_METRICS:\n",
        "    if USE_PICKLES:\n",
        "        show_metrics('XGBoost/Word2Vec',all_models[3]['xgb_clf_w2v'],validation_set_w2v,validation_labels_w2v,all_models[3]['xgb_clf_w2v_preds'],X_W2V,y,CV)\n",
        "        show_metrics('XGBoost/TFIDF',all_models[3]['xgb_clf_tfidf'],validation_set_tfidf,validation_labels_tfidf,all_models[3]['xgb_clf_tfidf_preds'],\\\n",
        "                    np.concatenate([train_set_tfidf,validation_set_tfidf,test_set_tfidf]),y,CV)\n",
        "        show_metrics('XGBoost/FastText',all_models[3]['xgb_clf_ft'],validation_set_ft,validation_labels_ft,all_models[3]['xgb_clf_ft_preds'],X_FT,y,CV)\n",
        "\n",
        "    else:\n",
        "        show_metrics('XGBoost/Word2Vec',xgb_clf_w2v,validation_set_w2v,validation_labels_w2v,validation_predictions_xgb_w2v,\\\n",
        "                    np.concatenate([train_set_w2v,validation_set_w2v,test_set_w2v]),y,CV)\n",
        "        show_metrics('XGBoost/TFIDF',xgb_clf_tfidf,validation_set_tfidf,validation_labels_tfidf,validation_predictions_xgb_tfidf,\\\n",
        "                    np.concatenate([train_set_tfidf,validation_set_tfidf,test_set_tfidf]),y,CV)\n",
        "        show_metrics('XGBoost/FastText',xgb_clf_ft,validation_set_ft,validation_labels_ft,validation_predictions_xgb_ft,\\\n",
        "                    np.concatenate([train_set_ft,validation_set_ft,test_set_ft]),y,CV)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "classifiers_nb = [{'nb_w2v':nb_clf_w2v,'nb_tfidf':nb_clf_tfidf,'nb_ft':nb_clf_ft}]\n",
        "\n",
        "classifiers_lr = [{'lr_w2v':lr_clf_w2v,'lr_tfidf':lr_clf_tfidf,'lr_ft':lr_clf_ft}]\n",
        "\n",
        "classifiers_rf = [{'rf_w2v':rf_clf_w2v,'rf_tfidf':rf_clf_tfidf,'rf_ft':rf_clf_ft}]\n",
        "\n",
        "classifiers_xgb = [{'xgb_w2v':xgb_clf_w2v,'xgb_tfidf':xgb_clf_tfidf,'xgb_ft':xgb_clf_ft}]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "colors = ['r','g','b']\n",
        "\n",
        "emb_labels = ['W2V','TFIDF','FT']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if PRINT_METRICS:\n",
        "    embed_headers = list(classifiers_nb[0].keys())\n",
        "    classifier_headers = list(classifiers_nb[0].values())\n",
        "    for i, classifier in enumerate(classifiers_nb):\n",
        "        # Compute learning curves\n",
        "        current_embed = embed_headers[i].split('_')[-1]\n",
        "        current_classifier = classifier[i]\n",
        "        if current_embed == 'w2v':\n",
        "            train_sizes, train_scores, test_scores = learning_curve(classifier[i], np.concatenate([train_set_w2v,validation_set_w2v,test_set_w2v]),\\\n",
        "                                                                    y, cv=CV, n_jobs=-1, train_sizes=np.linspace(0.1, 1.0, 10))\n",
        "        elif current_embed == 'tfidf':\n",
        "            train_sizes, train_scores, test_scores = learning_curve(classifier[i], np.concatenate([train_set_tfidf,validation_set_tfidf,test_set_tfidf]),\\\n",
        "                                                                    y, cv=CV, n_jobs=-1, train_sizes=np.linspace(0.1, 1.0, 10))\n",
        "        else:\n",
        "            train_sizes, train_scores, test_scores = learning_curve(classifier[i], np.concatenate([train_set_ft,validation_set_ft,test_set_ft]),\\\n",
        "                                                                    y, cv=CV, n_jobs=-1, train_sizes=np.linspace(0.1, 1.0, 10))\n",
        "        train_mean = np.mean(train_scores, axis=1)\n",
        "        train_std = np.std(train_scores, axis=1)\n",
        "        test_mean = np.mean(test_scores, axis=1)\n",
        "        test_std = np.std(test_scores, axis=1)\n",
        "        \n",
        "        # Plot learning curves\n",
        "        plt.plot(train_sizes, test_mean, 'o-', color=colors[i], label=f'{emb_labels[i]} Cross-Validation Score')\n",
        "\n",
        "    # Set plot properties\n",
        "    plt.title('Learning Curves for NB Classifiers using the three Embeddings')\n",
        "    plt.xlabel('Validation Examples')\n",
        "    plt.ylabel('Score')\n",
        "    plt.legend(loc='best')\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if PRINT_METRICS:\n",
        "    embed_headers = list(classifiers_lr[0].keys())\n",
        "    classifier_headers = list(classifiers_lr[0].values())\n",
        "    for i, classifier in enumerate(classifiers_nb):\n",
        "        # Compute learning curves\n",
        "        current_embed = embed_headers[i].split('_')[-1]\n",
        "        current_classifier = classifier[i]\n",
        "        if current_embed == 'w2v':\n",
        "            train_sizes, train_scores, test_scores = learning_curve(classifier[i], np.concatenate([train_set_w2v,validation_set_w2v,test_set_w2v]),\\\n",
        "                                                                    y, cv=CV, n_jobs=-1, train_sizes=np.linspace(0.1, 1.0, 10))\n",
        "        elif current_embed == 'tfidf':\n",
        "            train_sizes, train_scores, test_scores = learning_curve(classifier[i], np.concatenate([train_set_tfidf,validation_set_tfidf,test_set_tfidf]),\\\n",
        "                                                                    y, cv=CV, n_jobs=-1, train_sizes=np.linspace(0.1, 1.0, 10))\n",
        "        else:\n",
        "            train_sizes, train_scores, test_scores = learning_curve(classifier[i], np.concatenate([train_set_ft,validation_set_ft,test_set_ft]),\\\n",
        "                                                                    y, cv=CV, n_jobs=-1, train_sizes=np.linspace(0.1, 1.0, 10))\n",
        "        train_mean = np.mean(train_scores, axis=1)\n",
        "        train_std = np.std(train_scores, axis=1)\n",
        "        test_mean = np.mean(test_scores, axis=1)\n",
        "        test_std = np.std(test_scores, axis=1)\n",
        "        \n",
        "        # Plot learning curves\n",
        "        plt.plot(train_sizes, test_mean, 'o-', color=colors[i], label=f'{emb_labels[i]} Cross-Validation Score')\n",
        "\n",
        "    # Set plot properties\n",
        "    plt.title('Learning Curves for NB Classifiers using the three Embeddings')\n",
        "    plt.xlabel('Validation Examples')\n",
        "    plt.ylabel('Score')\n",
        "    plt.legend(loc='best')\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if PRINT_METRICS:\n",
        "    embed_headers = list(classifiers_rf[0].keys())\n",
        "    classifier_headers = list(classifiers_rf[0].values())\n",
        "    for i, classifier in enumerate(classifiers_nb):\n",
        "        # Compute learning curves\n",
        "        current_embed = embed_headers[i].split('_')[-1]\n",
        "        current_classifier = classifier[i]\n",
        "        if current_embed == 'w2v':\n",
        "            train_sizes, train_scores, test_scores = learning_curve(classifier[i], np.concatenate([train_set_w2v,validation_set_w2v,test_set_w2v]),\\\n",
        "                                                                    y, cv=CV, n_jobs=-1, train_sizes=np.linspace(0.1, 1.0, 10))\n",
        "        elif current_embed == 'tfidf':\n",
        "            train_sizes, train_scores, test_scores = learning_curve(classifier[i], np.concatenate([train_set_tfidf,validation_set_tfidf,test_set_tfidf]),\\\n",
        "                                                                    y, cv=CV, n_jobs=-1, train_sizes=np.linspace(0.1, 1.0, 10))\n",
        "        else:\n",
        "            train_sizes, train_scores, test_scores = learning_curve(classifier[i], np.concatenate([train_set_ft,validation_set_ft,test_set_ft]),\\\n",
        "                                                                    y, cv=CV, n_jobs=-1, train_sizes=np.linspace(0.1, 1.0, 10))\n",
        "        train_mean = np.mean(train_scores, axis=1)\n",
        "        train_std = np.std(train_scores, axis=1)\n",
        "        test_mean = np.mean(test_scores, axis=1)\n",
        "        test_std = np.std(test_scores, axis=1)\n",
        "        \n",
        "        # Plot learning curves\n",
        "        plt.plot(train_sizes, test_mean, 'o-', color=colors[i], label=f'{emb_labels[i]} Cross-Validation Score')\n",
        "\n",
        "    # Set plot properties\n",
        "    plt.title('Learning Curves for NB Classifiers using the three Embeddings')\n",
        "    plt.xlabel('Validation Examples')\n",
        "    plt.ylabel('Score')\n",
        "    plt.legend(loc='best')\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if PRINT_METRICS:\n",
        "    embed_headers = list(classifiers_xgb[0].keys())\n",
        "    classifier_headers = list(classifiers_xgb[0].values())\n",
        "    for i, classifier in enumerate(classifiers_nb):\n",
        "        # Compute learning curves\n",
        "        current_embed = embed_headers[i].split('_')[-1]\n",
        "        current_classifier = classifier[i]\n",
        "        if current_embed == 'w2v':\n",
        "            train_sizes, train_scores, test_scores = learning_curve(classifier[i], np.concatenate([train_set_w2v,validation_set_w2v,test_set_w2v]),\\\n",
        "                                                                    y, cv=CV, n_jobs=-1, train_sizes=np.linspace(0.1, 1.0, 10))\n",
        "        elif current_embed == 'tfidf':\n",
        "            train_sizes, train_scores, test_scores = learning_curve(classifier[i], np.concatenate([train_set_tfidf,validation_set_tfidf,test_set_tfidf]),\\\n",
        "                                                                    y, cv=CV, n_jobs=-1, train_sizes=np.linspace(0.1, 1.0, 10))\n",
        "        else:\n",
        "            train_sizes, train_scores, test_scores = learning_curve(classifier[i], np.concatenate([train_set_ft,validation_set_ft,test_set_ft]),\\\n",
        "                                                                    y, cv=CV, n_jobs=-1, train_sizes=np.linspace(0.1, 1.0, 10))\n",
        "        train_mean = np.mean(train_scores, axis=1)\n",
        "        train_std = np.std(train_scores, axis=1)\n",
        "        test_mean = np.mean(test_scores, axis=1)\n",
        "        test_std = np.std(test_scores, axis=1)\n",
        "        \n",
        "        # Plot learning curves\n",
        "        plt.plot(train_sizes, test_mean, 'o-', color=colors[i], label=f'{emb_labels[i]} Cross-Validation Score')\n",
        "\n",
        "    # Set plot properties\n",
        "    plt.title('Learning Curves for NB Classifiers using the three Embeddings')\n",
        "    plt.xlabel('Validation Examples')\n",
        "    plt.ylabel('Score')\n",
        "    plt.legend(loc='best')\n",
        "\n",
        "    plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
