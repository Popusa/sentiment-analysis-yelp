{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOWNLOAD_WORDS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\.libs\\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from archived_nlp_ai_utils import *\n",
    "from chunks_urls import CHUNKS_URLS\n",
    "if DOWNLOAD_WORDS:\n",
    "    nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set this variable to true if you are using light mode, or false if you are using dark mode.\n",
    "JUPYTER = False\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_ENABLE_ONEDNN_OPTS = 0\n",
    "URLS = CHUNKS_URLS\n",
    "LIMIT = 10\n",
    "LARGER_DATASET_PATH = \"../datasets\"\n",
    "PREPROCESSED_CHUNKS_PATH = LARGER_DATASET_PATH + \"/preprocessed_sd_data_chunks_no_stop_words/\"\n",
    "BASE_FILE_NAME = \"sd_chunk_\"\n",
    "FILE_FORMAT = \".csv\"\n",
    "DATA_IN_CHUNK = 99744\n",
    "if LIMIT == 10:\n",
    "    ACTUAL_DATA_SHAPE = DATA_IN_CHUNK * LIMIT - 2\n",
    "else:\n",
    "    ACTUAL_DATA_SHAPE = DATA_IN_CHUNK * LIMIT\n",
    "RANDOM_STATE = 42\n",
    "USE_READY_DATA = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Data Sourcing and Munging"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1 Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sd_chunk_1 already exists.\n",
      "sd_chunk_2 already exists.\n",
      "sd_chunk_3 already exists.\n",
      "sd_chunk_4 already exists.\n",
      "sd_chunk_5 already exists.\n",
      "sd_chunk_6 already exists.\n",
      "sd_chunk_7 already exists.\n",
      "sd_chunk_8 already exists.\n",
      "sd_chunk_9 already exists.\n",
      "sd_chunk_10 already exists.\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(LARGER_DATASET_PATH):\n",
    "    os.mkdir(LARGER_DATASET_PATH)\n",
    "if not os.path.exists(PREPROCESSED_CHUNKS_PATH):\n",
    "    os.mkdir(PREPROCESSED_CHUNKS_PATH)\n",
    "get_chunks(URLS,LIMIT,1,BASE_FILE_NAME,PREPROCESSED_CHUNKS_PATH,FILE_FORMAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all names of downloaded files\n",
    "all_file_names = get_all_file_names(BASE_FILE_NAME,LIMIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read all chunks into a list\n",
    "list_dfs = read_chunks(all_file_names,PREPROCESSED_CHUNKS_PATH,FILE_FORMAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate all chunks into a singular df\n",
    "df = group_up_chunks(list_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check how much of the data was actually downloaded\n",
    "percent_loaded = check_no_missing_data(df.shape[0],ACTUAL_DATA_SHAPE)\n",
    "percent_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['text', 'labels'], dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(inplace = True)\n",
    "df.drop(['index'],axis = 1,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text      9488\n",
       "labels       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'].replace('', np.nan, inplace=True)\n",
    "df.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract unique words from the text data\n",
    "if USE_READY_DATA:\n",
    "    if os.path.exists(\"../archived_files/pickle_files/unique_words.pickle\"):\n",
    "        unique_words = pickle.load(open(\"../archived_files/pickle_files/unique_words.pickle\",'rb'))\n",
    "    else:\n",
    "        unique_words = set(' '.join(df['text']).split())\n",
    "else:\n",
    "    unique_words = set(' '.join(df['text']).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of occurrences of each unique word and create a bar chart of the top 20 most frequent unique words\n",
    "unique_word_count = df['text'].str.split(expand=True).stack().value_counts()\n",
    "top_unique_words = unique_word_count.loc[unique_word_count.index.isin(unique_words)].head(20)\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "colors = ['b', 'g'] \n",
    "shades = [0.37, 0.4, 0.6, 0.8]\n",
    "palette = [sns.set_hls_values(color, l=s) for color in colors for s in shades]\n",
    "\n",
    "sns.barplot(x=top_unique_words.values, y=top_unique_words.index,palette=palette)\n",
    "plt.title('Top 20 Most Frequent Unique Words')\n",
    "plt.xlabel('Word Count')\n",
    "plt.ylabel('Word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a countplot of the star ratings using Seaborn\n",
    "visualize_ratings_bar(df['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_ratings_pie(df['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_lengths = df['text'].str.split().apply(lambda x: [len(word) for word in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Histogram of word lengths\n",
    "word_lengths = df['text'].str.split().apply(lambda x: [len(word) for word in x])\n",
    "word_lengths = [length for sublist in word_lengths for length in sublist]\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(word_lengths, bins=10, range=[1, 10], lw=0.2, edgecolor='black')\n",
    "plt.title('Histogram of Word Lengths')\n",
    "plt.xlabel('Word Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Histogram of review lengths\n",
    "review_lengths = [len(text.split()) for text in df['text']]\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(review_lengths, bins=15, range=[50, 350], edgecolor='black')\n",
    "plt.title('Histogram of Text Lengths by words')\n",
    "plt.xlabel('Review Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
